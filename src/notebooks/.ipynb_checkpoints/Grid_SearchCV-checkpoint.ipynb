{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7mJMiThKvtT"
   },
   "source": [
    "# <font color=\"purple\"><b>Grid Search CV Algorithm for Wide Residual Network</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUrakNvqKvtU"
   },
   "source": [
    "Using the grid search CV algorithm, the hyperparameters of this model is sought.\n",
    "<li><b> Learning Rate:</b> 0.1, 0.01</li>\n",
    "<li><b> Regularization Penalty:</b>0.01, 0.001, 0.0001</li>\n",
    "<li><b> Batch Size:</b> 64, 128, 256</li>\n",
    "<li><b> Epochs:</b> 50, 100, 150</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rFQbEcDKvtV"
   },
   "source": [
    "## <font color=\"blue\">Import Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nhsKKJZ02AK"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from wresnet import WideResidualNetwork\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow\n",
    "import json\n",
    "import cv2\n",
    "import io\n",
    "\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_e7tYj3yb3OM"
   },
   "source": [
    "<li> <font color=\"purple\"><b> read data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL6uD0ur_1FG"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open(\"data.pz\", 'rb') as file_:\n",
    "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
    "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
    "    new_data_X = []\n",
    "    Y_data = []\n",
    "    for row in data:\n",
    "        new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
    "        Y_data.append(row['label'])\n",
    "    new_data_X = np.array(new_data_X)\n",
    "\n",
    "    return new_data_X, Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM7DG7CtKvtg"
   },
   "source": [
    "<font color=\"sky blue\"><b> Data preprocessing</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlEfFPS50iBi"
   },
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    # creating initial dataframe\n",
    "    X, Y = read_data()\n",
    "    y_df = pd.DataFrame(Y, columns=['Label'])\n",
    "    labelencoder = LabelEncoder()\n",
    "    y_df['Categrory'] = labelencoder.fit_transform(y_df['Label'])\n",
    "\n",
    "\n",
    "    img_rows, img_cols = X[0].shape\n",
    "\n",
    "\n",
    "    # transform data set\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "    Y_cat = to_categorical(y_df['Categrory'])\n",
    "    return X, Y_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9X1E2wybnZj"
   },
   "source": [
    "<font color=\"blue\"> The algorithm below is that ... </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8U0Vk9t0l3V"
   },
   "outputs": [],
   "source": [
    "def KFold_GridSearchCV(input_dim, X, Y, X_test, y_test, combinations, filename=\"log.csv\", acc_loss_json=\"hist.json\"):\n",
    "    \"\"\"Summary: Grid Search CV for 3 Folds Cross Validation\n",
    "    \"\"\"\n",
    "    res_df = pd.DataFrame(columns=['momentum','learning rate','batch size',\n",
    "                                      'loss1', 'acc1','loss2', 'acc2','loss3', 'acc3', 'widing factor'])\n",
    "    generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
    "                               width_shift_range=5./32,\n",
    "                               height_shift_range=5./32,)\n",
    "    hist_dict_global = {}\n",
    "\n",
    "    for i, combination in enumerate(combinations):\n",
    "        kf = KFold(n_splits=3, random_state=42, shuffle=False)\n",
    "        metrics_dict = {}\n",
    "  \n",
    "        for j, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "            X_train, X_val = X[train_index], X[test_index]\n",
    "            y_train, y_val = Y[train_index], Y[test_index]\n",
    "            wresnet_ins = WideResidualNetwork(combination[2],  in_dim,combination[4], nb_classes=4, N=2, k=1, dropout=0.0)\n",
    "            model = wresnet_ins.create_wide_residual_network()\n",
    "            opt = tensorflow.keras.optimizers.SGD(learning_rate=combination[0])\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=opt, metrics= ['accuracy'])\n",
    "            hist = model.fit_generator(generator.flow(X_train, y_train, batch_size=combination[1]), steps_per_epoch=len(X_train) // combination[1], epochs=combination[3],\n",
    "                                      validation_data=(X_val, y_val),\n",
    "                                      validation_steps=len(X_val) // combination[1],)\n",
    "            loss, acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "            metrics_dict[j+1] = {\"loss\": loss, \"acc\": acc, \"epoch_stopped\": combination[3]}\n",
    "            graph_loss_acc = {\"id\": i, \"com\":j+1, \"val_acc\":hist.history[\"val_accuracy\"], \"train_acc\":hist.history[\"accuracy\"],\n",
    "                                    \"val_loss\":hist.history[\"val_loss\"], \"train_loss\":hist.history[\"loss\"], \"epoch_stopped\": combination[3], 'learning rate': combination[0],\n",
    "                                    'batch size': combination[1], 'reg_penalty': combination[2]}\n",
    "\n",
    "              # Write JSON file\n",
    "            with io.open(acc_loss_json, 'a+', encoding='utf8') as outfile:\n",
    "                str_ = json.dumps(graph_loss_acc)\n",
    "                outfile.write(to_unicode(str_))\n",
    "            row = {'momentum': combination[4],'learning rate': combination[0],\n",
    "                        'batch size': combination[1],\n",
    "                        'reg_penalty': combination[2],\n",
    "                        'epoch_stopped': metrics_dict[1][\"epoch_stopped\"],\n",
    "                        'widing factor' : 2,\n",
    "                        'loss1': metrics_dict[1][\"loss\"],\n",
    "                        'acc1': metrics_dict[1][\"acc\"],\n",
    "                        'loss2': metrics_dict[2][\"loss\"],\n",
    "                        'acc2': metrics_dict[2][\"acc\"],\n",
    "                        'loss3': metrics_dict[3][\"loss\"],\n",
    "                        'acc3': metrics_dict[3][\"acc\"]}\n",
    "        res_df = res_df.append(row , ignore_index=True)\n",
    "        res_df.to_csv(filename, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgO8xJ0Fe93I"
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    learning_rate = [0.1, 0.01]\n",
    "    batch_size = [64,128,256]\n",
    "    reg_penalty = [0.01, 0.001, 0.0001]\n",
    "    epochs = [50,100,150]\n",
    "    momentum = [0.9]\n",
    "    in_dim = (32,32,1)\n",
    "    grid_result = \"grid_16_1_11.csv\"\n",
    "    acc_loss_json = \"history.json\"\n",
    "\n",
    "    # create list of all different parameter combinations\n",
    "    param_grid = dict(learning_rate = learning_rate, batch_size = batch_size, \n",
    "                      reg_penalty = reg_penalty, epochs = epochs, momentum=momentum)\n",
    "    combinations = list(product(*param_grid.values()))\n",
    "    X, Y = preprocessing()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.1, shuffle=False)\n",
    "    print(combinations)\n",
    "    KFold_GridSearchCV(in_dim,X_train,y_train,X_test, y_test, combinations, grid_result, acc_loss_json)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Grid_SearchCV.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
