{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ParsevalNetwork.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_idgCcuZdv0S",
        "colab_type": "text"
      },
      "source": [
        "# **Parseval Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO1kBOuDd2_r",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esXS1STy_O_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11nX1rD7_O_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91635573-4d28-40e8-e760-28068d36883a"
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n",
        "import cv2\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuUWZk7KeFck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "371f3462-93a4-4ef4-8110-b69c41bce5e9"
      },
      "source": [
        "X = new_data_X.astype('float32')\n",
        "X.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T0HkiPfeH1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X[0].shape\n",
        "\n",
        "# transform data set\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crqYw_LJeJif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "y_df = pd.DataFrame(Y_data, columns=['Label'])\n",
        "y_df['Encoded'] = labelencoder.fit_transform(y_df['Label'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuCH_ylOeLLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d968e914-408f-4f13-abb8-5cba7bc35c94"
      },
      "source": [
        "y_df['Label'].value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "closed           1500\n",
              "open             1500\n",
              "partiallyOpen    1376\n",
              "notVisible       1346\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJGV2GZWeN-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "29b1c646-724b-4a4a-d0ca-7ed7989f3f03"
      },
      "source": [
        "y_df['Encoded'].value_counts()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    1500\n",
              "0    1500\n",
              "3    1376\n",
              "1    1346\n",
              "Name: Encoded, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBVaEoQBeP7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_cat = to_categorical(y_df['Encoded'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Q7NqR4eYQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, y_test = train_test_split(X, y_cat, test_size = 0.1)\n",
        "x_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTmOEDcMtSQp",
        "colab_type": "text"
      },
      "source": [
        "# Othogonal Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S2jnj2otWlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.constraints import Constraint\n",
        "from tensorflow.python.ops import math_ops, array_ops\n",
        "\n",
        "class TightFrame(Constraint):\n",
        "\n",
        "\n",
        "    def __init__(self, scale, num_passes=1):\n",
        "        self.scale = scale\n",
        "\n",
        "        if num_passes < 1:\n",
        "            raise ValueError(\"Number of passes cannot be non-positive! (got {})\".format(num_passes))\n",
        "        self.num_passes = num_passes\n",
        "\n",
        "\n",
        "    def __call__(self, w):\n",
        "        transpose_channels = (len(w.shape) == 4)\n",
        "\n",
        "        # Move channels_num to the front in order to make the dimensions correct for matmul\n",
        "        if transpose_channels:\n",
        "            w_reordered = array_ops.reshape(w, (-1, w.shape[0]))\n",
        "\n",
        "        else:\n",
        "            w_reordered = w\n",
        "\n",
        "        last = w_reordered\n",
        "        for i in range(self.num_passes):\n",
        "            temp1 = math_ops.matmul(last, last, transpose_a=True)\n",
        "            temp2 = (1 + self.scale) * w_reordered - self.scale * math_ops.matmul(w_reordered, temp1)\n",
        "\n",
        "            last = temp2\n",
        "\n",
        "        # Move channels_num to the back again\n",
        "        if transpose_channels:\n",
        "            return array_ops.reshape(last, w.shape)\n",
        "        else:\n",
        "            return last\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'scale': self.scale, 'num_passes': self.num_passes}\n",
        "\n",
        "\n",
        "# Alias\n",
        "tight_frame = TightFrame"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4VciYzctsSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import warnings\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0001\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3g_vzsy_ndF",
        "colab_type": "text"
      },
      "source": [
        "**Parseval Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN76FYUZ_3YA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b50e2850-2d94-4076-d4d2-8af846b59bbd"
      },
      "source": [
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "  \n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='orthogonal', kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='Orthogonal', kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv2:channel:  {}\".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv3 channel_axis:{} \".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='Orthogonal',\n",
        "                      kernel_constraint= tight_frame(0.0001),\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_parseval_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, activation='softmax' )(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Parseval Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    init = (32, 32,1)\n",
        "\n",
        "    parseval_16_2 = create_parseval_network(init, nb_classes=4, N=2, k=2, dropout=0.5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxyMKoeaBqPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.callbacks as callbacks\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liiFrat1Bv1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "BS = 128\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygMFWH8Bzfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhUvPL0dB48O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72e883df-d367-436a-997b-1e5061a4a299"
      },
      "source": [
        "parseval_16_2.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JrJ7xy1Q5rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow\n",
        "generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4EpGTMG9jeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_sch(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.1\n",
        "    elif epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_sch)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z0CbPY_24ns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2dd33f95-df43-4274-925f-1630e87f08b0"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "hist = parseval_16_2.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=EPOCHS,\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_steps=X_val.shape[0] // BS,)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 4s 113ms/step - loss: 1.4488 - acc: 0.3154 - val_loss: 1.3641 - val_acc: 0.4155 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.3486 - acc: 0.3617 - val_loss: 1.3250 - val_acc: 0.4039 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.3055 - acc: 0.3799 - val_loss: 1.5271 - val_acc: 0.2660 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.2930 - acc: 0.3895 - val_loss: 1.2269 - val_acc: 0.4311 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 1.2587 - acc: 0.4097 - val_loss: 1.2036 - val_acc: 0.4777 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 1.2393 - acc: 0.4458 - val_loss: 1.1648 - val_acc: 0.5223 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 1.1981 - acc: 0.4882 - val_loss: 1.1165 - val_acc: 0.5398 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 1.1474 - acc: 0.5320 - val_loss: 1.1038 - val_acc: 0.5650 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.1179 - acc: 0.5388 - val_loss: 1.0812 - val_acc: 0.5612 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.0792 - acc: 0.5628 - val_loss: 1.2794 - val_acc: 0.5534 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.0388 - acc: 0.5846 - val_loss: 1.1911 - val_acc: 0.5534 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 1.0166 - acc: 0.5852 - val_loss: 1.0691 - val_acc: 0.5359 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.9586 - acc: 0.6134 - val_loss: 1.1934 - val_acc: 0.5942 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.9187 - acc: 0.6289 - val_loss: 0.8048 - val_acc: 0.7010 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.9066 - acc: 0.6445 - val_loss: 1.1778 - val_acc: 0.5942 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.8339 - acc: 0.6884 - val_loss: 0.9064 - val_acc: 0.6466 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.8401 - acc: 0.6727 - val_loss: 0.7970 - val_acc: 0.7029 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.8039 - acc: 0.6955 - val_loss: 1.4973 - val_acc: 0.5515 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.7972 - acc: 0.6926 - val_loss: 0.8009 - val_acc: 0.6971 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.7727 - acc: 0.7013 - val_loss: 0.8284 - val_acc: 0.6971 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.7651 - acc: 0.6977 - val_loss: 0.8052 - val_acc: 0.6660 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.7266 - acc: 0.7248 - val_loss: 0.8018 - val_acc: 0.7301 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.7333 - acc: 0.7230 - val_loss: 0.8513 - val_acc: 0.7184 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.7155 - acc: 0.7315 - val_loss: 0.8371 - val_acc: 0.6990 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.6969 - acc: 0.7412 - val_loss: 1.0238 - val_acc: 0.6544 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.6760 - acc: 0.7512 - val_loss: 0.8494 - val_acc: 0.6544 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.6968 - acc: 0.7446 - val_loss: 0.7223 - val_acc: 0.7340 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.6751 - acc: 0.7432 - val_loss: 0.6679 - val_acc: 0.7515 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.6507 - acc: 0.7537 - val_loss: 0.7549 - val_acc: 0.7320 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.6461 - acc: 0.7583 - val_loss: 0.6882 - val_acc: 0.7437 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.6804 - acc: 0.7401 - val_loss: 0.7066 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.6122 - acc: 0.7719 - val_loss: 0.6670 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5820 - acc: 0.7894 - val_loss: 0.6592 - val_acc: 0.7612 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5680 - acc: 0.7965 - val_loss: 0.6615 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5569 - acc: 0.8000 - val_loss: 0.6421 - val_acc: 0.7476 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5559 - acc: 0.7980 - val_loss: 0.6733 - val_acc: 0.7534 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5462 - acc: 0.7976 - val_loss: 0.6303 - val_acc: 0.7631 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5360 - acc: 0.8003 - val_loss: 0.6288 - val_acc: 0.7553 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5291 - acc: 0.8063 - val_loss: 0.6251 - val_acc: 0.7650 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5344 - acc: 0.8063 - val_loss: 0.6316 - val_acc: 0.7476 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.5175 - acc: 0.8100 - val_loss: 0.6265 - val_acc: 0.7631 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5241 - acc: 0.8085 - val_loss: 0.6227 - val_acc: 0.7786 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5100 - acc: 0.8162 - val_loss: 0.6176 - val_acc: 0.7670 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5103 - acc: 0.8129 - val_loss: 0.6331 - val_acc: 0.7689 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5109 - acc: 0.8138 - val_loss: 0.6370 - val_acc: 0.7670 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.5121 - acc: 0.8114 - val_loss: 0.6207 - val_acc: 0.7767 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5060 - acc: 0.8156 - val_loss: 0.6268 - val_acc: 0.7650 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5012 - acc: 0.8233 - val_loss: 0.6330 - val_acc: 0.7670 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.5011 - acc: 0.8186 - val_loss: 0.6200 - val_acc: 0.7689 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.5039 - acc: 0.8198 - val_loss: 0.6166 - val_acc: 0.7728 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4920 - acc: 0.8260 - val_loss: 0.6520 - val_acc: 0.7650 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4929 - acc: 0.8176 - val_loss: 0.6197 - val_acc: 0.7806 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4908 - acc: 0.8220 - val_loss: 0.6155 - val_acc: 0.7728 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4797 - acc: 0.8289 - val_loss: 0.6306 - val_acc: 0.7495 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4868 - acc: 0.8198 - val_loss: 0.6356 - val_acc: 0.7573 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4857 - acc: 0.8211 - val_loss: 0.6156 - val_acc: 0.7748 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4880 - acc: 0.8211 - val_loss: 0.6244 - val_acc: 0.7709 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4807 - acc: 0.8269 - val_loss: 0.6118 - val_acc: 0.7689 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4773 - acc: 0.8316 - val_loss: 0.6109 - val_acc: 0.7767 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4656 - acc: 0.8300 - val_loss: 0.6156 - val_acc: 0.7748 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4757 - acc: 0.8247 - val_loss: 0.6289 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4732 - acc: 0.8276 - val_loss: 0.6059 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4721 - acc: 0.8311 - val_loss: 0.6262 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4768 - acc: 0.8251 - val_loss: 0.6100 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4700 - acc: 0.8289 - val_loss: 0.6195 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4694 - acc: 0.8285 - val_loss: 0.6071 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4705 - acc: 0.8316 - val_loss: 0.6145 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4588 - acc: 0.8340 - val_loss: 0.6337 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4690 - acc: 0.8329 - val_loss: 0.6238 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4718 - acc: 0.8347 - val_loss: 0.5973 - val_acc: 0.7845 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4728 - acc: 0.8287 - val_loss: 0.6100 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4716 - acc: 0.8278 - val_loss: 0.6091 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4697 - acc: 0.8269 - val_loss: 0.6175 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4660 - acc: 0.8327 - val_loss: 0.6243 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4744 - acc: 0.8269 - val_loss: 0.6362 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4661 - acc: 0.8309 - val_loss: 0.6072 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4730 - acc: 0.8262 - val_loss: 0.6052 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4671 - acc: 0.8280 - val_loss: 0.6070 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4761 - acc: 0.8260 - val_loss: 0.6253 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4747 - acc: 0.8251 - val_loss: 0.6140 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4725 - acc: 0.8285 - val_loss: 0.6198 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4656 - acc: 0.8338 - val_loss: 0.6185 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4651 - acc: 0.8344 - val_loss: 0.6060 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4767 - acc: 0.8262 - val_loss: 0.6461 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4708 - acc: 0.8265 - val_loss: 0.6089 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4666 - acc: 0.8316 - val_loss: 0.6057 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4662 - acc: 0.8322 - val_loss: 0.6007 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4693 - acc: 0.8296 - val_loss: 0.6173 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4633 - acc: 0.8296 - val_loss: 0.6001 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4619 - acc: 0.8351 - val_loss: 0.6057 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4689 - acc: 0.8245 - val_loss: 0.6105 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4648 - acc: 0.8278 - val_loss: 0.6174 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4606 - acc: 0.8329 - val_loss: 0.6149 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4676 - acc: 0.8331 - val_loss: 0.6399 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4643 - acc: 0.8253 - val_loss: 0.6269 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4626 - acc: 0.8304 - val_loss: 0.6085 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4712 - acc: 0.8287 - val_loss: 0.6207 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4695 - acc: 0.8262 - val_loss: 0.6034 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4768 - acc: 0.8265 - val_loss: 0.6150 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4665 - acc: 0.8285 - val_loss: 0.6208 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4663 - acc: 0.8336 - val_loss: 0.6316 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4678 - acc: 0.8287 - val_loss: 0.6136 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4793 - acc: 0.8182 - val_loss: 0.6111 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4640 - acc: 0.8336 - val_loss: 0.6071 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4580 - acc: 0.8358 - val_loss: 0.6122 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4630 - acc: 0.8285 - val_loss: 0.6088 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4621 - acc: 0.8293 - val_loss: 0.6188 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4690 - acc: 0.8316 - val_loss: 0.6034 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4650 - acc: 0.8293 - val_loss: 0.6002 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4660 - acc: 0.8273 - val_loss: 0.6128 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4629 - acc: 0.8338 - val_loss: 0.6334 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4660 - acc: 0.8245 - val_loss: 0.6214 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4673 - acc: 0.8267 - val_loss: 0.6148 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4558 - acc: 0.8329 - val_loss: 0.6095 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4677 - acc: 0.8282 - val_loss: 0.6070 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4698 - acc: 0.8291 - val_loss: 0.6051 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4563 - acc: 0.8336 - val_loss: 0.6037 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4693 - acc: 0.8267 - val_loss: 0.6218 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4646 - acc: 0.8307 - val_loss: 0.6076 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4699 - acc: 0.8293 - val_loss: 0.6184 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4680 - acc: 0.8267 - val_loss: 0.5993 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4652 - acc: 0.8316 - val_loss: 0.6023 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4691 - acc: 0.8318 - val_loss: 0.6082 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4691 - acc: 0.8267 - val_loss: 0.6721 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 3s 84ms/step - loss: 0.4648 - acc: 0.8289 - val_loss: 0.6297 - val_acc: 0.7612 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4657 - acc: 0.8233 - val_loss: 0.6118 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4641 - acc: 0.8298 - val_loss: 0.6072 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.4680 - acc: 0.8236 - val_loss: 0.6057 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4543 - acc: 0.8340 - val_loss: 0.6264 - val_acc: 0.7612 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4694 - acc: 0.8293 - val_loss: 0.6599 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4582 - acc: 0.8347 - val_loss: 0.6211 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4541 - acc: 0.8327 - val_loss: 0.6123 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4603 - acc: 0.8362 - val_loss: 0.6043 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4620 - acc: 0.8336 - val_loss: 0.6114 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4607 - acc: 0.8240 - val_loss: 0.6157 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4658 - acc: 0.8287 - val_loss: 0.6174 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4662 - acc: 0.8342 - val_loss: 0.6176 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4641 - acc: 0.8265 - val_loss: 0.6022 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4643 - acc: 0.8320 - val_loss: 0.6365 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4704 - acc: 0.8225 - val_loss: 0.6372 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4585 - acc: 0.8309 - val_loss: 0.6197 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4640 - acc: 0.8291 - val_loss: 0.6092 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4616 - acc: 0.8329 - val_loss: 0.6063 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4662 - acc: 0.8265 - val_loss: 0.6303 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4675 - acc: 0.8262 - val_loss: 0.6140 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4669 - acc: 0.8331 - val_loss: 0.6079 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4631 - acc: 0.8336 - val_loss: 0.6125 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4553 - acc: 0.8324 - val_loss: 0.6458 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4646 - acc: 0.8327 - val_loss: 0.6128 - val_acc: 0.7612 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4701 - acc: 0.8220 - val_loss: 0.6072 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4653 - acc: 0.8240 - val_loss: 0.6106 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4602 - acc: 0.8333 - val_loss: 0.6202 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4713 - acc: 0.8287 - val_loss: 0.6012 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4681 - acc: 0.8231 - val_loss: 0.6108 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4683 - acc: 0.8260 - val_loss: 0.6649 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4651 - acc: 0.8307 - val_loss: 0.6276 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4661 - acc: 0.8347 - val_loss: 0.6089 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4628 - acc: 0.8338 - val_loss: 0.6137 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4607 - acc: 0.8304 - val_loss: 0.6065 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4665 - acc: 0.8265 - val_loss: 0.6218 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4529 - acc: 0.8276 - val_loss: 0.6103 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4638 - acc: 0.8293 - val_loss: 0.6172 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 0.4651 - acc: 0.8302 - val_loss: 0.6185 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4666 - acc: 0.8327 - val_loss: 0.6319 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4590 - acc: 0.8338 - val_loss: 0.6255 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4583 - acc: 0.8318 - val_loss: 0.6096 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4591 - acc: 0.8316 - val_loss: 0.6050 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4645 - acc: 0.8320 - val_loss: 0.6333 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4699 - acc: 0.8269 - val_loss: 0.6108 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4743 - acc: 0.8242 - val_loss: 0.6180 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.4561 - acc: 0.8338 - val_loss: 0.6426 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4663 - acc: 0.8298 - val_loss: 0.6208 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4584 - acc: 0.8302 - val_loss: 0.6078 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4582 - acc: 0.8364 - val_loss: 0.6213 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4636 - acc: 0.8316 - val_loss: 0.6161 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4648 - acc: 0.8269 - val_loss: 0.6124 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4521 - acc: 0.8300 - val_loss: 0.6017 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4585 - acc: 0.8360 - val_loss: 0.6025 - val_acc: 0.7767 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4664 - acc: 0.8307 - val_loss: 0.6179 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4631 - acc: 0.8273 - val_loss: 0.6101 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4624 - acc: 0.8256 - val_loss: 0.6144 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4729 - acc: 0.8271 - val_loss: 0.6186 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4666 - acc: 0.8253 - val_loss: 0.6129 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.4637 - acc: 0.8322 - val_loss: 0.6216 - val_acc: 0.7612 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4673 - acc: 0.8296 - val_loss: 0.6146 - val_acc: 0.7631 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4615 - acc: 0.8240 - val_loss: 0.6150 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4587 - acc: 0.8347 - val_loss: 0.6204 - val_acc: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.4579 - acc: 0.8336 - val_loss: 0.6059 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4657 - acc: 0.8287 - val_loss: 0.6421 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4598 - acc: 0.8300 - val_loss: 0.6301 - val_acc: 0.7689 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 0.4658 - acc: 0.8305 - val_loss: 0.6248 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4614 - acc: 0.8320 - val_loss: 0.6128 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4600 - acc: 0.8318 - val_loss: 0.5955 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4652 - acc: 0.8340 - val_loss: 0.6305 - val_acc: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4612 - acc: 0.8296 - val_loss: 0.6142 - val_acc: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4608 - acc: 0.8336 - val_loss: 0.6026 - val_acc: 0.7806 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4633 - acc: 0.8282 - val_loss: 0.6136 - val_acc: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4717 - acc: 0.8291 - val_loss: 0.6050 - val_acc: 0.7786 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 0.4613 - acc: 0.8322 - val_loss: 0.5996 - val_acc: 0.7825 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 3s 85ms/step - loss: 0.4618 - acc: 0.8342 - val_loss: 0.6164 - val_acc: 0.7670 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNGXxJTQq-UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAVQ_YwYpeMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ae7674dc-b01e-42d8-e834-c1061c726701"
      },
      "source": [
        "from matplotlib import  pyplot\n",
        "\n",
        "pyplot.plot(hist.history[\"acc\"], label='train')\n",
        "pyplot.plot(hist.history['val_acc'], label='test')\n",
        "pyplot.title('model accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'val'], loc='upper left')\n",
        "pyplot.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-QerZ4RN91R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "024d42f4-f24a-4207-a394-1aa5ed8e4dbf"
      },
      "source": [
        "from matplotlib import  pyplot\n",
        "\n",
        "pyplot.plot(hist.history[\"acc\"], label='train')\n",
        "pyplot.plot(hist.history['val_acc'], label='test')\n",
        "pyplot.title('model accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'val'], loc='upper left')\n",
        "pyplot.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeg6MBf-GSgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_err = [1.0-x for x in hist.history['val_acc']]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC6DagsghRg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "cc8153c7-b61d-4349-9eb5-58ebf5add931"
      },
      "source": [
        "# summarize history for loss\n",
        "pyplot.plot(hist.history['loss'])\n",
        "pyplot.plot(hist.history['val_loss'])\n",
        "pyplot.title('model loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'test'], loc='upper left')\n",
        "pyplot.show()\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgiKT6i5GiJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "fc18a10a-147d-4a36-82d2-5ac37612e435"
      },
      "source": [
        "pyplot.plot(test_err, label='test')\n",
        "pyplot.savefig(\"deneme_err.png\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4-IJ9Z-dQKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "parseval_16_2.save(\"parseval_OC.h5\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqddlwbmEV9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = parseval_16_2.predict(X_test)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Ysop6NFDbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "535d8315-3ad6-4f18-c0a3-ac4172d7ab81"
      },
      "source": [
        "parseval_16_2.evaluate(X_test,y_test)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6367 - acc: 0.7592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6367035508155823, 0.7591623067855835]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUCxeuFMmMw7",
        "colab_type": "text"
      },
      "source": [
        "# **Adversarial Examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knp-F1cdmQgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import  KFold\n",
        "\n",
        "class Non_adversarial(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train_iterate(self, X_train, Y_train, X_test, y_test, epochs, BS,sgd, epsilon_list):\n",
        "          init = (32, 32,1)\n",
        "          res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                  'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                    'acc3','loss4', 'acc4'])\n",
        "          kf = KFold(n_splits=3, random_state=42, shuffle=False)\n",
        "          \n",
        "          for j, (train, val) in enumerate(kf.split(X_train)):\n",
        "            x_train, y_train,  x_val, y_val = X_train[train], Y_train[train], X_train[val], Y_train[val]\n",
        "            model = create_parseval_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "\n",
        "            model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "            hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                            callbacks = [lr_scheduler],\n",
        "                            validation_data=(x_val, y_val),\n",
        "                            validation_steps=x_val.shape[0] // BS,)\n",
        "            loss, acc = model.evaluate(X_test, y_test)\n",
        "            loss1, acc1 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "            loss2, acc2 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "            loss3, acc3 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "            loss4, acc4 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "            row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                    'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "            res_df = res_df.append(row , ignore_index=True)\n",
        "            \n",
        "          return res_df"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVhO4DCbmWAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "94bed26f-6a09-4cf6-bb1a-582587151c0b"
      },
      "source": [
        "\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)\n",
        "print(\"Cleverhans Version: \" + cleverhans.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-3vrv6t6n/cleverhans\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-3vrv6t6n/cleverhans\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 2.7MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.2.1)\n",
            "Collecting mnist~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.15.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.12.0)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=262572 sha256=3ff54fd967a1d3f8c08bf93111cc757e255d63d82a72e5ffbc9d0fc1d65e9be3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0_45vrf2/wheels/6e/59/ec/723a6f654aaf62c8c40f0f0850fdf71a4948598697f56c3bfa\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: nose, pycodestyle, mnist, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 nose-1.3.7 pycodestyle-2.6.0\n",
            "\n",
            "Tensorflow Version: 2.2.0\n",
            "Cleverhans Version: 3.0.1-fc7b7c7ec903258e0e3fb88503fa629f\n",
            "WARNING:tensorflow:From <ipython-input-39-21b906498791>:8: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU Available:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv0ZE8pumboZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "def get_adversarial_examples(pretrained_model, X_true, y_true, epsilon):\n",
        "  #The attack requires the model to ouput the logits\n",
        "   \n",
        "  logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
        "  X_adv = []\n",
        "  for i in range(len(X_true)):\n",
        "    random_index = i\n",
        "    original_image = X_true[random_index]\n",
        "    original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "    original_label = y_true[random_index]\n",
        "    original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "    adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "    X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "  X_adv = np.array(X_adv)\n",
        "  return X_adv\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxeGP9HYmd7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_graph(hist):\n",
        "  history = hist\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"wrn_tensor.png\")\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"deneme.png\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6Wgf8sOmgMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_test(model,X_adv, X_test, y_test, epsilon):\n",
        "  loss, acc = model.evaluate(X_adv,y_test)\n",
        "  print(\"epsilon: {} and test evaluation : {}, {}\".format(epsilon,loss, acc))\n",
        "  SNR = 20*np.log10(np.linalg.norm(X_test)/np.linalg.norm(X_test-X_adv))\n",
        "  print(\"SNR: {}\".format(SNR))\n",
        "  return loss, acc"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKoVsxZDOxyL",
        "colab_type": "text"
      },
      "source": [
        "**Non-Adversarial Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DWYnAsDsRJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78MYsOGOw-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b25ff5f7-0a27-44e3-8fe9-0949e03409dc"
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "train_object = Non_adversarial()\n",
        "result_df = train_object.train_iterate(X_train, Y_train, X_test, y_test, EPOCHS, BS,sgd, epsilon_list)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 4s 141ms/step - loss: 1.4713 - acc: 0.2863 - val_loss: 1.4247 - val_acc: 0.3611 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3996 - acc: 0.3562 - val_loss: 1.3585 - val_acc: 0.3722 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.3514 - acc: 0.3620 - val_loss: 1.3280 - val_acc: 0.3535 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.3093 - acc: 0.3789 - val_loss: 1.4049 - val_acc: 0.3081 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2975 - acc: 0.3780 - val_loss: 1.3101 - val_acc: 0.3821 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2790 - acc: 0.4004 - val_loss: 1.3021 - val_acc: 0.4112 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2775 - acc: 0.4025 - val_loss: 1.2968 - val_acc: 0.3751 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2650 - acc: 0.4337 - val_loss: 1.2385 - val_acc: 0.4158 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2289 - acc: 0.4649 - val_loss: 1.2321 - val_acc: 0.4776 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1937 - acc: 0.5015 - val_loss: 1.2648 - val_acc: 0.4321 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1410 - acc: 0.5275 - val_loss: 1.1436 - val_acc: 0.5108 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1056 - acc: 0.5457 - val_loss: 1.1353 - val_acc: 0.5160 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0866 - acc: 0.5533 - val_loss: 1.3600 - val_acc: 0.4030 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0535 - acc: 0.5702 - val_loss: 1.2365 - val_acc: 0.5044 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0228 - acc: 0.5881 - val_loss: 1.1056 - val_acc: 0.5498 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9966 - acc: 0.6032 - val_loss: 0.9804 - val_acc: 0.5906 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9553 - acc: 0.6301 - val_loss: 1.0296 - val_acc: 0.5946 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9067 - acc: 0.6435 - val_loss: 1.6602 - val_acc: 0.3826 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9009 - acc: 0.6426 - val_loss: 1.0677 - val_acc: 0.6098 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8835 - acc: 0.6601 - val_loss: 1.0908 - val_acc: 0.5434 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8508 - acc: 0.6710 - val_loss: 1.0706 - val_acc: 0.5521 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8156 - acc: 0.6889 - val_loss: 0.9326 - val_acc: 0.6080 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8230 - acc: 0.6780 - val_loss: 0.8989 - val_acc: 0.6278 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7797 - acc: 0.7040 - val_loss: 0.8697 - val_acc: 0.6657 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8044 - acc: 0.6873 - val_loss: 0.8042 - val_acc: 0.6902 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7675 - acc: 0.7119 - val_loss: 0.7674 - val_acc: 0.7030 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7507 - acc: 0.7222 - val_loss: 1.1696 - val_acc: 0.5859 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7198 - acc: 0.7324 - val_loss: 0.8479 - val_acc: 0.6773 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7426 - acc: 0.7212 - val_loss: 0.8640 - val_acc: 0.6814 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7089 - acc: 0.7367 - val_loss: 0.8979 - val_acc: 0.6517 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7852 - acc: 0.6873 - val_loss: 0.8160 - val_acc: 0.6768 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7024 - acc: 0.7288 - val_loss: 0.7673 - val_acc: 0.6942 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6525 - acc: 0.7539 - val_loss: 0.7506 - val_acc: 0.7059 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6389 - acc: 0.7660 - val_loss: 0.7348 - val_acc: 0.7193 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6287 - acc: 0.7700 - val_loss: 0.7155 - val_acc: 0.7199 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6164 - acc: 0.7830 - val_loss: 0.7052 - val_acc: 0.7251 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6133 - acc: 0.7724 - val_loss: 0.7075 - val_acc: 0.7274 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5924 - acc: 0.7827 - val_loss: 0.7209 - val_acc: 0.7169 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5982 - acc: 0.7848 - val_loss: 0.6961 - val_acc: 0.7368 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5801 - acc: 0.7887 - val_loss: 0.7413 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5951 - acc: 0.7785 - val_loss: 0.6879 - val_acc: 0.7385 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5981 - acc: 0.7812 - val_loss: 0.6888 - val_acc: 0.7373 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5879 - acc: 0.7884 - val_loss: 0.6911 - val_acc: 0.7338 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5766 - acc: 0.7906 - val_loss: 0.6899 - val_acc: 0.7338 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5851 - acc: 0.7863 - val_loss: 0.6868 - val_acc: 0.7362 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5818 - acc: 0.7887 - val_loss: 0.7095 - val_acc: 0.7245 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5703 - acc: 0.7893 - val_loss: 0.6866 - val_acc: 0.7327 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5582 - acc: 0.7987 - val_loss: 0.6754 - val_acc: 0.7420 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5625 - acc: 0.7969 - val_loss: 0.6837 - val_acc: 0.7391 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5700 - acc: 0.7966 - val_loss: 0.6774 - val_acc: 0.7420 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5607 - acc: 0.7924 - val_loss: 0.6905 - val_acc: 0.7356 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5542 - acc: 0.7960 - val_loss: 0.6866 - val_acc: 0.7391 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5484 - acc: 0.7987 - val_loss: 0.7077 - val_acc: 0.7286 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5547 - acc: 0.7954 - val_loss: 0.7404 - val_acc: 0.7234 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5526 - acc: 0.7921 - val_loss: 0.6851 - val_acc: 0.7338 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5556 - acc: 0.7939 - val_loss: 0.6755 - val_acc: 0.7391 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5494 - acc: 0.8033 - val_loss: 0.6860 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5508 - acc: 0.8005 - val_loss: 0.6811 - val_acc: 0.7414 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5424 - acc: 0.8078 - val_loss: 0.6686 - val_acc: 0.7420 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5377 - acc: 0.8033 - val_loss: 0.6734 - val_acc: 0.7385 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5390 - acc: 0.8090 - val_loss: 0.6813 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5485 - acc: 0.7999 - val_loss: 0.6728 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5313 - acc: 0.8062 - val_loss: 0.6796 - val_acc: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5365 - acc: 0.8081 - val_loss: 0.6774 - val_acc: 0.7350 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5281 - acc: 0.8145 - val_loss: 0.6732 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5446 - acc: 0.7984 - val_loss: 0.6754 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5355 - acc: 0.8078 - val_loss: 0.6708 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5260 - acc: 0.8145 - val_loss: 0.6818 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5383 - acc: 0.8015 - val_loss: 0.6966 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5276 - acc: 0.8178 - val_loss: 0.6731 - val_acc: 0.7408 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5378 - acc: 0.8096 - val_loss: 0.6747 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5345 - acc: 0.8116 - val_loss: 0.6773 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5344 - acc: 0.8130 - val_loss: 0.6771 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5332 - acc: 0.8084 - val_loss: 0.6860 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5292 - acc: 0.8081 - val_loss: 0.6714 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5331 - acc: 0.8081 - val_loss: 0.6803 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5342 - acc: 0.8099 - val_loss: 0.6815 - val_acc: 0.7362 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5244 - acc: 0.8108 - val_loss: 0.6888 - val_acc: 0.7338 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5340 - acc: 0.8059 - val_loss: 0.6665 - val_acc: 0.7402 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5364 - acc: 0.7993 - val_loss: 0.6638 - val_acc: 0.7467 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5358 - acc: 0.8087 - val_loss: 0.6722 - val_acc: 0.7402 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5431 - acc: 0.8042 - val_loss: 0.6657 - val_acc: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5412 - acc: 0.8036 - val_loss: 0.6697 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5376 - acc: 0.8036 - val_loss: 0.6652 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5358 - acc: 0.8005 - val_loss: 0.6883 - val_acc: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5372 - acc: 0.8021 - val_loss: 0.6768 - val_acc: 0.7402 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5389 - acc: 0.8008 - val_loss: 0.6726 - val_acc: 0.7408 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5338 - acc: 0.8045 - val_loss: 0.6700 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5397 - acc: 0.7954 - val_loss: 0.6840 - val_acc: 0.7338 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5297 - acc: 0.8090 - val_loss: 0.6823 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5260 - acc: 0.8142 - val_loss: 0.6679 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5300 - acc: 0.8081 - val_loss: 0.6677 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5312 - acc: 0.8078 - val_loss: 0.6782 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5402 - acc: 0.8033 - val_loss: 0.6859 - val_acc: 0.7321 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5349 - acc: 0.7984 - val_loss: 0.6795 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5328 - acc: 0.8026 - val_loss: 0.6714 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5255 - acc: 0.8099 - val_loss: 0.6700 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5202 - acc: 0.8087 - val_loss: 0.6728 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5318 - acc: 0.8060 - val_loss: 0.6745 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5281 - acc: 0.8120 - val_loss: 0.6640 - val_acc: 0.7455 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5308 - acc: 0.8099 - val_loss: 0.6774 - val_acc: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5356 - acc: 0.8075 - val_loss: 0.6744 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5364 - acc: 0.8130 - val_loss: 0.6751 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5272 - acc: 0.8063 - val_loss: 0.6740 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5318 - acc: 0.7996 - val_loss: 0.6737 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5273 - acc: 0.8130 - val_loss: 0.7047 - val_acc: 0.7315 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5386 - acc: 0.8024 - val_loss: 0.6655 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5271 - acc: 0.8069 - val_loss: 0.6831 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5301 - acc: 0.8039 - val_loss: 0.6681 - val_acc: 0.7432 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5373 - acc: 0.7945 - val_loss: 0.6729 - val_acc: 0.7408 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5331 - acc: 0.8033 - val_loss: 0.6660 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5441 - acc: 0.8021 - val_loss: 0.6920 - val_acc: 0.7292 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5333 - acc: 0.8051 - val_loss: 0.6677 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5265 - acc: 0.8105 - val_loss: 0.6698 - val_acc: 0.7350 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5305 - acc: 0.8130 - val_loss: 0.6654 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5221 - acc: 0.8172 - val_loss: 0.6768 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5248 - acc: 0.8048 - val_loss: 0.6636 - val_acc: 0.7362 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5366 - acc: 0.8012 - val_loss: 0.6776 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5233 - acc: 0.8127 - val_loss: 0.6621 - val_acc: 0.7478 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5214 - acc: 0.8114 - val_loss: 0.6769 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5336 - acc: 0.8090 - val_loss: 0.6710 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5223 - acc: 0.8148 - val_loss: 0.6698 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5359 - acc: 0.8069 - val_loss: 0.7137 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5353 - acc: 0.8045 - val_loss: 0.6766 - val_acc: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 96ms/step - loss: 0.5257 - acc: 0.8110 - val_loss: 0.6635 - val_acc: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5282 - acc: 0.8042 - val_loss: 0.6792 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5294 - acc: 0.7984 - val_loss: 0.6709 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5403 - acc: 0.8008 - val_loss: 0.6677 - val_acc: 0.7467 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5323 - acc: 0.7972 - val_loss: 0.6642 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5238 - acc: 0.8160 - val_loss: 0.6664 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5217 - acc: 0.8181 - val_loss: 0.6664 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5229 - acc: 0.8142 - val_loss: 0.6737 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5262 - acc: 0.8117 - val_loss: 0.6678 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5304 - acc: 0.8051 - val_loss: 0.6636 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5360 - acc: 0.8084 - val_loss: 0.6722 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5318 - acc: 0.7987 - val_loss: 0.6776 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5339 - acc: 0.8005 - val_loss: 0.6684 - val_acc: 0.7443 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5232 - acc: 0.8127 - val_loss: 0.6663 - val_acc: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5326 - acc: 0.8075 - val_loss: 0.6645 - val_acc: 0.7455 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5290 - acc: 0.8090 - val_loss: 0.6849 - val_acc: 0.7356 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5276 - acc: 0.8096 - val_loss: 0.6791 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5278 - acc: 0.8087 - val_loss: 0.6742 - val_acc: 0.7350 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5247 - acc: 0.8069 - val_loss: 0.6666 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5259 - acc: 0.8072 - val_loss: 0.6637 - val_acc: 0.7472 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5340 - acc: 0.8021 - val_loss: 0.6714 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5389 - acc: 0.8030 - val_loss: 0.6666 - val_acc: 0.7420 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5253 - acc: 0.8108 - val_loss: 0.6749 - val_acc: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5381 - acc: 0.8042 - val_loss: 0.6703 - val_acc: 0.7443 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5289 - acc: 0.8036 - val_loss: 0.6673 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5339 - acc: 0.7999 - val_loss: 0.6751 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5149 - acc: 0.8145 - val_loss: 0.6650 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5225 - acc: 0.8105 - val_loss: 0.6636 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5301 - acc: 0.8066 - val_loss: 0.6740 - val_acc: 0.7432 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5266 - acc: 0.8130 - val_loss: 0.6728 - val_acc: 0.7443 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5243 - acc: 0.8081 - val_loss: 0.6739 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5282 - acc: 0.8072 - val_loss: 0.6673 - val_acc: 0.7362 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5314 - acc: 0.8074 - val_loss: 0.6630 - val_acc: 0.7484 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5369 - acc: 0.8030 - val_loss: 0.6683 - val_acc: 0.7432 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5304 - acc: 0.8042 - val_loss: 0.6651 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5290 - acc: 0.8021 - val_loss: 0.6687 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5330 - acc: 0.8008 - val_loss: 0.6640 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5327 - acc: 0.8048 - val_loss: 0.6678 - val_acc: 0.7478 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5343 - acc: 0.8012 - val_loss: 0.6908 - val_acc: 0.7338 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5337 - acc: 0.8039 - val_loss: 0.6632 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5241 - acc: 0.8096 - val_loss: 0.6670 - val_acc: 0.7426 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5269 - acc: 0.8081 - val_loss: 0.6670 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 96ms/step - loss: 0.5329 - acc: 0.8033 - val_loss: 0.6702 - val_acc: 0.7408 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5399 - acc: 0.8012 - val_loss: 0.6743 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5228 - acc: 0.8096 - val_loss: 0.6683 - val_acc: 0.7432 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5259 - acc: 0.8105 - val_loss: 0.6617 - val_acc: 0.7443 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5297 - acc: 0.8054 - val_loss: 0.6621 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5259 - acc: 0.8075 - val_loss: 0.6853 - val_acc: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5383 - acc: 0.8030 - val_loss: 0.6726 - val_acc: 0.7373 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5266 - acc: 0.8090 - val_loss: 0.6689 - val_acc: 0.7397 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5325 - acc: 0.8036 - val_loss: 0.7031 - val_acc: 0.7268 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5121 - acc: 0.8164 - val_loss: 0.6646 - val_acc: 0.7461 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5322 - acc: 0.7978 - val_loss: 0.6648 - val_acc: 0.7449 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5244 - acc: 0.8030 - val_loss: 0.6757 - val_acc: 0.7350 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5195 - acc: 0.8030 - val_loss: 0.6636 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5296 - acc: 0.8078 - val_loss: 0.6709 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5172 - acc: 0.8093 - val_loss: 0.6789 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5196 - acc: 0.8102 - val_loss: 0.6667 - val_acc: 0.7362 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5225 - acc: 0.8102 - val_loss: 0.6705 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5249 - acc: 0.8057 - val_loss: 0.6781 - val_acc: 0.7350 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5259 - acc: 0.8048 - val_loss: 0.6624 - val_acc: 0.7467 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5215 - acc: 0.8084 - val_loss: 0.6677 - val_acc: 0.7402 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5223 - acc: 0.8099 - val_loss: 0.6610 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5316 - acc: 0.8054 - val_loss: 0.6585 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5421 - acc: 0.8012 - val_loss: 0.6691 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5265 - acc: 0.8063 - val_loss: 0.6806 - val_acc: 0.7368 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5163 - acc: 0.8039 - val_loss: 0.6710 - val_acc: 0.7414 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5263 - acc: 0.8072 - val_loss: 0.6839 - val_acc: 0.7338 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5179 - acc: 0.8172 - val_loss: 0.6675 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5312 - acc: 0.8057 - val_loss: 0.6664 - val_acc: 0.7443 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5277 - acc: 0.8084 - val_loss: 0.6681 - val_acc: 0.7472 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5375 - acc: 0.8063 - val_loss: 0.6631 - val_acc: 0.7472 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5306 - acc: 0.8072 - val_loss: 0.6705 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5224 - acc: 0.8066 - val_loss: 0.6756 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5272 - acc: 0.8090 - val_loss: 0.6667 - val_acc: 0.7385 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5274 - acc: 0.8045 - val_loss: 0.6706 - val_acc: 0.7391 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.6512 - acc: 0.7574\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.7611 - acc: 0.7138\n",
            "epsilon: 0.003 and test evaluation : 0.7610782980918884, 0.7137870788574219\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.8404 - acc: 0.6754\n",
            "epsilon: 0.005 and test evaluation : 0.8403658270835876, 0.6753926873207092\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0543 - acc: 0.5777\n",
            "epsilon: 0.01 and test evaluation : 1.0543460845947266, 0.5776614546775818\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.5116 - acc: 0.4555\n",
            "epsilon: 0.02 and test evaluation : 1.5115708112716675, 0.4554973840713501\n",
            "SNR: 33.75504732131958\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 3s 130ms/step - loss: 1.4702 - acc: 0.2965 - val_loss: 1.4345 - val_acc: 0.3450 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3793 - acc: 0.3728 - val_loss: 1.3534 - val_acc: 0.3689 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3265 - acc: 0.3852 - val_loss: 1.3953 - val_acc: 0.3217 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2967 - acc: 0.4039 - val_loss: 1.3309 - val_acc: 0.3625 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2781 - acc: 0.4333 - val_loss: 1.2951 - val_acc: 0.3881 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2334 - acc: 0.4681 - val_loss: 1.7496 - val_acc: 0.2471 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2174 - acc: 0.4811 - val_loss: 1.3782 - val_acc: 0.3578 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1816 - acc: 0.5183 - val_loss: 1.1695 - val_acc: 0.5146 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1453 - acc: 0.5280 - val_loss: 1.4268 - val_acc: 0.4266 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1520 - acc: 0.5319 - val_loss: 1.2823 - val_acc: 0.4668 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0815 - acc: 0.5685 - val_loss: 1.9115 - val_acc: 0.2931 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0664 - acc: 0.5761 - val_loss: 1.2795 - val_acc: 0.5023 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0195 - acc: 0.5955 - val_loss: 1.2906 - val_acc: 0.4907 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9734 - acc: 0.6130 - val_loss: 2.2041 - val_acc: 0.3281 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9412 - acc: 0.6239 - val_loss: 0.9762 - val_acc: 0.6002 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9242 - acc: 0.6345 - val_loss: 0.9890 - val_acc: 0.5985 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8691 - acc: 0.6584 - val_loss: 0.9183 - val_acc: 0.6492 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8496 - acc: 0.6663 - val_loss: 0.9481 - val_acc: 0.6364 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8343 - acc: 0.6741 - val_loss: 0.9664 - val_acc: 0.6661 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8346 - acc: 0.6753 - val_loss: 0.9756 - val_acc: 0.6177 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7833 - acc: 0.7035 - val_loss: 0.9206 - val_acc: 0.6422 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.7750 - acc: 0.7089 - val_loss: 0.8988 - val_acc: 0.6562 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7610 - acc: 0.7077 - val_loss: 1.2834 - val_acc: 0.5437 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7451 - acc: 0.7171 - val_loss: 0.8557 - val_acc: 0.6515 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7331 - acc: 0.7210 - val_loss: 0.7924 - val_acc: 0.6981 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7096 - acc: 0.7319 - val_loss: 1.1127 - val_acc: 0.5909 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7056 - acc: 0.7371 - val_loss: 0.9246 - val_acc: 0.6579 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7008 - acc: 0.7371 - val_loss: 0.8840 - val_acc: 0.6684 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6951 - acc: 0.7328 - val_loss: 0.8921 - val_acc: 0.6585 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6738 - acc: 0.7504 - val_loss: 1.0404 - val_acc: 0.6340 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6810 - acc: 0.7425 - val_loss: 0.7992 - val_acc: 0.6993 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6483 - acc: 0.7649 - val_loss: 0.8221 - val_acc: 0.6964 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6149 - acc: 0.7661 - val_loss: 0.7873 - val_acc: 0.7127 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6026 - acc: 0.7764 - val_loss: 0.7650 - val_acc: 0.7174 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5871 - acc: 0.7855 - val_loss: 0.7735 - val_acc: 0.7156 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5777 - acc: 0.7894 - val_loss: 0.7861 - val_acc: 0.7168 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5894 - acc: 0.7797 - val_loss: 0.7375 - val_acc: 0.7314 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5675 - acc: 0.7840 - val_loss: 0.7401 - val_acc: 0.7267 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5745 - acc: 0.7870 - val_loss: 0.7131 - val_acc: 0.7284 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5587 - acc: 0.7939 - val_loss: 0.7363 - val_acc: 0.7314 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5486 - acc: 0.8033 - val_loss: 0.7517 - val_acc: 0.7267 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5427 - acc: 0.8073 - val_loss: 0.7271 - val_acc: 0.7325 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5536 - acc: 0.7979 - val_loss: 0.7291 - val_acc: 0.7337 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5479 - acc: 0.8021 - val_loss: 0.7129 - val_acc: 0.7331 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5563 - acc: 0.7982 - val_loss: 0.7655 - val_acc: 0.7284 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5386 - acc: 0.8042 - val_loss: 0.7239 - val_acc: 0.7343 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5451 - acc: 0.8033 - val_loss: 0.7307 - val_acc: 0.7314 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5388 - acc: 0.8085 - val_loss: 0.7068 - val_acc: 0.7354 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5337 - acc: 0.8070 - val_loss: 0.7233 - val_acc: 0.7372 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5293 - acc: 0.8036 - val_loss: 0.7055 - val_acc: 0.7354 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5220 - acc: 0.8191 - val_loss: 0.7739 - val_acc: 0.7238 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5219 - acc: 0.8109 - val_loss: 0.8022 - val_acc: 0.7168 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5200 - acc: 0.8106 - val_loss: 0.7142 - val_acc: 0.7372 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5173 - acc: 0.8212 - val_loss: 0.7364 - val_acc: 0.7319 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5211 - acc: 0.8130 - val_loss: 0.7234 - val_acc: 0.7372 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5199 - acc: 0.8097 - val_loss: 0.7225 - val_acc: 0.7354 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5181 - acc: 0.8191 - val_loss: 0.7350 - val_acc: 0.7337 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5171 - acc: 0.8136 - val_loss: 0.7224 - val_acc: 0.7424 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5059 - acc: 0.8230 - val_loss: 0.7494 - val_acc: 0.7331 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5175 - acc: 0.8130 - val_loss: 0.6961 - val_acc: 0.7413 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5139 - acc: 0.8130 - val_loss: 0.7265 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5042 - acc: 0.8158 - val_loss: 0.7060 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5105 - acc: 0.8085 - val_loss: 0.7063 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5129 - acc: 0.8127 - val_loss: 0.7366 - val_acc: 0.7354 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4979 - acc: 0.8158 - val_loss: 0.7248 - val_acc: 0.7337 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5043 - acc: 0.8209 - val_loss: 0.7499 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5054 - acc: 0.8112 - val_loss: 0.7008 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5081 - acc: 0.8133 - val_loss: 0.7127 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5132 - acc: 0.8082 - val_loss: 0.7187 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5159 - acc: 0.8160 - val_loss: 0.6940 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5164 - acc: 0.8127 - val_loss: 0.6941 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5145 - acc: 0.8163 - val_loss: 0.7050 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5068 - acc: 0.8100 - val_loss: 0.7100 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5120 - acc: 0.8118 - val_loss: 0.7601 - val_acc: 0.7314 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5110 - acc: 0.8154 - val_loss: 0.6946 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5160 - acc: 0.8097 - val_loss: 0.6882 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5153 - acc: 0.8122 - val_loss: 0.7392 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5049 - acc: 0.8115 - val_loss: 0.7212 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.4920 - acc: 0.8263 - val_loss: 0.7428 - val_acc: 0.7325 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5114 - acc: 0.8103 - val_loss: 0.6984 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5116 - acc: 0.8100 - val_loss: 0.7025 - val_acc: 0.7354 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5005 - acc: 0.8185 - val_loss: 0.7162 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5096 - acc: 0.8185 - val_loss: 0.7050 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5026 - acc: 0.8158 - val_loss: 0.7399 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5165 - acc: 0.8115 - val_loss: 0.7136 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5005 - acc: 0.8169 - val_loss: 0.7143 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4975 - acc: 0.8203 - val_loss: 0.7047 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5117 - acc: 0.8145 - val_loss: 0.7149 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5005 - acc: 0.8206 - val_loss: 0.7185 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5066 - acc: 0.8200 - val_loss: 0.7107 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5041 - acc: 0.8242 - val_loss: 0.7130 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5079 - acc: 0.8133 - val_loss: 0.6918 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5103 - acc: 0.8136 - val_loss: 0.7137 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5033 - acc: 0.8179 - val_loss: 0.7208 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5091 - acc: 0.8082 - val_loss: 0.6967 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5019 - acc: 0.8154 - val_loss: 0.7144 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5042 - acc: 0.8100 - val_loss: 0.7256 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5040 - acc: 0.8166 - val_loss: 0.7069 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5087 - acc: 0.8094 - val_loss: 0.6864 - val_acc: 0.7448 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5049 - acc: 0.8172 - val_loss: 0.7366 - val_acc: 0.7325 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4964 - acc: 0.8254 - val_loss: 0.7499 - val_acc: 0.7319 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5065 - acc: 0.8121 - val_loss: 0.7246 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5012 - acc: 0.8175 - val_loss: 0.7220 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4993 - acc: 0.8194 - val_loss: 0.6954 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5014 - acc: 0.8179 - val_loss: 0.6904 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5081 - acc: 0.8155 - val_loss: 0.6820 - val_acc: 0.7430 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5095 - acc: 0.8137 - val_loss: 0.7201 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5192 - acc: 0.8051 - val_loss: 0.7098 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5020 - acc: 0.8197 - val_loss: 0.7463 - val_acc: 0.7302 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5098 - acc: 0.8130 - val_loss: 0.7139 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5072 - acc: 0.8118 - val_loss: 0.7045 - val_acc: 0.7401 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5008 - acc: 0.8136 - val_loss: 0.7287 - val_acc: 0.7354 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5079 - acc: 0.8070 - val_loss: 0.7207 - val_acc: 0.7354 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5110 - acc: 0.8076 - val_loss: 0.6875 - val_acc: 0.7354 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5146 - acc: 0.8115 - val_loss: 0.7327 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5057 - acc: 0.8175 - val_loss: 0.7203 - val_acc: 0.7314 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5203 - acc: 0.8051 - val_loss: 0.6920 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4929 - acc: 0.8197 - val_loss: 0.6960 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5085 - acc: 0.8088 - val_loss: 0.6910 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5050 - acc: 0.8169 - val_loss: 0.6981 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4981 - acc: 0.8209 - val_loss: 0.6978 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5151 - acc: 0.8148 - val_loss: 0.6948 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5097 - acc: 0.8136 - val_loss: 0.7700 - val_acc: 0.7261 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4944 - acc: 0.8169 - val_loss: 0.7100 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5085 - acc: 0.8124 - val_loss: 0.7171 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5087 - acc: 0.8121 - val_loss: 0.6996 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4967 - acc: 0.8166 - val_loss: 0.6865 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5101 - acc: 0.8106 - val_loss: 0.7180 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4953 - acc: 0.8169 - val_loss: 0.7017 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5118 - acc: 0.8154 - val_loss: 0.6931 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5085 - acc: 0.8154 - val_loss: 0.7090 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4924 - acc: 0.8254 - val_loss: 0.7402 - val_acc: 0.7325 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5060 - acc: 0.8175 - val_loss: 0.7299 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5079 - acc: 0.8133 - val_loss: 0.6910 - val_acc: 0.7430 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5079 - acc: 0.8094 - val_loss: 0.7566 - val_acc: 0.7279 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4961 - acc: 0.8203 - val_loss: 0.7432 - val_acc: 0.7273 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5002 - acc: 0.8121 - val_loss: 0.7004 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5092 - acc: 0.8148 - val_loss: 0.7372 - val_acc: 0.7308 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5090 - acc: 0.8118 - val_loss: 0.7050 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5175 - acc: 0.8130 - val_loss: 0.7122 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5035 - acc: 0.8163 - val_loss: 0.7037 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5078 - acc: 0.8148 - val_loss: 0.6956 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5002 - acc: 0.8136 - val_loss: 0.7336 - val_acc: 0.7319 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4967 - acc: 0.8242 - val_loss: 0.6831 - val_acc: 0.7430 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5030 - acc: 0.8185 - val_loss: 0.7076 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5088 - acc: 0.8127 - val_loss: 0.7170 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5050 - acc: 0.8088 - val_loss: 0.6947 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5077 - acc: 0.8130 - val_loss: 0.6983 - val_acc: 0.7424 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5143 - acc: 0.8088 - val_loss: 0.7596 - val_acc: 0.7255 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4987 - acc: 0.8188 - val_loss: 0.7409 - val_acc: 0.7308 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.4989 - acc: 0.8200 - val_loss: 0.6927 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4950 - acc: 0.8148 - val_loss: 0.7458 - val_acc: 0.7290 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5187 - acc: 0.8054 - val_loss: 0.6984 - val_acc: 0.7401 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5085 - acc: 0.8163 - val_loss: 0.6804 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5048 - acc: 0.8218 - val_loss: 0.7262 - val_acc: 0.7343 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5103 - acc: 0.8121 - val_loss: 0.6942 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5089 - acc: 0.8169 - val_loss: 0.7000 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4992 - acc: 0.8145 - val_loss: 0.7169 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5079 - acc: 0.8140 - val_loss: 0.7524 - val_acc: 0.7267 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5101 - acc: 0.8151 - val_loss: 0.7092 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5023 - acc: 0.8045 - val_loss: 0.6934 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5055 - acc: 0.8148 - val_loss: 0.6834 - val_acc: 0.7448 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5145 - acc: 0.8061 - val_loss: 0.7261 - val_acc: 0.7337 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5058 - acc: 0.8085 - val_loss: 0.7094 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5142 - acc: 0.8176 - val_loss: 0.7345 - val_acc: 0.7331 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5124 - acc: 0.8142 - val_loss: 0.7091 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5099 - acc: 0.8115 - val_loss: 0.7333 - val_acc: 0.7308 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5059 - acc: 0.8139 - val_loss: 0.6941 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5082 - acc: 0.8100 - val_loss: 0.7659 - val_acc: 0.7209 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5028 - acc: 0.8172 - val_loss: 0.6977 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5099 - acc: 0.8064 - val_loss: 0.7137 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5029 - acc: 0.8070 - val_loss: 0.7249 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5048 - acc: 0.8209 - val_loss: 0.7087 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5015 - acc: 0.8185 - val_loss: 0.7321 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4975 - acc: 0.8169 - val_loss: 0.7169 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5069 - acc: 0.8112 - val_loss: 0.7227 - val_acc: 0.7331 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5028 - acc: 0.8169 - val_loss: 0.7391 - val_acc: 0.7314 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 96ms/step - loss: 0.5005 - acc: 0.8212 - val_loss: 0.6868 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5202 - acc: 0.8067 - val_loss: 0.7080 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5056 - acc: 0.8169 - val_loss: 0.7468 - val_acc: 0.7308 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4923 - acc: 0.8185 - val_loss: 0.6935 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4987 - acc: 0.8221 - val_loss: 0.6921 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5048 - acc: 0.8124 - val_loss: 0.6993 - val_acc: 0.7366 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5112 - acc: 0.8045 - val_loss: 0.6961 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5062 - acc: 0.8079 - val_loss: 0.6875 - val_acc: 0.7442 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5079 - acc: 0.8145 - val_loss: 0.6893 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4959 - acc: 0.8166 - val_loss: 0.7178 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5084 - acc: 0.8082 - val_loss: 0.6961 - val_acc: 0.7378 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4899 - acc: 0.8248 - val_loss: 0.7183 - val_acc: 0.7325 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.4991 - acc: 0.8209 - val_loss: 0.7465 - val_acc: 0.7302 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4995 - acc: 0.8172 - val_loss: 0.7167 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.4940 - acc: 0.8160 - val_loss: 0.6872 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5002 - acc: 0.8151 - val_loss: 0.6832 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5067 - acc: 0.8154 - val_loss: 0.6956 - val_acc: 0.7372 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5039 - acc: 0.8200 - val_loss: 0.6899 - val_acc: 0.7424 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5028 - acc: 0.8127 - val_loss: 0.6888 - val_acc: 0.7407 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5170 - acc: 0.8121 - val_loss: 0.6994 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5001 - acc: 0.8148 - val_loss: 0.7091 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.4966 - acc: 0.8233 - val_loss: 0.6951 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5057 - acc: 0.8163 - val_loss: 0.6959 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6474 - acc: 0.7627\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7638 - acc: 0.7051\n",
            "epsilon: 0.003 and test evaluation : 0.7638108134269714, 0.7050610780715942\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.8490 - acc: 0.6824\n",
            "epsilon: 0.005 and test evaluation : 0.8489747643470764, 0.6823734641075134\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0800 - acc: 0.5986\n",
            "epsilon: 0.01 and test evaluation : 1.0799797773361206, 0.5986038446426392\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.5736 - acc: 0.4607\n",
            "epsilon: 0.02 and test evaluation : 1.573563814163208, 0.46073299646377563\n",
            "SNR: 33.75504732131958\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 3s 110ms/step - loss: 1.4749 - acc: 0.2699 - val_loss: 1.4493 - val_acc: 0.3479 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 1.4047 - acc: 0.3561 - val_loss: 1.3949 - val_acc: 0.3386 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3459 - acc: 0.3643 - val_loss: 1.3199 - val_acc: 0.3805 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.3168 - acc: 0.3785 - val_loss: 1.3194 - val_acc: 0.3794 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3055 - acc: 0.3752 - val_loss: 1.2809 - val_acc: 0.4172 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2861 - acc: 0.4073 - val_loss: 1.4816 - val_acc: 0.3677 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2795 - acc: 0.4054 - val_loss: 2.0034 - val_acc: 0.2751 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2364 - acc: 0.4711 - val_loss: 1.3962 - val_acc: 0.3654 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2052 - acc: 0.4799 - val_loss: 1.1194 - val_acc: 0.5659 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 1.1712 - acc: 0.5068 - val_loss: 3.2292 - val_acc: 0.3852 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1349 - acc: 0.5228 - val_loss: 1.1668 - val_acc: 0.5029 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1108 - acc: 0.5349 - val_loss: 1.5370 - val_acc: 0.3689 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0978 - acc: 0.5452 - val_loss: 1.0755 - val_acc: 0.5554 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0492 - acc: 0.5649 - val_loss: 1.0101 - val_acc: 0.5997 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0418 - acc: 0.5679 - val_loss: 1.0839 - val_acc: 0.5554 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0059 - acc: 0.5967 - val_loss: 0.9593 - val_acc: 0.6166 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9664 - acc: 0.6157 - val_loss: 1.0910 - val_acc: 0.5344 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9570 - acc: 0.6139 - val_loss: 1.0677 - val_acc: 0.5647 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9262 - acc: 0.6321 - val_loss: 0.9051 - val_acc: 0.6562 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8846 - acc: 0.6584 - val_loss: 1.1431 - val_acc: 0.5565 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8680 - acc: 0.6596 - val_loss: 0.7879 - val_acc: 0.7016 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8315 - acc: 0.6794 - val_loss: 0.8010 - val_acc: 0.6900 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8539 - acc: 0.6587 - val_loss: 0.9336 - val_acc: 0.6329 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8130 - acc: 0.6865 - val_loss: 0.9682 - val_acc: 0.6608 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7856 - acc: 0.6908 - val_loss: 1.2476 - val_acc: 0.5897 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7812 - acc: 0.6959 - val_loss: 0.9591 - val_acc: 0.6434 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.7547 - acc: 0.7174 - val_loss: 0.7613 - val_acc: 0.7273 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7488 - acc: 0.7135 - val_loss: 0.8075 - val_acc: 0.7028 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7473 - acc: 0.7089 - val_loss: 0.7234 - val_acc: 0.7337 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7453 - acc: 0.7177 - val_loss: 0.7932 - val_acc: 0.6981 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7659 - acc: 0.7159 - val_loss: 0.7669 - val_acc: 0.7121 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7216 - acc: 0.7247 - val_loss: 0.7369 - val_acc: 0.7319 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.6877 - acc: 0.7380 - val_loss: 0.7123 - val_acc: 0.7395 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6611 - acc: 0.7528 - val_loss: 0.6967 - val_acc: 0.7471 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6389 - acc: 0.7549 - val_loss: 0.6956 - val_acc: 0.7442 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6360 - acc: 0.7631 - val_loss: 0.6753 - val_acc: 0.7442 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6424 - acc: 0.7679 - val_loss: 0.7033 - val_acc: 0.7494 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6182 - acc: 0.7782 - val_loss: 0.6653 - val_acc: 0.7541 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6136 - acc: 0.7749 - val_loss: 0.7134 - val_acc: 0.7488 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6159 - acc: 0.7806 - val_loss: 0.6812 - val_acc: 0.7483 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6135 - acc: 0.7752 - val_loss: 0.6633 - val_acc: 0.7570 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6028 - acc: 0.7770 - val_loss: 0.6685 - val_acc: 0.7582 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5998 - acc: 0.7812 - val_loss: 0.7034 - val_acc: 0.7453 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5972 - acc: 0.7837 - val_loss: 0.6518 - val_acc: 0.7552 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6034 - acc: 0.7840 - val_loss: 0.6442 - val_acc: 0.7582 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5855 - acc: 0.7852 - val_loss: 0.6695 - val_acc: 0.7547 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5849 - acc: 0.7831 - val_loss: 0.6438 - val_acc: 0.7634 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5808 - acc: 0.7879 - val_loss: 0.6623 - val_acc: 0.7570 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5800 - acc: 0.7900 - val_loss: 0.6602 - val_acc: 0.7593 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5896 - acc: 0.7852 - val_loss: 0.6553 - val_acc: 0.7576 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5670 - acc: 0.7997 - val_loss: 0.6431 - val_acc: 0.7599 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5739 - acc: 0.7909 - val_loss: 0.6377 - val_acc: 0.7652 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5712 - acc: 0.7958 - val_loss: 0.6388 - val_acc: 0.7669 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5763 - acc: 0.7812 - val_loss: 0.6355 - val_acc: 0.7669 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5559 - acc: 0.8003 - val_loss: 0.6281 - val_acc: 0.7681 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5642 - acc: 0.7967 - val_loss: 0.6551 - val_acc: 0.7570 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5604 - acc: 0.7970 - val_loss: 0.6428 - val_acc: 0.7646 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5553 - acc: 0.7949 - val_loss: 0.6409 - val_acc: 0.7657 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5509 - acc: 0.8021 - val_loss: 0.6427 - val_acc: 0.7657 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5508 - acc: 0.8009 - val_loss: 0.6409 - val_acc: 0.7681 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5537 - acc: 0.8045 - val_loss: 0.6298 - val_acc: 0.7745 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5626 - acc: 0.7930 - val_loss: 0.6360 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5515 - acc: 0.8006 - val_loss: 0.6150 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5456 - acc: 0.8073 - val_loss: 0.6324 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5379 - acc: 0.8067 - val_loss: 0.6497 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5487 - acc: 0.8003 - val_loss: 0.6482 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5469 - acc: 0.7994 - val_loss: 0.6262 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5592 - acc: 0.7939 - val_loss: 0.6594 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5558 - acc: 0.8003 - val_loss: 0.6264 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5565 - acc: 0.7930 - val_loss: 0.6293 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5463 - acc: 0.8009 - val_loss: 0.6246 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5443 - acc: 0.7994 - val_loss: 0.7008 - val_acc: 0.7430 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5577 - acc: 0.7967 - val_loss: 0.6269 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5387 - acc: 0.8012 - val_loss: 0.6282 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5492 - acc: 0.8015 - val_loss: 0.6278 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5459 - acc: 0.8021 - val_loss: 0.6353 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5449 - acc: 0.8030 - val_loss: 0.6367 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5525 - acc: 0.7964 - val_loss: 0.6253 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5530 - acc: 0.8012 - val_loss: 0.6401 - val_acc: 0.7622 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5468 - acc: 0.8033 - val_loss: 0.6338 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5484 - acc: 0.8012 - val_loss: 0.6583 - val_acc: 0.7640 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5510 - acc: 0.7958 - val_loss: 0.6241 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5537 - acc: 0.7964 - val_loss: 0.6576 - val_acc: 0.7599 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5571 - acc: 0.7943 - val_loss: 0.6795 - val_acc: 0.7483 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5546 - acc: 0.7939 - val_loss: 0.6176 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5451 - acc: 0.7949 - val_loss: 0.6620 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5450 - acc: 0.8024 - val_loss: 0.6256 - val_acc: 0.7716 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5425 - acc: 0.8024 - val_loss: 0.6276 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5478 - acc: 0.7997 - val_loss: 0.6228 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5522 - acc: 0.7964 - val_loss: 0.6310 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5537 - acc: 0.7958 - val_loss: 0.6330 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5432 - acc: 0.7994 - val_loss: 0.6375 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5434 - acc: 0.7994 - val_loss: 0.6321 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5480 - acc: 0.7927 - val_loss: 0.6627 - val_acc: 0.7535 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5471 - acc: 0.7943 - val_loss: 0.6185 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5448 - acc: 0.8015 - val_loss: 0.6551 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5475 - acc: 0.7991 - val_loss: 0.6285 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5447 - acc: 0.7994 - val_loss: 0.6521 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5450 - acc: 0.7997 - val_loss: 0.6281 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5524 - acc: 0.7903 - val_loss: 0.6515 - val_acc: 0.7640 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5552 - acc: 0.7897 - val_loss: 0.6714 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5505 - acc: 0.7964 - val_loss: 0.6340 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5483 - acc: 0.7958 - val_loss: 0.6398 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5413 - acc: 0.8054 - val_loss: 0.6817 - val_acc: 0.7465 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5465 - acc: 0.7991 - val_loss: 0.6242 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5510 - acc: 0.7894 - val_loss: 0.6235 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5421 - acc: 0.7994 - val_loss: 0.6370 - val_acc: 0.7622 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5448 - acc: 0.7952 - val_loss: 0.6478 - val_acc: 0.7646 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5418 - acc: 0.8009 - val_loss: 0.6445 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5482 - acc: 0.7970 - val_loss: 0.6242 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5427 - acc: 0.8006 - val_loss: 0.6282 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5535 - acc: 0.7909 - val_loss: 0.6287 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5403 - acc: 0.8036 - val_loss: 0.6178 - val_acc: 0.7710 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5483 - acc: 0.8009 - val_loss: 0.6272 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5401 - acc: 0.8009 - val_loss: 0.6324 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5505 - acc: 0.7970 - val_loss: 0.6281 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5475 - acc: 0.8024 - val_loss: 0.6526 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5452 - acc: 0.8012 - val_loss: 0.6269 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5353 - acc: 0.8003 - val_loss: 0.6219 - val_acc: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5426 - acc: 0.7994 - val_loss: 0.6349 - val_acc: 0.7652 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5511 - acc: 0.7933 - val_loss: 0.6272 - val_acc: 0.7652 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5449 - acc: 0.7946 - val_loss: 0.6225 - val_acc: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5479 - acc: 0.7927 - val_loss: 0.6261 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5530 - acc: 0.7930 - val_loss: 0.6422 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5516 - acc: 0.7933 - val_loss: 0.6200 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5506 - acc: 0.7943 - val_loss: 0.6276 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5514 - acc: 0.7958 - val_loss: 0.6218 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5476 - acc: 0.7979 - val_loss: 0.6431 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5477 - acc: 0.7955 - val_loss: 0.6521 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5379 - acc: 0.8070 - val_loss: 0.6422 - val_acc: 0.7652 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5457 - acc: 0.7955 - val_loss: 0.6259 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5533 - acc: 0.7933 - val_loss: 0.6442 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5545 - acc: 0.7915 - val_loss: 0.6212 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5468 - acc: 0.7964 - val_loss: 0.6237 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5452 - acc: 0.7973 - val_loss: 0.6482 - val_acc: 0.7599 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5574 - acc: 0.7943 - val_loss: 0.6307 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5358 - acc: 0.8048 - val_loss: 0.6407 - val_acc: 0.7611 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5396 - acc: 0.7991 - val_loss: 0.6329 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5397 - acc: 0.8033 - val_loss: 0.6139 - val_acc: 0.7739 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5370 - acc: 0.7988 - val_loss: 0.6166 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5434 - acc: 0.8033 - val_loss: 0.6301 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5331 - acc: 0.8012 - val_loss: 0.6211 - val_acc: 0.7739 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5501 - acc: 0.7939 - val_loss: 0.6288 - val_acc: 0.7710 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5355 - acc: 0.8036 - val_loss: 0.6346 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5330 - acc: 0.8067 - val_loss: 0.6265 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5434 - acc: 0.7979 - val_loss: 0.6210 - val_acc: 0.7646 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5488 - acc: 0.7936 - val_loss: 0.6177 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5373 - acc: 0.8051 - val_loss: 0.6436 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5419 - acc: 0.8003 - val_loss: 0.6328 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5440 - acc: 0.7973 - val_loss: 0.6327 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5472 - acc: 0.7967 - val_loss: 0.6173 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5503 - acc: 0.7903 - val_loss: 0.6177 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5535 - acc: 0.8009 - val_loss: 0.6285 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5454 - acc: 0.7924 - val_loss: 0.6309 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5442 - acc: 0.8003 - val_loss: 0.6268 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5557 - acc: 0.7933 - val_loss: 0.6147 - val_acc: 0.7739 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5471 - acc: 0.7994 - val_loss: 0.6313 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5402 - acc: 0.8053 - val_loss: 0.6116 - val_acc: 0.7710 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5522 - acc: 0.7982 - val_loss: 0.6143 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5553 - acc: 0.7891 - val_loss: 0.6313 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5398 - acc: 0.8024 - val_loss: 0.6400 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5503 - acc: 0.7967 - val_loss: 0.6276 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5403 - acc: 0.7948 - val_loss: 0.6219 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5346 - acc: 0.7994 - val_loss: 0.6412 - val_acc: 0.7593 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5398 - acc: 0.8039 - val_loss: 0.6263 - val_acc: 0.7733 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5482 - acc: 0.7876 - val_loss: 0.6407 - val_acc: 0.7686 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5459 - acc: 0.7964 - val_loss: 0.6249 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5343 - acc: 0.8054 - val_loss: 0.6348 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5424 - acc: 0.7970 - val_loss: 0.6156 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5435 - acc: 0.8003 - val_loss: 0.6161 - val_acc: 0.7716 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5416 - acc: 0.8003 - val_loss: 0.6330 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5421 - acc: 0.8024 - val_loss: 0.6340 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5435 - acc: 0.7979 - val_loss: 0.6314 - val_acc: 0.7652 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5454 - acc: 0.8012 - val_loss: 0.6250 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5341 - acc: 0.8024 - val_loss: 0.6276 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5430 - acc: 0.7961 - val_loss: 0.6248 - val_acc: 0.7692 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5400 - acc: 0.7994 - val_loss: 0.6228 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5464 - acc: 0.7955 - val_loss: 0.6353 - val_acc: 0.7663 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5421 - acc: 0.7967 - val_loss: 0.6234 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5350 - acc: 0.8024 - val_loss: 0.6178 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5503 - acc: 0.7873 - val_loss: 0.6470 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5503 - acc: 0.7973 - val_loss: 0.6168 - val_acc: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5416 - acc: 0.7961 - val_loss: 0.6151 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5377 - acc: 0.8006 - val_loss: 0.6193 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5370 - acc: 0.8057 - val_loss: 0.6685 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5371 - acc: 0.8021 - val_loss: 0.6263 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5338 - acc: 0.8048 - val_loss: 0.6179 - val_acc: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5366 - acc: 0.8000 - val_loss: 0.6508 - val_acc: 0.7617 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5472 - acc: 0.7933 - val_loss: 0.6225 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5451 - acc: 0.7949 - val_loss: 0.6132 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5391 - acc: 0.8015 - val_loss: 0.6164 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5351 - acc: 0.8015 - val_loss: 0.6272 - val_acc: 0.7681 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5394 - acc: 0.8021 - val_loss: 0.6195 - val_acc: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5496 - acc: 0.7903 - val_loss: 0.6126 - val_acc: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5287 - acc: 0.8073 - val_loss: 0.6191 - val_acc: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5409 - acc: 0.8024 - val_loss: 0.6136 - val_acc: 0.7721 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5328 - acc: 0.8030 - val_loss: 0.6262 - val_acc: 0.7669 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5471 - acc: 0.7997 - val_loss: 0.6256 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5388 - acc: 0.8003 - val_loss: 0.6113 - val_acc: 0.7768 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5450 - acc: 0.7964 - val_loss: 0.6215 - val_acc: 0.7704 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6829 - acc: 0.7435\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.7906 - acc: 0.6998\n",
            "epsilon: 0.003 and test evaluation : 0.7905946969985962, 0.6998254656791687\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.8677 - acc: 0.6649\n",
            "epsilon: 0.005 and test evaluation : 0.8676757216453552, 0.6649214625358582\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.0763 - acc: 0.5846\n",
            "epsilon: 0.01 and test evaluation : 1.0762577056884766, 0.584642231464386\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 1.5264 - acc: 0.4415\n",
            "epsilon: 0.02 and test evaluation : 1.526391625404358, 0.4415357708930969\n",
            "SNR: 33.75504732131958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5KYPHDImp62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_df[\"acc_clean_mean\"]= np.sum(result_df['acc_clean'])/3.0\n",
        "result_df[\"acc_0.003_mean\"]= np.sum(result_df['acc1'])/3.0\n",
        "result_df[\"acc_0.005_mean\"]= np.sum(result_df['acc2'])/3.0\n",
        "result_df[\"acc_0.02_mean\"]= np.sum(result_df['acc3'])/3.0\n",
        "result_df[\"acc_0.01_mean\"]= np.sum(result_df['acc4'])/3.0"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihcS6J2vzv6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "575fd86f-050d-450f-994b-0ed72450212a"
      },
      "source": [
        "result_df.head(1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.65122</td>\n",
              "      <td>0.757417</td>\n",
              "      <td>0.761078</td>\n",
              "      <td>0.713787</td>\n",
              "      <td>0.840366</td>\n",
              "      <td>0.675393</td>\n",
              "      <td>1.054346</td>\n",
              "      <td>0.577661</td>\n",
              "      <td>1.511571</td>\n",
              "      <td>0.455497</td>\n",
              "      <td>0.754508</td>\n",
              "      <td>0.706225</td>\n",
              "      <td>0.674229</td>\n",
              "      <td>0.586969</td>\n",
              "      <td>0.452589</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0     0.65122   0.757417  ...       0.586969       0.452589\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC_90XJcmu5l",
        "colab_type": "text"
      },
      "source": [
        "# **Adversarial Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7m-n2RCmyse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "class AdversarialTraining(object):\n",
        "    \"\"\"Adversarial Training  \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def train(self, pretrained_model, X_train, Y_train, X_test, y_test, epochs, BS, epsilon_list, sgd):\n",
        "        init = (32, 32,1)\n",
        "        res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "\n",
        "        kfold = KFold(n_splits = 3, random_state = 42)\n",
        "        for j, (train, val) in enumerate(kfold.split(X_train)):\n",
        "          x_train, y_train = self.data_augmentation(X_train[train], Y_train[train], BS, pretrained_model, epsilon_list)\n",
        "          x_val, y_val = self.data_augmentation(X_train[val], Y_train[val], BS, pretrained_model, epsilon_list)\n",
        "          model = create_parseval_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "          model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "          hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                          callbacks = [lr_scheduler],\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          validation_steps=x_val.shape[0] // BS,)\n",
        "          loss, acc = model.evaluate(X_test, y_test)\n",
        "          loss1, acc1 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "          loss2, acc2 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "          loss3, acc3 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "          loss4, acc4 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "          row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                  'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "          res_df = res_df.append(row , ignore_index=True)\n",
        "          \n",
        "        return res_df\n",
        "    def mini_batch_train(self, model, X_train,y_train, x_val, y_val, BS, pretrained_model, epsilon):\n",
        "\n",
        "\n",
        "        hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=1,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   validation_steps=x_val.shape[0] // BS, shuffle = True)\n",
        "        \n",
        "        ### TODO ###\n",
        "        ## Save hist on file.###\n",
        "\n",
        "\n",
        "    def data_augmentation(self, X_train, Y_train, batch_size, pretrained_model, epsilon_list):\n",
        "      ### divide data 16,16,16,16 for 4 different epsilons and 64 is true image. ### \n",
        "        #start_index = self.data_iteration(X_train, batch_size)\n",
        "        first_half_end = int(len(X_train)/2)\n",
        "        second_half_end = int(len(X_train))\n",
        "        x_clean = X_train[0:first_half_end,:,:,:]\n",
        "        x_adv = self.get_adversarial(X_train[first_half_end:second_half_end,:,:,:], Y_train[first_half_end:second_half_end], epsilon_list)\n",
        "        x_mix = self.merge_data(x_clean, x_adv)\n",
        "        y_mix = Y_train[0:second_half_end]\n",
        "        ### TODO###\n",
        "        # Mixture data for 4 epsilon values\n",
        "\n",
        "        return x_mix, y_mix\n",
        "\n",
        "    def data_iteration(self, X_train, batch_size):\n",
        "        N = X_train.shape[0]\n",
        "        start = np.random.randint(0, N-batch_size)\n",
        "        return start\n",
        "\n",
        "    def merge_data(self, x_clean, x_adv):\n",
        "        x_mix = []\n",
        "        for i in range(len(x_clean)):\n",
        "          x_mix.append(x_clean[i])\n",
        "        for j in range(len(x_adv)):\n",
        "          x_mix.append(x_adv[j])\n",
        "        x_mix = np.array(x_mix)\n",
        "\n",
        "        return x_mix\n",
        "\n",
        "\n",
        "    def get_adversarial(self, X_true, y_true, epsilon_list):\n",
        "\n",
        "        return self.adversarial_example(X_true, y_true, epsilon_list)\n",
        "\n",
        "    def adversarial_example(self, X_true, Y_true, epsilon_list):\n",
        "        size = len(X_true)\n",
        "        X_adv = []\n",
        "        interval = int(size/4)\n",
        "        index_list = [0,interval, interval*2, interval*3, size]\n",
        "        index = 0\n",
        "        for epsilon in epsilon_list:\n",
        "          if index == 4:\n",
        "            break\n",
        "          x_true = X_true[index_list[index]:index_list[index+1],:,:,:]\n",
        "          y_true = Y_true[index_list[index]:index_list[index+1]]\n",
        "\n",
        "          index = index + 1\n",
        "\n",
        "          for i in range(len(x_true)):\n",
        "            random_index = i\n",
        "            original_image = x_true[random_index]\n",
        "            original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "            original_label = y_true[random_index]\n",
        "            original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "            adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "            X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "          \n",
        "        X_adv = np.array(X_adv)\n",
        "        return X_adv\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swjxYRDJm5Iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47a0439a-daf7-4a7c-ca76-8767ee4bc7a6"
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "adversarial_training =  AdversarialTraining()\n",
        "sgd = SGD(lr=0.1, momentum=0.6)\n",
        "logits_model = tf.keras.Model(parseval_16_2.input, parseval_16_2.layers[-1].output)\n",
        "result_adv_df = adversarial_training.train(logits_model, X_train, Y_train, X_test, y_test, EPOCHS, BS, epsilon_list, sgd)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 3s 110ms/step - loss: 1.4735 - acc: 0.2839 - val_loss: 1.4276 - val_acc: 0.3518 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 3s 108ms/step - loss: 1.3967 - acc: 0.3553 - val_loss: 1.3490 - val_acc: 0.3751 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3367 - acc: 0.3577 - val_loss: 1.4070 - val_acc: 0.3168 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3095 - acc: 0.3847 - val_loss: 1.3145 - val_acc: 0.3652 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.3051 - acc: 0.3789 - val_loss: 1.2793 - val_acc: 0.3617 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2801 - acc: 0.3841 - val_loss: 1.2681 - val_acc: 0.4153 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2587 - acc: 0.4334 - val_loss: 1.3076 - val_acc: 0.3675 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2465 - acc: 0.4268 - val_loss: 1.3080 - val_acc: 0.3891 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2520 - acc: 0.4476 - val_loss: 1.2110 - val_acc: 0.5055 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1945 - acc: 0.5067 - val_loss: 1.2279 - val_acc: 0.4683 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1608 - acc: 0.5112 - val_loss: 3.4442 - val_acc: 0.3460 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1239 - acc: 0.5348 - val_loss: 1.1614 - val_acc: 0.5306 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 1.0922 - acc: 0.5512 - val_loss: 1.1980 - val_acc: 0.4985 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 1.0549 - acc: 0.5763 - val_loss: 1.0882 - val_acc: 0.5393 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0569 - acc: 0.5669 - val_loss: 1.1509 - val_acc: 0.5172 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0019 - acc: 0.6014 - val_loss: 1.0638 - val_acc: 0.5795 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9659 - acc: 0.6220 - val_loss: 1.1332 - val_acc: 0.5213 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9573 - acc: 0.6223 - val_loss: 1.2784 - val_acc: 0.4851 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.9151 - acc: 0.6353 - val_loss: 1.0517 - val_acc: 0.6040 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8837 - acc: 0.6544 - val_loss: 1.0155 - val_acc: 0.5946 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.8679 - acc: 0.6601 - val_loss: 1.2309 - val_acc: 0.5574 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8460 - acc: 0.6731 - val_loss: 1.0341 - val_acc: 0.5894 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.8220 - acc: 0.6828 - val_loss: 1.1886 - val_acc: 0.6080 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.8111 - acc: 0.6828 - val_loss: 0.9994 - val_acc: 0.6249 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7854 - acc: 0.7079 - val_loss: 1.1056 - val_acc: 0.5783 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7727 - acc: 0.7079 - val_loss: 0.9955 - val_acc: 0.6034 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7716 - acc: 0.7100 - val_loss: 1.1123 - val_acc: 0.5923 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7578 - acc: 0.7179 - val_loss: 0.8711 - val_acc: 0.6704 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7219 - acc: 0.7276 - val_loss: 0.8867 - val_acc: 0.6704 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7392 - acc: 0.7170 - val_loss: 1.2374 - val_acc: 0.5183 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8417 - acc: 0.6631 - val_loss: 0.8836 - val_acc: 0.6529 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7488 - acc: 0.7179 - val_loss: 0.8273 - val_acc: 0.6797 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6954 - acc: 0.7415 - val_loss: 0.7854 - val_acc: 0.6942 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6746 - acc: 0.7485 - val_loss: 0.7729 - val_acc: 0.7018 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6581 - acc: 0.7542 - val_loss: 0.7757 - val_acc: 0.7036 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6573 - acc: 0.7548 - val_loss: 0.7630 - val_acc: 0.7070 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6337 - acc: 0.7688 - val_loss: 0.7945 - val_acc: 0.6954 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6390 - acc: 0.7633 - val_loss: 0.7648 - val_acc: 0.7123 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6313 - acc: 0.7706 - val_loss: 0.7563 - val_acc: 0.7135 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6260 - acc: 0.7673 - val_loss: 0.7599 - val_acc: 0.7135 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6230 - acc: 0.7736 - val_loss: 0.7566 - val_acc: 0.7164 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6232 - acc: 0.7718 - val_loss: 0.7487 - val_acc: 0.7263 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6103 - acc: 0.7757 - val_loss: 0.7488 - val_acc: 0.7175 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6065 - acc: 0.7730 - val_loss: 0.7580 - val_acc: 0.7169 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5943 - acc: 0.7915 - val_loss: 0.7553 - val_acc: 0.7222 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5970 - acc: 0.7800 - val_loss: 0.7549 - val_acc: 0.7257 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5975 - acc: 0.7851 - val_loss: 0.7569 - val_acc: 0.7158 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5914 - acc: 0.7866 - val_loss: 0.7591 - val_acc: 0.7158 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5917 - acc: 0.7839 - val_loss: 0.7466 - val_acc: 0.7234 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5936 - acc: 0.7791 - val_loss: 0.7522 - val_acc: 0.7187 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5821 - acc: 0.7954 - val_loss: 0.7480 - val_acc: 0.7216 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5935 - acc: 0.7866 - val_loss: 0.7501 - val_acc: 0.7187 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5845 - acc: 0.7896 - val_loss: 0.7492 - val_acc: 0.7263 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5925 - acc: 0.7815 - val_loss: 0.7550 - val_acc: 0.7187 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5878 - acc: 0.7866 - val_loss: 0.7420 - val_acc: 0.7187 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5777 - acc: 0.7866 - val_loss: 0.7550 - val_acc: 0.7193 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5691 - acc: 0.7930 - val_loss: 0.7492 - val_acc: 0.7268 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5792 - acc: 0.7848 - val_loss: 0.7626 - val_acc: 0.7111 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5736 - acc: 0.7924 - val_loss: 0.7457 - val_acc: 0.7257 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5761 - acc: 0.7893 - val_loss: 0.7410 - val_acc: 0.7251 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5538 - acc: 0.8012 - val_loss: 0.7642 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5686 - acc: 0.7945 - val_loss: 0.7406 - val_acc: 0.7210 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5646 - acc: 0.8005 - val_loss: 0.7437 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5583 - acc: 0.7993 - val_loss: 0.7554 - val_acc: 0.7257 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5686 - acc: 0.7909 - val_loss: 0.7449 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5608 - acc: 0.7963 - val_loss: 0.7391 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5653 - acc: 0.7878 - val_loss: 0.7597 - val_acc: 0.7169 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5707 - acc: 0.7884 - val_loss: 0.7612 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5613 - acc: 0.7939 - val_loss: 0.7394 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5617 - acc: 0.8008 - val_loss: 0.7886 - val_acc: 0.7059 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5623 - acc: 0.7954 - val_loss: 0.7448 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5675 - acc: 0.7900 - val_loss: 0.7395 - val_acc: 0.7298 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5605 - acc: 0.7978 - val_loss: 0.7358 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5569 - acc: 0.7918 - val_loss: 0.7459 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5581 - acc: 0.7990 - val_loss: 0.7434 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5619 - acc: 0.8012 - val_loss: 0.7342 - val_acc: 0.7268 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5605 - acc: 0.8005 - val_loss: 0.7299 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5603 - acc: 0.7936 - val_loss: 0.7434 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5655 - acc: 0.7881 - val_loss: 0.7495 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5545 - acc: 0.8032 - val_loss: 0.7398 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5657 - acc: 0.7957 - val_loss: 0.7466 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5589 - acc: 0.7954 - val_loss: 0.7377 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5539 - acc: 0.8051 - val_loss: 0.7386 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5570 - acc: 0.7984 - val_loss: 0.7377 - val_acc: 0.7158 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5610 - acc: 0.7912 - val_loss: 0.7465 - val_acc: 0.7169 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5558 - acc: 0.8002 - val_loss: 0.7484 - val_acc: 0.7181 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5541 - acc: 0.7942 - val_loss: 0.7483 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5575 - acc: 0.8033 - val_loss: 0.7447 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5687 - acc: 0.7915 - val_loss: 0.7423 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5666 - acc: 0.7951 - val_loss: 0.7351 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5646 - acc: 0.7930 - val_loss: 0.7399 - val_acc: 0.7210 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5598 - acc: 0.7954 - val_loss: 0.7439 - val_acc: 0.7158 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5500 - acc: 0.8012 - val_loss: 0.7423 - val_acc: 0.7187 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5541 - acc: 0.8005 - val_loss: 0.7459 - val_acc: 0.7181 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5645 - acc: 0.7924 - val_loss: 0.7404 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5656 - acc: 0.7884 - val_loss: 0.7444 - val_acc: 0.7158 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5734 - acc: 0.7893 - val_loss: 0.7282 - val_acc: 0.7210 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5650 - acc: 0.7963 - val_loss: 0.7656 - val_acc: 0.7082 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5602 - acc: 0.7972 - val_loss: 0.7505 - val_acc: 0.7263 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5551 - acc: 0.7936 - val_loss: 0.7499 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5602 - acc: 0.7951 - val_loss: 0.7407 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5603 - acc: 0.7936 - val_loss: 0.7338 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5527 - acc: 0.7954 - val_loss: 0.7401 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5584 - acc: 0.7951 - val_loss: 0.7446 - val_acc: 0.7315 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5541 - acc: 0.7993 - val_loss: 0.7630 - val_acc: 0.7123 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5523 - acc: 0.7978 - val_loss: 0.7371 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5522 - acc: 0.7975 - val_loss: 0.7429 - val_acc: 0.7298 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5598 - acc: 0.7975 - val_loss: 0.7601 - val_acc: 0.7105 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5536 - acc: 0.8005 - val_loss: 0.7643 - val_acc: 0.7076 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5519 - acc: 0.8021 - val_loss: 0.7365 - val_acc: 0.7274 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5597 - acc: 0.7936 - val_loss: 0.7488 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5604 - acc: 0.7909 - val_loss: 0.7263 - val_acc: 0.7309 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5557 - acc: 0.7972 - val_loss: 0.7410 - val_acc: 0.7158 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5624 - acc: 0.7887 - val_loss: 0.7523 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5478 - acc: 0.8005 - val_loss: 0.7493 - val_acc: 0.7135 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5589 - acc: 0.7987 - val_loss: 0.7324 - val_acc: 0.7268 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5542 - acc: 0.7981 - val_loss: 0.7630 - val_acc: 0.7123 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5554 - acc: 0.7933 - val_loss: 0.7442 - val_acc: 0.7187 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5474 - acc: 0.8005 - val_loss: 0.7477 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5574 - acc: 0.7960 - val_loss: 0.7490 - val_acc: 0.7140 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5639 - acc: 0.7909 - val_loss: 0.7310 - val_acc: 0.7327 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5625 - acc: 0.7936 - val_loss: 0.7356 - val_acc: 0.7292 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5577 - acc: 0.7996 - val_loss: 0.7417 - val_acc: 0.7303 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5574 - acc: 0.7981 - val_loss: 0.7392 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5652 - acc: 0.7921 - val_loss: 0.7567 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5551 - acc: 0.7933 - val_loss: 0.7376 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5557 - acc: 0.7990 - val_loss: 0.7432 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5556 - acc: 0.7933 - val_loss: 0.7503 - val_acc: 0.7193 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5641 - acc: 0.7927 - val_loss: 0.7309 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5579 - acc: 0.7960 - val_loss: 0.7398 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5477 - acc: 0.7963 - val_loss: 0.7348 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5542 - acc: 0.7984 - val_loss: 0.7358 - val_acc: 0.7274 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5624 - acc: 0.7954 - val_loss: 0.7512 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5624 - acc: 0.7893 - val_loss: 0.7414 - val_acc: 0.7158 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5650 - acc: 0.7909 - val_loss: 0.7351 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5541 - acc: 0.7960 - val_loss: 0.7578 - val_acc: 0.7100 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5577 - acc: 0.7975 - val_loss: 0.7316 - val_acc: 0.7303 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5535 - acc: 0.7975 - val_loss: 0.7403 - val_acc: 0.7222 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5590 - acc: 0.7912 - val_loss: 0.7520 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5588 - acc: 0.7933 - val_loss: 0.7457 - val_acc: 0.7123 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5568 - acc: 0.7939 - val_loss: 0.7285 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5592 - acc: 0.7945 - val_loss: 0.7469 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5513 - acc: 0.8039 - val_loss: 0.7465 - val_acc: 0.7140 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5588 - acc: 0.7939 - val_loss: 0.7339 - val_acc: 0.7216 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5509 - acc: 0.7963 - val_loss: 0.7410 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5463 - acc: 0.8045 - val_loss: 0.7487 - val_acc: 0.7169 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5536 - acc: 0.7906 - val_loss: 0.7411 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5546 - acc: 0.7954 - val_loss: 0.7483 - val_acc: 0.7292 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5577 - acc: 0.7960 - val_loss: 0.7366 - val_acc: 0.7303 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5557 - acc: 0.7999 - val_loss: 0.7290 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5569 - acc: 0.7960 - val_loss: 0.7530 - val_acc: 0.7140 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5566 - acc: 0.8005 - val_loss: 0.7297 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5573 - acc: 0.7966 - val_loss: 0.7358 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5542 - acc: 0.8021 - val_loss: 0.7345 - val_acc: 0.7210 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5571 - acc: 0.7990 - val_loss: 0.7256 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5631 - acc: 0.7896 - val_loss: 0.7406 - val_acc: 0.7210 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5549 - acc: 0.8005 - val_loss: 0.7340 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5620 - acc: 0.7963 - val_loss: 0.7395 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5550 - acc: 0.7963 - val_loss: 0.7345 - val_acc: 0.7286 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5520 - acc: 0.7987 - val_loss: 0.7294 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5568 - acc: 0.7936 - val_loss: 0.7303 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5531 - acc: 0.8005 - val_loss: 0.7322 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5634 - acc: 0.7960 - val_loss: 0.7414 - val_acc: 0.7111 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5514 - acc: 0.7987 - val_loss: 0.7386 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5556 - acc: 0.7912 - val_loss: 0.7431 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5655 - acc: 0.7957 - val_loss: 0.7350 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5522 - acc: 0.7930 - val_loss: 0.7395 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5663 - acc: 0.7869 - val_loss: 0.7342 - val_acc: 0.7169 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5478 - acc: 0.8012 - val_loss: 0.7509 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5499 - acc: 0.8042 - val_loss: 0.7421 - val_acc: 0.7135 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5606 - acc: 0.7915 - val_loss: 0.7667 - val_acc: 0.7117 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5511 - acc: 0.8008 - val_loss: 0.7385 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5573 - acc: 0.7990 - val_loss: 0.7332 - val_acc: 0.7263 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5484 - acc: 0.7963 - val_loss: 0.7361 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5538 - acc: 0.7978 - val_loss: 0.7315 - val_acc: 0.7292 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5556 - acc: 0.7978 - val_loss: 0.7490 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5578 - acc: 0.7981 - val_loss: 0.7525 - val_acc: 0.7100 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5640 - acc: 0.7893 - val_loss: 0.7360 - val_acc: 0.7239 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5616 - acc: 0.7951 - val_loss: 0.7320 - val_acc: 0.7280 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5448 - acc: 0.8096 - val_loss: 0.7494 - val_acc: 0.7111 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5599 - acc: 0.7927 - val_loss: 0.7378 - val_acc: 0.7257 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5591 - acc: 0.7890 - val_loss: 0.7443 - val_acc: 0.7152 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5491 - acc: 0.7999 - val_loss: 0.7382 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5535 - acc: 0.8018 - val_loss: 0.7524 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5491 - acc: 0.7975 - val_loss: 0.7400 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5601 - acc: 0.7969 - val_loss: 0.7425 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5576 - acc: 0.7921 - val_loss: 0.7482 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5563 - acc: 0.7990 - val_loss: 0.7312 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5521 - acc: 0.8005 - val_loss: 0.7492 - val_acc: 0.7164 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5523 - acc: 0.7969 - val_loss: 0.7364 - val_acc: 0.7274 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5446 - acc: 0.8029 - val_loss: 0.7374 - val_acc: 0.7234 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5561 - acc: 0.7915 - val_loss: 0.7412 - val_acc: 0.7228 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5489 - acc: 0.7972 - val_loss: 0.7339 - val_acc: 0.7274 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5535 - acc: 0.7957 - val_loss: 0.7429 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5458 - acc: 0.8060 - val_loss: 0.7426 - val_acc: 0.7245 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5536 - acc: 0.8027 - val_loss: 0.7327 - val_acc: 0.7303 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5459 - acc: 0.7990 - val_loss: 0.7389 - val_acc: 0.7199 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5526 - acc: 0.7954 - val_loss: 0.7308 - val_acc: 0.7303 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5555 - acc: 0.7933 - val_loss: 0.7333 - val_acc: 0.7286 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5565 - acc: 0.7915 - val_loss: 0.7303 - val_acc: 0.7251 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6504 - acc: 0.7592\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6777 - acc: 0.7469\n",
            "epsilon: 0.003 and test evaluation : 0.6777461171150208, 0.7469459176063538\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.6965 - acc: 0.7365\n",
            "epsilon: 0.005 and test evaluation : 0.6965183019638062, 0.7364746928215027\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.7451 - acc: 0.7260\n",
            "epsilon: 0.01 and test evaluation : 0.7450900077819824, 0.7260034680366516\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.8500 - acc: 0.6736\n",
            "epsilon: 0.02 and test evaluation : 0.8499993681907654, 0.6736474633216858\n",
            "SNR: 33.75504732131958\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 3s 114ms/step - loss: 1.4717 - acc: 0.2950 - val_loss: 1.4449 - val_acc: 0.3473 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3873 - acc: 0.3773 - val_loss: 1.3549 - val_acc: 0.3561 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.3107 - acc: 0.3912 - val_loss: 1.4540 - val_acc: 0.3007 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3124 - acc: 0.3970 - val_loss: 1.3505 - val_acc: 0.3718 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 1.2879 - acc: 0.4094 - val_loss: 1.3476 - val_acc: 0.3986 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.2518 - acc: 0.4502 - val_loss: 1.4665 - val_acc: 0.3042 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2108 - acc: 0.4932 - val_loss: 1.2845 - val_acc: 0.4598 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1723 - acc: 0.5141 - val_loss: 1.1631 - val_acc: 0.5373 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1470 - acc: 0.5380 - val_loss: 1.7486 - val_acc: 0.4155 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.1087 - acc: 0.5467 - val_loss: 1.0959 - val_acc: 0.5501 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0748 - acc: 0.5640 - val_loss: 1.1257 - val_acc: 0.5181 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1095 - acc: 0.5395 - val_loss: 1.1769 - val_acc: 0.4988 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0463 - acc: 0.5837 - val_loss: 1.1685 - val_acc: 0.4988 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0031 - acc: 0.5982 - val_loss: 1.2806 - val_acc: 0.4878 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9637 - acc: 0.6112 - val_loss: 1.5184 - val_acc: 0.4522 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9296 - acc: 0.6333 - val_loss: 1.1964 - val_acc: 0.5484 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9147 - acc: 0.6333 - val_loss: 1.6490 - val_acc: 0.4505 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8796 - acc: 0.6545 - val_loss: 1.0696 - val_acc: 0.5991 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8451 - acc: 0.6693 - val_loss: 0.9959 - val_acc: 0.6253 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8714 - acc: 0.6502 - val_loss: 1.3047 - val_acc: 0.5058 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8164 - acc: 0.6787 - val_loss: 1.1101 - val_acc: 0.6183 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8103 - acc: 0.6835 - val_loss: 1.1708 - val_acc: 0.5880 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7869 - acc: 0.6959 - val_loss: 1.1616 - val_acc: 0.5828 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.7891 - acc: 0.6920 - val_loss: 1.1055 - val_acc: 0.6055 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7646 - acc: 0.7080 - val_loss: 1.0152 - val_acc: 0.6026 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7370 - acc: 0.7221 - val_loss: 0.9342 - val_acc: 0.6486 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.7361 - acc: 0.7227 - val_loss: 0.9451 - val_acc: 0.6439 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7172 - acc: 0.7265 - val_loss: 1.1623 - val_acc: 0.6061 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.7220 - acc: 0.7225 - val_loss: 0.8653 - val_acc: 0.6836 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6928 - acc: 0.7377 - val_loss: 0.9681 - val_acc: 0.6480 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7226 - acc: 0.7201 - val_loss: 0.9584 - val_acc: 0.6672 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6680 - acc: 0.7507 - val_loss: 0.9420 - val_acc: 0.6696 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6436 - acc: 0.7555 - val_loss: 0.9105 - val_acc: 0.6847 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6364 - acc: 0.7622 - val_loss: 0.8992 - val_acc: 0.6865 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6210 - acc: 0.7685 - val_loss: 0.8814 - val_acc: 0.6964 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6025 - acc: 0.7809 - val_loss: 0.9175 - val_acc: 0.6859 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 96ms/step - loss: 0.5976 - acc: 0.7837 - val_loss: 0.9009 - val_acc: 0.6941 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5954 - acc: 0.7828 - val_loss: 0.8843 - val_acc: 0.6987 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5863 - acc: 0.7849 - val_loss: 0.8595 - val_acc: 0.7040 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5733 - acc: 0.7885 - val_loss: 0.8494 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5787 - acc: 0.7834 - val_loss: 0.8402 - val_acc: 0.6993 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5684 - acc: 0.7918 - val_loss: 0.9331 - val_acc: 0.6865 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5733 - acc: 0.7927 - val_loss: 0.9108 - val_acc: 0.6888 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5687 - acc: 0.7915 - val_loss: 0.8686 - val_acc: 0.7028 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5653 - acc: 0.7885 - val_loss: 0.8649 - val_acc: 0.7069 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5639 - acc: 0.7927 - val_loss: 0.8903 - val_acc: 0.6923 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5619 - acc: 0.7952 - val_loss: 0.8723 - val_acc: 0.7086 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5600 - acc: 0.7930 - val_loss: 0.9198 - val_acc: 0.6970 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5559 - acc: 0.8015 - val_loss: 0.9280 - val_acc: 0.6917 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5565 - acc: 0.7952 - val_loss: 0.9412 - val_acc: 0.6876 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5563 - acc: 0.7927 - val_loss: 0.8918 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5533 - acc: 0.7921 - val_loss: 0.8992 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5526 - acc: 0.7964 - val_loss: 0.9085 - val_acc: 0.6935 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5443 - acc: 0.8051 - val_loss: 0.9103 - val_acc: 0.7028 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5416 - acc: 0.8012 - val_loss: 0.8734 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5416 - acc: 0.7991 - val_loss: 0.8849 - val_acc: 0.7005 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5401 - acc: 0.8079 - val_loss: 0.8721 - val_acc: 0.7022 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5383 - acc: 0.7997 - val_loss: 0.9262 - val_acc: 0.6906 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5329 - acc: 0.8061 - val_loss: 0.8758 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5373 - acc: 0.8030 - val_loss: 0.9781 - val_acc: 0.6876 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5288 - acc: 0.8057 - val_loss: 0.8501 - val_acc: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5285 - acc: 0.8070 - val_loss: 0.9467 - val_acc: 0.6917 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5295 - acc: 0.8027 - val_loss: 0.9791 - val_acc: 0.6900 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5324 - acc: 0.8070 - val_loss: 0.8649 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5283 - acc: 0.8064 - val_loss: 0.8687 - val_acc: 0.7133 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5258 - acc: 0.8124 - val_loss: 0.9651 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5202 - acc: 0.8166 - val_loss: 0.8777 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5311 - acc: 0.8036 - val_loss: 0.9207 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5408 - acc: 0.8036 - val_loss: 0.8737 - val_acc: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5312 - acc: 0.8024 - val_loss: 0.9198 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5312 - acc: 0.8136 - val_loss: 0.8980 - val_acc: 0.6999 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5219 - acc: 0.8038 - val_loss: 0.9087 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5310 - acc: 0.8012 - val_loss: 0.9190 - val_acc: 0.6993 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5290 - acc: 0.8076 - val_loss: 0.8684 - val_acc: 0.7034 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5418 - acc: 0.8050 - val_loss: 0.8579 - val_acc: 0.6993 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5452 - acc: 0.7967 - val_loss: 0.8882 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5258 - acc: 0.8109 - val_loss: 0.8752 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5320 - acc: 0.8051 - val_loss: 0.9320 - val_acc: 0.6900 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5160 - acc: 0.8130 - val_loss: 0.9304 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5295 - acc: 0.8000 - val_loss: 0.9806 - val_acc: 0.6836 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5231 - acc: 0.8064 - val_loss: 0.9549 - val_acc: 0.6853 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5271 - acc: 0.8133 - val_loss: 0.8500 - val_acc: 0.7057 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5164 - acc: 0.8154 - val_loss: 0.8413 - val_acc: 0.7051 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5349 - acc: 0.8024 - val_loss: 0.8600 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5272 - acc: 0.8082 - val_loss: 0.9460 - val_acc: 0.6952 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5250 - acc: 0.8091 - val_loss: 1.0075 - val_acc: 0.6760 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5245 - acc: 0.8169 - val_loss: 0.8874 - val_acc: 0.7040 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5433 - acc: 0.8012 - val_loss: 0.9782 - val_acc: 0.6882 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5191 - acc: 0.8064 - val_loss: 0.8763 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5277 - acc: 0.8073 - val_loss: 0.8964 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5236 - acc: 0.8115 - val_loss: 0.9280 - val_acc: 0.6946 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5221 - acc: 0.8103 - val_loss: 0.8455 - val_acc: 0.7069 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5213 - acc: 0.8127 - val_loss: 0.8989 - val_acc: 0.7034 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5225 - acc: 0.8094 - val_loss: 0.8882 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5225 - acc: 0.8124 - val_loss: 0.8252 - val_acc: 0.7133 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5196 - acc: 0.8079 - val_loss: 0.9017 - val_acc: 0.6976 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5298 - acc: 0.8076 - val_loss: 0.9551 - val_acc: 0.6824 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5249 - acc: 0.8097 - val_loss: 0.8897 - val_acc: 0.6999 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5207 - acc: 0.8112 - val_loss: 0.8875 - val_acc: 0.7075 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5295 - acc: 0.8057 - val_loss: 0.8605 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5254 - acc: 0.8109 - val_loss: 0.8626 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5310 - acc: 0.7991 - val_loss: 0.8748 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5241 - acc: 0.8067 - val_loss: 0.8727 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5206 - acc: 0.8070 - val_loss: 0.9185 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5267 - acc: 0.8076 - val_loss: 0.9289 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5243 - acc: 0.8127 - val_loss: 0.9544 - val_acc: 0.6888 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5169 - acc: 0.8112 - val_loss: 0.9154 - val_acc: 0.6917 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5198 - acc: 0.8188 - val_loss: 0.8257 - val_acc: 0.7133 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5265 - acc: 0.8057 - val_loss: 0.8756 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5213 - acc: 0.8145 - val_loss: 0.9382 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5240 - acc: 0.8076 - val_loss: 0.8426 - val_acc: 0.7092 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5241 - acc: 0.8030 - val_loss: 0.9220 - val_acc: 0.6946 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5160 - acc: 0.8151 - val_loss: 0.8694 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5249 - acc: 0.8076 - val_loss: 0.9209 - val_acc: 0.6917 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5331 - acc: 0.8036 - val_loss: 0.8870 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5238 - acc: 0.8109 - val_loss: 0.8888 - val_acc: 0.6941 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5245 - acc: 0.8064 - val_loss: 0.8737 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5234 - acc: 0.8057 - val_loss: 0.8780 - val_acc: 0.7028 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5254 - acc: 0.8112 - val_loss: 0.8696 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5176 - acc: 0.8091 - val_loss: 0.9011 - val_acc: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5315 - acc: 0.8082 - val_loss: 0.9001 - val_acc: 0.6923 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5201 - acc: 0.8094 - val_loss: 0.8331 - val_acc: 0.7080 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5192 - acc: 0.8098 - val_loss: 0.9273 - val_acc: 0.6970 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5239 - acc: 0.8000 - val_loss: 0.9054 - val_acc: 0.6952 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5226 - acc: 0.8119 - val_loss: 0.9140 - val_acc: 0.6970 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5161 - acc: 0.8079 - val_loss: 0.9359 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5227 - acc: 0.8097 - val_loss: 0.9661 - val_acc: 0.6941 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5215 - acc: 0.8070 - val_loss: 0.8965 - val_acc: 0.6946 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5320 - acc: 0.8021 - val_loss: 0.8617 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5264 - acc: 0.8015 - val_loss: 0.8894 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5166 - acc: 0.8142 - val_loss: 0.9918 - val_acc: 0.6766 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5211 - acc: 0.8124 - val_loss: 0.8947 - val_acc: 0.7051 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5235 - acc: 0.8070 - val_loss: 0.8889 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5169 - acc: 0.8015 - val_loss: 0.9033 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5220 - acc: 0.8106 - val_loss: 0.9137 - val_acc: 0.7051 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5264 - acc: 0.8100 - val_loss: 0.8793 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5195 - acc: 0.8103 - val_loss: 0.9146 - val_acc: 0.6993 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5238 - acc: 0.8067 - val_loss: 0.9450 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5202 - acc: 0.8085 - val_loss: 0.9108 - val_acc: 0.6941 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5161 - acc: 0.8121 - val_loss: 0.9134 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5328 - acc: 0.7985 - val_loss: 0.9801 - val_acc: 0.6876 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5176 - acc: 0.8103 - val_loss: 1.0598 - val_acc: 0.6731 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5197 - acc: 0.8061 - val_loss: 0.8422 - val_acc: 0.7080 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5210 - acc: 0.8106 - val_loss: 0.9335 - val_acc: 0.6865 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5222 - acc: 0.8115 - val_loss: 0.9053 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5212 - acc: 0.8109 - val_loss: 0.9279 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5248 - acc: 0.8042 - val_loss: 0.9063 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5139 - acc: 0.8085 - val_loss: 0.9243 - val_acc: 0.6917 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5196 - acc: 0.8133 - val_loss: 0.9212 - val_acc: 0.6993 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5195 - acc: 0.8100 - val_loss: 0.8555 - val_acc: 0.7040 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5200 - acc: 0.8124 - val_loss: 0.8938 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5229 - acc: 0.8057 - val_loss: 0.8766 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5139 - acc: 0.8097 - val_loss: 0.8876 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5302 - acc: 0.8036 - val_loss: 0.9692 - val_acc: 0.6836 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5141 - acc: 0.8076 - val_loss: 0.8773 - val_acc: 0.7028 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5341 - acc: 0.8021 - val_loss: 0.8973 - val_acc: 0.6976 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5068 - acc: 0.8127 - val_loss: 0.8740 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5207 - acc: 0.8021 - val_loss: 0.8866 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5316 - acc: 0.8045 - val_loss: 0.8833 - val_acc: 0.6999 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5250 - acc: 0.8015 - val_loss: 0.9435 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5322 - acc: 0.8015 - val_loss: 0.9204 - val_acc: 0.6923 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5241 - acc: 0.8012 - val_loss: 0.8811 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5228 - acc: 0.8076 - val_loss: 0.8874 - val_acc: 0.7040 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5199 - acc: 0.8151 - val_loss: 0.8589 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5143 - acc: 0.8100 - val_loss: 0.8632 - val_acc: 0.7057 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5210 - acc: 0.8051 - val_loss: 0.9410 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5199 - acc: 0.8033 - val_loss: 0.9038 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5186 - acc: 0.8061 - val_loss: 0.9589 - val_acc: 0.6929 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5219 - acc: 0.8036 - val_loss: 0.9465 - val_acc: 0.6941 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5278 - acc: 0.8109 - val_loss: 0.9318 - val_acc: 0.6894 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5312 - acc: 0.8061 - val_loss: 0.8820 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5214 - acc: 0.8051 - val_loss: 0.8773 - val_acc: 0.7040 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5330 - acc: 0.7955 - val_loss: 0.8937 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5261 - acc: 0.8024 - val_loss: 0.8826 - val_acc: 0.7034 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5299 - acc: 0.8054 - val_loss: 0.9446 - val_acc: 0.6876 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5214 - acc: 0.8124 - val_loss: 0.8521 - val_acc: 0.7080 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5138 - acc: 0.8124 - val_loss: 0.8637 - val_acc: 0.6999 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5279 - acc: 0.8076 - val_loss: 0.9156 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5192 - acc: 0.8073 - val_loss: 0.8888 - val_acc: 0.6970 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5140 - acc: 0.8116 - val_loss: 0.9166 - val_acc: 0.6946 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5211 - acc: 0.8097 - val_loss: 0.9416 - val_acc: 0.6906 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5206 - acc: 0.8033 - val_loss: 0.9784 - val_acc: 0.6795 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5260 - acc: 0.8051 - val_loss: 0.8649 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 3s 104ms/step - loss: 0.5133 - acc: 0.8067 - val_loss: 0.9489 - val_acc: 0.6964 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5175 - acc: 0.8106 - val_loss: 0.9109 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5198 - acc: 0.8056 - val_loss: 0.9859 - val_acc: 0.6841 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5160 - acc: 0.8124 - val_loss: 0.9099 - val_acc: 0.6970 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5174 - acc: 0.8054 - val_loss: 0.8796 - val_acc: 0.7069 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5136 - acc: 0.8175 - val_loss: 0.8440 - val_acc: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5243 - acc: 0.8127 - val_loss: 0.9168 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5228 - acc: 0.8106 - val_loss: 0.9429 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5119 - acc: 0.8103 - val_loss: 0.9098 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5260 - acc: 0.8064 - val_loss: 0.9723 - val_acc: 0.6853 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5159 - acc: 0.8070 - val_loss: 0.8988 - val_acc: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5123 - acc: 0.8067 - val_loss: 0.9036 - val_acc: 0.6987 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5209 - acc: 0.8073 - val_loss: 0.8395 - val_acc: 0.7104 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5250 - acc: 0.8121 - val_loss: 0.8407 - val_acc: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5202 - acc: 0.8042 - val_loss: 0.8994 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5240 - acc: 0.8076 - val_loss: 0.8678 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 96ms/step - loss: 0.5237 - acc: 0.8109 - val_loss: 0.9202 - val_acc: 0.6958 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.8846 - acc: 0.6998\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.9174 - acc: 0.6876\n",
            "epsilon: 0.003 and test evaluation : 0.9173702001571655, 0.687609076499939\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.9399 - acc: 0.6841\n",
            "epsilon: 0.005 and test evaluation : 0.9399174451828003, 0.6841186881065369\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 9ms/step - loss: 0.9979 - acc: 0.6702\n",
            "epsilon: 0.01 and test evaluation : 0.9978927373886108, 0.6701570749282837\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 1.1184 - acc: 0.6248\n",
            "epsilon: 0.02 and test evaluation : 1.1184452772140503, 0.6247818470001221\n",
            "SNR: 33.75504732131958\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Parseval Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "26/26 [==============================] - 3s 112ms/step - loss: 1.4730 - acc: 0.2702 - val_loss: 1.4308 - val_acc: 0.3770 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.4011 - acc: 0.3573 - val_loss: 1.3470 - val_acc: 0.3823 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3563 - acc: 0.3474 - val_loss: 1.3105 - val_acc: 0.3928 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 1.3241 - acc: 0.3655 - val_loss: 1.2977 - val_acc: 0.3957 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.3138 - acc: 0.3710 - val_loss: 1.2806 - val_acc: 0.4126 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2955 - acc: 0.3858 - val_loss: 1.2519 - val_acc: 0.4172 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2717 - acc: 0.4061 - val_loss: 1.3429 - val_acc: 0.3998 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2549 - acc: 0.4430 - val_loss: 1.2150 - val_acc: 0.4860 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.2107 - acc: 0.4862 - val_loss: 1.1657 - val_acc: 0.5023 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1968 - acc: 0.5023 - val_loss: 1.1741 - val_acc: 0.5262 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1659 - acc: 0.5150 - val_loss: 1.1450 - val_acc: 0.5262 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.1508 - acc: 0.5213 - val_loss: 1.3618 - val_acc: 0.4534 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0827 - acc: 0.5582 - val_loss: 1.0076 - val_acc: 0.6136 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 1.0703 - acc: 0.5592 - val_loss: 1.2435 - val_acc: 0.5082 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 1.0097 - acc: 0.5964 - val_loss: 1.1587 - val_acc: 0.5303 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9680 - acc: 0.6151 - val_loss: 0.9043 - val_acc: 0.6556 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.9615 - acc: 0.6136 - val_loss: 0.9110 - val_acc: 0.6486 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9163 - acc: 0.6266 - val_loss: 0.8935 - val_acc: 0.6515 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.9203 - acc: 0.6303 - val_loss: 0.9603 - val_acc: 0.6166 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8691 - acc: 0.6599 - val_loss: 0.9107 - val_acc: 0.6661 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8449 - acc: 0.6814 - val_loss: 1.0130 - val_acc: 0.6340 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8478 - acc: 0.6708 - val_loss: 0.8610 - val_acc: 0.6503 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8116 - acc: 0.6938 - val_loss: 1.1795 - val_acc: 0.5705 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.8077 - acc: 0.6823 - val_loss: 0.9635 - val_acc: 0.6492 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7994 - acc: 0.6817 - val_loss: 0.8920 - val_acc: 0.6795 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.8006 - acc: 0.6917 - val_loss: 0.7571 - val_acc: 0.7092 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7876 - acc: 0.6935 - val_loss: 0.8069 - val_acc: 0.6976 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7636 - acc: 0.6974 - val_loss: 0.7489 - val_acc: 0.7296 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7573 - acc: 0.7023 - val_loss: 0.7672 - val_acc: 0.7040 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.7467 - acc: 0.7141 - val_loss: 0.8287 - val_acc: 0.6911 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.7964 - acc: 0.6917 - val_loss: 0.7765 - val_acc: 0.7174 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.7015 - acc: 0.7322 - val_loss: 0.7102 - val_acc: 0.7424 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.6876 - acc: 0.7449 - val_loss: 0.7033 - val_acc: 0.7494 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6614 - acc: 0.7519 - val_loss: 0.7031 - val_acc: 0.7448 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.6504 - acc: 0.7589 - val_loss: 0.6851 - val_acc: 0.7459 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6550 - acc: 0.7552 - val_loss: 0.6915 - val_acc: 0.7477 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6352 - acc: 0.7664 - val_loss: 0.7170 - val_acc: 0.7442 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.6405 - acc: 0.7631 - val_loss: 0.6859 - val_acc: 0.7494 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6278 - acc: 0.7728 - val_loss: 0.6833 - val_acc: 0.7453 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6194 - acc: 0.7755 - val_loss: 0.6937 - val_acc: 0.7436 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6227 - acc: 0.7710 - val_loss: 0.6689 - val_acc: 0.7494 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6195 - acc: 0.7722 - val_loss: 0.7009 - val_acc: 0.7541 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6109 - acc: 0.7731 - val_loss: 0.6676 - val_acc: 0.7541 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6016 - acc: 0.7791 - val_loss: 0.6807 - val_acc: 0.7506 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.6104 - acc: 0.7746 - val_loss: 0.6776 - val_acc: 0.7535 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6028 - acc: 0.7746 - val_loss: 0.6567 - val_acc: 0.7523 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.6013 - acc: 0.7837 - val_loss: 0.7047 - val_acc: 0.7483 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5996 - acc: 0.7809 - val_loss: 0.7096 - val_acc: 0.7483 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5892 - acc: 0.7800 - val_loss: 0.6774 - val_acc: 0.7547 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5917 - acc: 0.7806 - val_loss: 0.6769 - val_acc: 0.7535 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5939 - acc: 0.7779 - val_loss: 0.7051 - val_acc: 0.7552 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5842 - acc: 0.7885 - val_loss: 0.6840 - val_acc: 0.7558 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5885 - acc: 0.7815 - val_loss: 0.6676 - val_acc: 0.7599 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5753 - acc: 0.7825 - val_loss: 0.6779 - val_acc: 0.7517 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5911 - acc: 0.7761 - val_loss: 0.7063 - val_acc: 0.7442 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5817 - acc: 0.7870 - val_loss: 0.6963 - val_acc: 0.7424 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5766 - acc: 0.7873 - val_loss: 0.6743 - val_acc: 0.7558 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5691 - acc: 0.7852 - val_loss: 0.7005 - val_acc: 0.7512 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5745 - acc: 0.7939 - val_loss: 0.6901 - val_acc: 0.7547 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5700 - acc: 0.7849 - val_loss: 0.6694 - val_acc: 0.7576 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5657 - acc: 0.7897 - val_loss: 0.6835 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5736 - acc: 0.7882 - val_loss: 0.6733 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5735 - acc: 0.7967 - val_loss: 0.6794 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5803 - acc: 0.7755 - val_loss: 0.6631 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5705 - acc: 0.7909 - val_loss: 0.6773 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5635 - acc: 0.7930 - val_loss: 0.6920 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5760 - acc: 0.7809 - val_loss: 0.6731 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5694 - acc: 0.7906 - val_loss: 0.7127 - val_acc: 0.7453 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5835 - acc: 0.7846 - val_loss: 0.6715 - val_acc: 0.7628 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5603 - acc: 0.7885 - val_loss: 0.6694 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5838 - acc: 0.7770 - val_loss: 0.6689 - val_acc: 0.7611 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5725 - acc: 0.7870 - val_loss: 0.6978 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5683 - acc: 0.7894 - val_loss: 0.6724 - val_acc: 0.7622 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5700 - acc: 0.7903 - val_loss: 0.6953 - val_acc: 0.7529 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5748 - acc: 0.7936 - val_loss: 0.7221 - val_acc: 0.7500 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5689 - acc: 0.7825 - val_loss: 0.6740 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5663 - acc: 0.7933 - val_loss: 0.7112 - val_acc: 0.7477 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5579 - acc: 0.7918 - val_loss: 0.6935 - val_acc: 0.7517 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5658 - acc: 0.7879 - val_loss: 0.6795 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5693 - acc: 0.7821 - val_loss: 0.6686 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5655 - acc: 0.7943 - val_loss: 0.6823 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5672 - acc: 0.7840 - val_loss: 0.7006 - val_acc: 0.7506 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5726 - acc: 0.7843 - val_loss: 0.6712 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5672 - acc: 0.7858 - val_loss: 0.6945 - val_acc: 0.7628 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5573 - acc: 0.7897 - val_loss: 0.7061 - val_acc: 0.7494 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5681 - acc: 0.7927 - val_loss: 0.6962 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5708 - acc: 0.7843 - val_loss: 0.6966 - val_acc: 0.7500 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5657 - acc: 0.7903 - val_loss: 0.6996 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5622 - acc: 0.7921 - val_loss: 0.6796 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5634 - acc: 0.7906 - val_loss: 0.6751 - val_acc: 0.7611 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5578 - acc: 0.7976 - val_loss: 0.6951 - val_acc: 0.7512 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5701 - acc: 0.7815 - val_loss: 0.6688 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5672 - acc: 0.7909 - val_loss: 0.7105 - val_acc: 0.7494 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5652 - acc: 0.7921 - val_loss: 0.6658 - val_acc: 0.7552 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5676 - acc: 0.7867 - val_loss: 0.6875 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5710 - acc: 0.7882 - val_loss: 0.6712 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5686 - acc: 0.7822 - val_loss: 0.6921 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "26/26 [==============================] - 2s 95ms/step - loss: 0.5619 - acc: 0.7891 - val_loss: 0.6777 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5597 - acc: 0.7927 - val_loss: 0.6519 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5714 - acc: 0.7858 - val_loss: 0.6687 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5710 - acc: 0.7858 - val_loss: 0.6851 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5659 - acc: 0.7828 - val_loss: 0.6704 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5644 - acc: 0.7885 - val_loss: 0.6730 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5630 - acc: 0.7894 - val_loss: 0.6956 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5800 - acc: 0.7818 - val_loss: 0.6938 - val_acc: 0.7512 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5628 - acc: 0.7918 - val_loss: 0.7160 - val_acc: 0.7459 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5591 - acc: 0.7906 - val_loss: 0.7371 - val_acc: 0.7389 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5644 - acc: 0.7918 - val_loss: 0.6647 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5722 - acc: 0.7788 - val_loss: 0.6653 - val_acc: 0.7599 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5612 - acc: 0.7873 - val_loss: 0.6783 - val_acc: 0.7628 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5577 - acc: 0.7906 - val_loss: 0.7006 - val_acc: 0.7477 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5721 - acc: 0.7806 - val_loss: 0.6742 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5629 - acc: 0.7915 - val_loss: 0.7533 - val_acc: 0.7314 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5597 - acc: 0.7897 - val_loss: 0.7042 - val_acc: 0.7483 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5747 - acc: 0.7897 - val_loss: 0.7009 - val_acc: 0.7483 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5671 - acc: 0.7891 - val_loss: 0.6659 - val_acc: 0.7593 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5684 - acc: 0.7831 - val_loss: 0.6939 - val_acc: 0.7506 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5592 - acc: 0.7930 - val_loss: 0.6968 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5597 - acc: 0.7930 - val_loss: 0.6870 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5713 - acc: 0.7834 - val_loss: 0.7182 - val_acc: 0.7477 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5622 - acc: 0.7873 - val_loss: 0.6740 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5565 - acc: 0.7897 - val_loss: 0.6840 - val_acc: 0.7552 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5627 - acc: 0.7897 - val_loss: 0.6921 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5629 - acc: 0.7924 - val_loss: 0.6655 - val_acc: 0.7535 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5702 - acc: 0.7864 - val_loss: 0.7330 - val_acc: 0.7413 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5631 - acc: 0.7921 - val_loss: 0.6572 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5704 - acc: 0.7843 - val_loss: 0.7328 - val_acc: 0.7383 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5632 - acc: 0.7870 - val_loss: 0.6904 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5624 - acc: 0.7879 - val_loss: 0.6799 - val_acc: 0.7593 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5569 - acc: 0.7831 - val_loss: 0.6645 - val_acc: 0.7634 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5610 - acc: 0.7903 - val_loss: 0.6635 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5546 - acc: 0.7888 - val_loss: 0.6723 - val_acc: 0.7622 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5683 - acc: 0.7891 - val_loss: 0.6941 - val_acc: 0.7552 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5510 - acc: 0.7943 - val_loss: 0.6840 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5610 - acc: 0.7849 - val_loss: 0.6857 - val_acc: 0.7517 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5689 - acc: 0.7855 - val_loss: 0.6759 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5652 - acc: 0.7900 - val_loss: 0.6738 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5685 - acc: 0.7849 - val_loss: 0.6799 - val_acc: 0.7512 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5589 - acc: 0.7939 - val_loss: 0.6796 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5667 - acc: 0.7900 - val_loss: 0.6772 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5712 - acc: 0.7885 - val_loss: 0.7061 - val_acc: 0.7535 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5618 - acc: 0.7888 - val_loss: 0.6817 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5774 - acc: 0.7788 - val_loss: 0.7003 - val_acc: 0.7483 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5611 - acc: 0.7906 - val_loss: 0.6791 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5692 - acc: 0.7849 - val_loss: 0.6698 - val_acc: 0.7593 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5591 - acc: 0.7900 - val_loss: 0.7310 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5689 - acc: 0.7924 - val_loss: 0.6905 - val_acc: 0.7506 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5680 - acc: 0.7809 - val_loss: 0.6932 - val_acc: 0.7535 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5713 - acc: 0.7797 - val_loss: 0.6858 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5601 - acc: 0.7873 - val_loss: 0.6819 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5692 - acc: 0.7794 - val_loss: 0.7051 - val_acc: 0.7488 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5740 - acc: 0.7809 - val_loss: 0.7200 - val_acc: 0.7459 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5642 - acc: 0.7894 - val_loss: 0.6857 - val_acc: 0.7547 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5602 - acc: 0.7873 - val_loss: 0.6770 - val_acc: 0.7611 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5585 - acc: 0.7918 - val_loss: 0.6944 - val_acc: 0.7471 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5705 - acc: 0.7831 - val_loss: 0.7029 - val_acc: 0.7453 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5709 - acc: 0.7803 - val_loss: 0.6851 - val_acc: 0.7506 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5656 - acc: 0.7897 - val_loss: 0.6605 - val_acc: 0.7576 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5637 - acc: 0.7946 - val_loss: 0.7495 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5578 - acc: 0.7891 - val_loss: 0.6633 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5592 - acc: 0.7864 - val_loss: 0.6834 - val_acc: 0.7529 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5612 - acc: 0.7918 - val_loss: 0.6766 - val_acc: 0.7599 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5630 - acc: 0.7855 - val_loss: 0.6722 - val_acc: 0.7622 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5661 - acc: 0.7867 - val_loss: 0.6852 - val_acc: 0.7494 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5624 - acc: 0.7897 - val_loss: 0.7353 - val_acc: 0.7360 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5600 - acc: 0.7915 - val_loss: 0.6947 - val_acc: 0.7517 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5581 - acc: 0.7903 - val_loss: 0.7034 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5666 - acc: 0.7852 - val_loss: 0.6631 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5547 - acc: 0.7900 - val_loss: 0.6865 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5535 - acc: 0.7936 - val_loss: 0.6716 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5564 - acc: 0.7882 - val_loss: 0.6807 - val_acc: 0.7541 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5590 - acc: 0.7861 - val_loss: 0.7238 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5634 - acc: 0.7918 - val_loss: 0.7080 - val_acc: 0.7488 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5605 - acc: 0.7909 - val_loss: 0.6602 - val_acc: 0.7657 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5613 - acc: 0.7858 - val_loss: 0.6703 - val_acc: 0.7570 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5644 - acc: 0.7867 - val_loss: 0.6857 - val_acc: 0.7593 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5585 - acc: 0.7927 - val_loss: 0.6852 - val_acc: 0.7512 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5501 - acc: 0.7943 - val_loss: 0.6841 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5488 - acc: 0.7958 - val_loss: 0.6777 - val_acc: 0.7628 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5677 - acc: 0.7779 - val_loss: 0.6701 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5622 - acc: 0.7900 - val_loss: 0.6727 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5548 - acc: 0.7858 - val_loss: 0.7269 - val_acc: 0.7395 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5590 - acc: 0.7897 - val_loss: 0.6763 - val_acc: 0.7605 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5554 - acc: 0.7891 - val_loss: 0.6890 - val_acc: 0.7535 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5603 - acc: 0.7864 - val_loss: 0.6787 - val_acc: 0.7523 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5528 - acc: 0.7903 - val_loss: 0.6851 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5655 - acc: 0.7870 - val_loss: 0.7095 - val_acc: 0.7477 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5599 - acc: 0.7879 - val_loss: 0.6560 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5593 - acc: 0.7870 - val_loss: 0.6866 - val_acc: 0.7517 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5701 - acc: 0.7825 - val_loss: 0.6904 - val_acc: 0.7558 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5687 - acc: 0.7809 - val_loss: 0.6581 - val_acc: 0.7611 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5625 - acc: 0.7891 - val_loss: 0.7525 - val_acc: 0.7348 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5612 - acc: 0.7909 - val_loss: 0.6591 - val_acc: 0.7617 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5607 - acc: 0.7942 - val_loss: 0.6717 - val_acc: 0.7564 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "26/26 [==============================] - 2s 92ms/step - loss: 0.5663 - acc: 0.7846 - val_loss: 0.6759 - val_acc: 0.7587 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5631 - acc: 0.7879 - val_loss: 0.6874 - val_acc: 0.7529 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5643 - acc: 0.7876 - val_loss: 0.7191 - val_acc: 0.7436 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "26/26 [==============================] - 2s 94ms/step - loss: 0.5562 - acc: 0.7949 - val_loss: 0.6603 - val_acc: 0.7582 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5491 - acc: 0.7924 - val_loss: 0.7414 - val_acc: 0.7337 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "26/26 [==============================] - 2s 93ms/step - loss: 0.5613 - acc: 0.7891 - val_loss: 0.6684 - val_acc: 0.7506 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6633 - acc: 0.7609\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.6923 - acc: 0.7487\n",
            "epsilon: 0.003 and test evaluation : 0.6922819018363953, 0.7486910820007324\n",
            "SNR: 50.23353576660156\n",
            "18/18 [==============================] - 0s 11ms/step - loss: 0.7124 - acc: 0.7382\n",
            "epsilon: 0.005 and test evaluation : 0.7124205827713013, 0.7382199168205261\n",
            "SNR: 45.796241760253906\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.7652 - acc: 0.7120\n",
            "epsilon: 0.01 and test evaluation : 0.765203595161438, 0.7120419144630432\n",
            "SNR: 39.77564811706543\n",
            "18/18 [==============================] - 0s 10ms/step - loss: 0.8815 - acc: 0.6702\n",
            "epsilon: 0.02 and test evaluation : 0.8814883232116699, 0.6701570749282837\n",
            "SNR: 33.75504732131958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjkUsNKHq_L5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_adv_df[\"acc_clean_mean\"]= np.sum(result_adv_df['acc_clean'])/3.0\n",
        "result_adv_df[\"acc_0.003_mean\"]= np.sum(result_adv_df['acc1'])/3.0\n",
        "result_adv_df[\"acc_0.005_mean\"]= np.sum(result_adv_df['acc2'])/3.0\n",
        "result_adv_df[\"acc_0.02_mean\"]= np.sum(result_adv_df['acc3'])/3.0\n",
        "result_adv_df[\"acc_0.01_mean\"]= np.sum(result_adv_df['acc4'])/3.0"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iYansuX0T5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "5ee669ca-6a1c-4f9c-ba62-5b221a5d44bb"
      },
      "source": [
        "result_adv_df.head(1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.650385</td>\n",
              "      <td>0.759162</td>\n",
              "      <td>0.677746</td>\n",
              "      <td>0.746946</td>\n",
              "      <td>0.696518</td>\n",
              "      <td>0.736475</td>\n",
              "      <td>0.74509</td>\n",
              "      <td>0.726003</td>\n",
              "      <td>0.849999</td>\n",
              "      <td>0.673647</td>\n",
              "      <td>0.739965</td>\n",
              "      <td>0.727749</td>\n",
              "      <td>0.719604</td>\n",
              "      <td>0.702734</td>\n",
              "      <td>0.656195</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    0.650385   0.759162  ...       0.702734       0.656195\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}