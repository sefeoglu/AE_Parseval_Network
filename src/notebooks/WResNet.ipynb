{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "WResNet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM4Jt-bjlVI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba64b99b-3604-46a0-eada-1d4e58906a74"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z75KUVuslVJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZqJ5pXWlVJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "7f75a26d-8fde-45d0-c94f-d6b17d4bd89d"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8645891680405088204\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16073571077437798775\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 7000102027655140843\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11150726272\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5403499405147449980\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTNQ1F1hlVJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de9874ac-fb9a-4890-e01b-f1d9403ec1ba"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0005\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.common.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.common.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.common.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from keras.utils import plot_model\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "\n",
        "    init = (68, 100,1)\n",
        "\n",
        "    wrn_28_10 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.0)\n",
        "\n",
        "    wrn_28_10.summary()\n",
        "\n",
        "   # plot_model(wrn_28_10, \"WRN-16-2.png\", show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wide Residual Network-16-2 created.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 68, 100, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 68, 100, 16)  144         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 68, 100, 16)  64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 68, 100, 16)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 68, 100, 32)  4608        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 68, 100, 32)  128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 68, 100, 32)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 68, 100, 32)  9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 68, 100, 32)  512         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 68, 100, 32)  0           conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 68, 100, 32)  128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 68, 100, 32)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 68, 100, 32)  9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 68, 100, 32)  128         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 68, 100, 32)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 68, 100, 32)  9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 68, 100, 32)  0           add_1[0][0]                      \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 68, 100, 32)  128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 68, 100, 32)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 34, 50, 64)   18432       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 34, 50, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 34, 50, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 34, 50, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 34, 50, 64)   2048        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 34, 50, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 34, 50, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 34, 50, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 34, 50, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 34, 50, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 34, 50, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 34, 50, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 34, 50, 64)   0           add_3[0][0]                      \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 34, 50, 64)   256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 34, 50, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 17, 25, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 17, 25, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 17, 25, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 17, 25, 128)  147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 17, 25, 128)  8192        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 17, 25, 128)  0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 17, 25, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 17, 25, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 17, 25, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 17, 25, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 17, 25, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 17, 25, 128)  147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 17, 25, 128)  0           add_5[0][0]                      \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 17, 25, 128)  512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 17, 25, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 2, 3, 128)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 768)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            3076        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 694,996\n",
            "Trainable params: 693,172\n",
            "Non-trainable params: 1,824\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjrjOyAlVJR",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDBUYk7UlVJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVYpCeUrlVJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f62a6258-50d9-49d2-db5c-c63fe53c870f"
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(row['crop'])\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 68, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9pAOHKlVJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_data_X, Y_data, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXkjB3OlVJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdneiMC7lVJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# creating initial dataframe\n",
        "\n",
        "y_train_df = pd.DataFrame(y_train, columns=['Label'])\n",
        "# creating instance of labelencoder\n",
        "labelencoder = LabelEncoder()\n",
        "# Assigning numerical values and storing in another column\n",
        "y_train_df['New'] = labelencoder.fit_transform(y_train_df['Label'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['Label'])\n",
        "# creating instance of labelencoder\n",
        "labelencoder = LabelEncoder()\n",
        "# Assigning numerical values and storing in another column\n",
        "y_test_df['New'] = labelencoder.fit_transform(y_test_df['Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbRj89tlVJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA9IwliGlVJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.callbacks as callbacks\n",
        "import keras.utils.np_utils as kutils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMgoWxaklVJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100\n",
        "BS = 128\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "horizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrX48KBplVJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X_train[0].shape\n",
        "\n",
        "\n",
        "# transform data set\n",
        "if K.common.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnAMOCV-lVJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "from keras.optimizers import SGD\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.1\n",
        "  drop = 0.2\n",
        "  epochs_drop = 60\n",
        "  lrate = 0.1\n",
        "  if math.floor((1+epoch)/epochs_drop) !=0:\n",
        "    lrate = initial_lrate*math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        " \n",
        "sgd = SGD(lr=0.1, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-tuVr6lVJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8a33c62-949a-4782-f445-0895dae405fa"
      },
      "source": [
        "wrn_28_10.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl_-nmapa7qr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10d983a5-40a1-4742-8350-05357d9d6175"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# learning schedule callback\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [lrate]\n",
        "# Fit the model\n",
        "hist = wrn_28_10.fit(X_train, to_categorical(y_train_df['New']), validation_split=0.33, epochs=EPOCHS, batch_size=BS, verbose=2, callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2568/2568 [==============================] - 24s 9ms/step - loss: 2.4853 - acc: 0.3326 - val_loss: 2.5886 - val_acc: 0.2751\n",
            "Epoch 2/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.3577 - acc: 0.3898 - val_loss: 2.4395 - val_acc: 0.3462\n",
            "Epoch 3/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.2977 - acc: 0.4073 - val_loss: 2.2589 - val_acc: 0.4229\n",
            "Epoch 4/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.2581 - acc: 0.4143 - val_loss: 2.2949 - val_acc: 0.4174\n",
            "Epoch 5/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.1728 - acc: 0.4330 - val_loss: 2.1829 - val_acc: 0.4245\n",
            "Epoch 6/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.0995 - acc: 0.4529 - val_loss: 2.2563 - val_acc: 0.4134\n",
            "Epoch 7/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 2.0170 - acc: 0.4891 - val_loss: 2.0742 - val_acc: 0.4664\n",
            "Epoch 8/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.9700 - acc: 0.5047 - val_loss: 2.1736 - val_acc: 0.3968\n",
            "Epoch 9/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.9203 - acc: 0.5265 - val_loss: 2.0902 - val_acc: 0.4387\n",
            "Epoch 10/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.8420 - acc: 0.5615 - val_loss: 1.9860 - val_acc: 0.4791\n",
            "Epoch 11/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.7621 - acc: 0.5744 - val_loss: 1.8371 - val_acc: 0.5447\n",
            "Epoch 12/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.7019 - acc: 0.5985 - val_loss: 1.7257 - val_acc: 0.5676\n",
            "Epoch 13/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.6334 - acc: 0.6211 - val_loss: 1.8325 - val_acc: 0.5431\n",
            "Epoch 14/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.6678 - acc: 0.5814 - val_loss: 1.6771 - val_acc: 0.5881\n",
            "Epoch 15/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.5466 - acc: 0.6359 - val_loss: 1.7750 - val_acc: 0.5644\n",
            "Epoch 16/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.5650 - acc: 0.6168 - val_loss: 1.6414 - val_acc: 0.5960\n",
            "Epoch 17/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.4599 - acc: 0.6651 - val_loss: 1.5073 - val_acc: 0.6466\n",
            "Epoch 18/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.3653 - acc: 0.6776 - val_loss: 1.8107 - val_acc: 0.5012\n",
            "Epoch 19/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.5064 - acc: 0.6281 - val_loss: 2.0860 - val_acc: 0.4292\n",
            "Epoch 20/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.4436 - acc: 0.6507 - val_loss: 1.4484 - val_acc: 0.6458\n",
            "Epoch 21/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.3039 - acc: 0.7091 - val_loss: 1.6909 - val_acc: 0.5897\n",
            "Epoch 22/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.3592 - acc: 0.6636 - val_loss: 1.3708 - val_acc: 0.6577\n",
            "Epoch 23/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.2641 - acc: 0.7083 - val_loss: 1.5084 - val_acc: 0.5984\n",
            "Epoch 24/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.2400 - acc: 0.7142 - val_loss: 1.5047 - val_acc: 0.6016\n",
            "Epoch 25/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.1788 - acc: 0.7247 - val_loss: 1.4532 - val_acc: 0.6206\n",
            "Epoch 26/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.1082 - acc: 0.7590 - val_loss: 1.4753 - val_acc: 0.6166\n",
            "Epoch 27/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.1697 - acc: 0.7138 - val_loss: 1.4514 - val_acc: 0.6269\n",
            "Epoch 28/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.0750 - acc: 0.7687 - val_loss: 1.4685 - val_acc: 0.6395\n",
            "Epoch 29/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.0041 - acc: 0.7843 - val_loss: 1.3194 - val_acc: 0.6783\n",
            "Epoch 30/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.0600 - acc: 0.7570 - val_loss: 1.3645 - val_acc: 0.6545\n",
            "Epoch 31/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.0497 - acc: 0.7539 - val_loss: 1.3318 - val_acc: 0.6411\n",
            "Epoch 32/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.9413 - acc: 0.7991 - val_loss: 1.5271 - val_acc: 0.5779\n",
            "Epoch 33/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.9953 - acc: 0.7714 - val_loss: 1.3267 - val_acc: 0.6609\n",
            "Epoch 34/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8967 - acc: 0.8107 - val_loss: 1.4197 - val_acc: 0.6553\n",
            "Epoch 35/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8147 - acc: 0.8466 - val_loss: 1.2999 - val_acc: 0.6893\n",
            "Epoch 36/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7755 - acc: 0.8540 - val_loss: 1.4454 - val_acc: 0.6862\n",
            "Epoch 37/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7373 - acc: 0.8734 - val_loss: 1.5153 - val_acc: 0.6490\n",
            "Epoch 38/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7533 - acc: 0.8606 - val_loss: 1.4757 - val_acc: 0.6395\n",
            "Epoch 39/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6838 - acc: 0.8851 - val_loss: 1.4070 - val_acc: 0.6656\n",
            "Epoch 40/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7209 - acc: 0.8711 - val_loss: 1.3968 - val_acc: 0.6964\n",
            "Epoch 41/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5982 - acc: 0.9229 - val_loss: 1.3581 - val_acc: 0.6862\n",
            "Epoch 42/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5528 - acc: 0.9361 - val_loss: 1.3857 - val_acc: 0.6949\n",
            "Epoch 43/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4917 - acc: 0.9525 - val_loss: 1.5618 - val_acc: 0.6609\n",
            "Epoch 44/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4939 - acc: 0.9537 - val_loss: 1.7721 - val_acc: 0.6735\n",
            "Epoch 45/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6820 - acc: 0.8731 - val_loss: 2.2097 - val_acc: 0.6032\n",
            "Epoch 46/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 1.0600 - acc: 0.7426 - val_loss: 1.4596 - val_acc: 0.6474\n",
            "Epoch 47/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8173 - acc: 0.8287 - val_loss: 1.5077 - val_acc: 0.6538\n",
            "Epoch 48/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7703 - acc: 0.8427 - val_loss: 1.7395 - val_acc: 0.6348\n",
            "Epoch 49/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6646 - acc: 0.8879 - val_loss: 1.6342 - val_acc: 0.6680\n",
            "Epoch 50/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6402 - acc: 0.8952 - val_loss: 1.4408 - val_acc: 0.6672\n",
            "Epoch 51/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6041 - acc: 0.9198 - val_loss: 1.4969 - val_acc: 0.6893\n",
            "Epoch 52/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5558 - acc: 0.9350 - val_loss: 1.5173 - val_acc: 0.6877\n",
            "Epoch 53/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4680 - acc: 0.9677 - val_loss: 1.8309 - val_acc: 0.6648\n",
            "Epoch 54/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4601 - acc: 0.9638 - val_loss: 1.6682 - val_acc: 0.6395\n",
            "Epoch 55/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5193 - acc: 0.9369 - val_loss: 1.8315 - val_acc: 0.6727\n",
            "Epoch 56/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5982 - acc: 0.9011 - val_loss: 1.8513 - val_acc: 0.6640\n",
            "Epoch 57/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5026 - acc: 0.9463 - val_loss: 1.8682 - val_acc: 0.6846\n",
            "Epoch 58/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5180 - acc: 0.9369 - val_loss: 1.7047 - val_acc: 0.6498\n",
            "Epoch 59/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4970 - acc: 0.9502 - val_loss: 1.8598 - val_acc: 0.6087\n",
            "Epoch 60/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4266 - acc: 0.9747 - val_loss: 1.5429 - val_acc: 0.6759\n",
            "Epoch 61/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3829 - acc: 0.9864 - val_loss: 1.6608 - val_acc: 0.6925\n",
            "Epoch 62/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3520 - acc: 0.9953 - val_loss: 1.9957 - val_acc: 0.6672\n",
            "Epoch 63/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3357 - acc: 0.9961 - val_loss: 1.6872 - val_acc: 0.6846\n",
            "Epoch 64/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3176 - acc: 0.9988 - val_loss: 1.8168 - val_acc: 0.6751\n",
            "Epoch 65/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2996 - acc: 1.0000 - val_loss: 1.5721 - val_acc: 0.7020\n",
            "Epoch 66/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2857 - acc: 1.0000 - val_loss: 1.5594 - val_acc: 0.7059\n",
            "Epoch 67/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2737 - acc: 1.0000 - val_loss: 1.6225 - val_acc: 0.7036\n",
            "Epoch 68/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2624 - acc: 1.0000 - val_loss: 1.6638 - val_acc: 0.7075\n",
            "Epoch 69/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2517 - acc: 1.0000 - val_loss: 1.5097 - val_acc: 0.6949\n",
            "Epoch 70/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2417 - acc: 1.0000 - val_loss: 1.6374 - val_acc: 0.6870\n",
            "Epoch 71/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2662 - acc: 0.9918 - val_loss: 1.5836 - val_acc: 0.6458\n",
            "Epoch 72/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3094 - acc: 0.9731 - val_loss: 2.9048 - val_acc: 0.5289\n",
            "Epoch 73/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8719 - acc: 0.7776 - val_loss: 2.3979 - val_acc: 0.5123\n",
            "Epoch 74/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8139 - acc: 0.7979 - val_loss: 1.8527 - val_acc: 0.5739\n",
            "Epoch 75/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8213 - acc: 0.8065 - val_loss: 1.6198 - val_acc: 0.5897\n",
            "Epoch 76/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6382 - acc: 0.8672 - val_loss: 1.4561 - val_acc: 0.6561\n",
            "Epoch 77/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4279 - acc: 0.9653 - val_loss: 1.3963 - val_acc: 0.6806\n",
            "Epoch 78/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3468 - acc: 0.9930 - val_loss: 1.5125 - val_acc: 0.6957\n",
            "Epoch 79/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3057 - acc: 0.9977 - val_loss: 1.4843 - val_acc: 0.6846\n",
            "Epoch 80/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3002 - acc: 0.9981 - val_loss: 1.8398 - val_acc: 0.6727\n",
            "Epoch 81/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2820 - acc: 0.9996 - val_loss: 1.6400 - val_acc: 0.6988\n",
            "Epoch 82/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2647 - acc: 1.0000 - val_loss: 1.4924 - val_acc: 0.7107\n",
            "Epoch 83/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2524 - acc: 1.0000 - val_loss: 1.6283 - val_acc: 0.7051\n",
            "Epoch 84/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2431 - acc: 1.0000 - val_loss: 1.6979 - val_acc: 0.6711\n",
            "Epoch 85/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2327 - acc: 1.0000 - val_loss: 1.5751 - val_acc: 0.7051\n",
            "Epoch 86/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2228 - acc: 1.0000 - val_loss: 1.5016 - val_acc: 0.7059\n",
            "Epoch 87/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2137 - acc: 1.0000 - val_loss: 1.4996 - val_acc: 0.7012\n",
            "Epoch 88/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2065 - acc: 1.0000 - val_loss: 1.5409 - val_acc: 0.7091\n",
            "Epoch 89/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1975 - acc: 1.0000 - val_loss: 1.6327 - val_acc: 0.7036\n",
            "Epoch 90/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1892 - acc: 1.0000 - val_loss: 1.6750 - val_acc: 0.6964\n",
            "Epoch 91/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1817 - acc: 1.0000 - val_loss: 1.7204 - val_acc: 0.6838\n",
            "Epoch 92/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1746 - acc: 1.0000 - val_loss: 1.5631 - val_acc: 0.6957\n",
            "Epoch 93/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1689 - acc: 1.0000 - val_loss: 1.3348 - val_acc: 0.7130\n",
            "Epoch 94/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1615 - acc: 1.0000 - val_loss: 1.4901 - val_acc: 0.7051\n",
            "Epoch 95/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1549 - acc: 1.0000 - val_loss: 1.4834 - val_acc: 0.7028\n",
            "Epoch 96/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1484 - acc: 1.0000 - val_loss: 1.4533 - val_acc: 0.7099\n",
            "Epoch 97/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1433 - acc: 1.0000 - val_loss: 1.5184 - val_acc: 0.7043\n",
            "Epoch 98/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1372 - acc: 1.0000 - val_loss: 1.3520 - val_acc: 0.7067\n",
            "Epoch 99/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1317 - acc: 1.0000 - val_loss: 1.3880 - val_acc: 0.6949\n",
            "Epoch 100/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1265 - acc: 1.0000 - val_loss: 1.4483 - val_acc: 0.7051\n",
            "Epoch 101/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1368 - acc: 0.9969 - val_loss: 2.2923 - val_acc: 0.5154\n",
            "Epoch 102/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6328 - acc: 0.8135 - val_loss: 1.5317 - val_acc: 0.5850\n",
            "Epoch 103/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8919 - acc: 0.7352 - val_loss: 1.4733 - val_acc: 0.5502\n",
            "Epoch 104/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8639 - acc: 0.7360 - val_loss: 1.3584 - val_acc: 0.5826\n",
            "Epoch 105/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7111 - acc: 0.8010 - val_loss: 1.3621 - val_acc: 0.6150\n",
            "Epoch 106/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6840 - acc: 0.8240 - val_loss: 1.4978 - val_acc: 0.6158\n",
            "Epoch 107/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5184 - acc: 0.8945 - val_loss: 1.3345 - val_acc: 0.6585\n",
            "Epoch 108/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6208 - acc: 0.8544 - val_loss: 1.5719 - val_acc: 0.5818\n",
            "Epoch 109/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5973 - acc: 0.8703 - val_loss: 1.3205 - val_acc: 0.6435\n",
            "Epoch 110/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4882 - acc: 0.9233 - val_loss: 1.3153 - val_acc: 0.6822\n",
            "Epoch 111/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3970 - acc: 0.9556 - val_loss: 1.4910 - val_acc: 0.6553\n",
            "Epoch 112/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3879 - acc: 0.9607 - val_loss: 1.5039 - val_acc: 0.6672\n",
            "Epoch 113/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3170 - acc: 0.9895 - val_loss: 1.4917 - val_acc: 0.6656\n",
            "Epoch 114/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3155 - acc: 0.9805 - val_loss: 1.6726 - val_acc: 0.6593\n",
            "Epoch 115/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5746 - acc: 0.8894 - val_loss: 1.9259 - val_acc: 0.6103\n",
            "Epoch 116/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6615 - acc: 0.8621 - val_loss: 2.2386 - val_acc: 0.5771\n",
            "Epoch 117/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8161 - acc: 0.8018 - val_loss: 1.5315 - val_acc: 0.6601\n",
            "Epoch 118/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6235 - acc: 0.8882 - val_loss: 1.4340 - val_acc: 0.6680\n",
            "Epoch 119/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4302 - acc: 0.9692 - val_loss: 1.5119 - val_acc: 0.6735\n",
            "Epoch 120/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4128 - acc: 0.9731 - val_loss: 1.6079 - val_acc: 0.6862\n",
            "Epoch 121/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3437 - acc: 0.9942 - val_loss: 1.6029 - val_acc: 0.6980\n",
            "Epoch 122/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3134 - acc: 0.9984 - val_loss: 1.5404 - val_acc: 0.6846\n",
            "Epoch 123/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2944 - acc: 1.0000 - val_loss: 1.5720 - val_acc: 0.7036\n",
            "Epoch 124/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2812 - acc: 1.0000 - val_loss: 1.6102 - val_acc: 0.6751\n",
            "Epoch 125/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2692 - acc: 1.0000 - val_loss: 1.5984 - val_acc: 0.7020\n",
            "Epoch 126/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2581 - acc: 1.0000 - val_loss: 1.5247 - val_acc: 0.6893\n",
            "Epoch 127/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2477 - acc: 1.0000 - val_loss: 1.5140 - val_acc: 0.6909\n",
            "Epoch 128/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2499 - acc: 0.9977 - val_loss: 1.4676 - val_acc: 0.6830\n",
            "Epoch 129/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2366 - acc: 0.9988 - val_loss: 1.6550 - val_acc: 0.6877\n",
            "Epoch 130/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2232 - acc: 1.0000 - val_loss: 1.4535 - val_acc: 0.6846\n",
            "Epoch 131/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2134 - acc: 1.0000 - val_loss: 1.4958 - val_acc: 0.6988\n",
            "Epoch 132/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2044 - acc: 1.0000 - val_loss: 1.5202 - val_acc: 0.6980\n",
            "Epoch 133/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2002 - acc: 1.0000 - val_loss: 1.5856 - val_acc: 0.6727\n",
            "Epoch 134/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1915 - acc: 1.0000 - val_loss: 1.4372 - val_acc: 0.6877\n",
            "Epoch 135/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2015 - acc: 0.9965 - val_loss: 1.5357 - val_acc: 0.6838\n",
            "Epoch 136/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2222 - acc: 0.9887 - val_loss: 1.5526 - val_acc: 0.6577\n",
            "Epoch 137/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2224 - acc: 0.9891 - val_loss: 1.5679 - val_acc: 0.6719\n",
            "Epoch 138/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2101 - acc: 0.9914 - val_loss: 1.5517 - val_acc: 0.6941\n",
            "Epoch 139/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1894 - acc: 0.9973 - val_loss: 1.5258 - val_acc: 0.6830\n",
            "Epoch 140/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1805 - acc: 0.9988 - val_loss: 1.7127 - val_acc: 0.6672\n",
            "Epoch 141/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1708 - acc: 0.9992 - val_loss: 1.5329 - val_acc: 0.6925\n",
            "Epoch 142/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1630 - acc: 0.9996 - val_loss: 1.5144 - val_acc: 0.6933\n",
            "Epoch 143/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1558 - acc: 1.0000 - val_loss: 1.5339 - val_acc: 0.6798\n",
            "Epoch 144/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1496 - acc: 1.0000 - val_loss: 1.4147 - val_acc: 0.6996\n",
            "Epoch 145/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1437 - acc: 1.0000 - val_loss: 1.8658 - val_acc: 0.6617\n",
            "Epoch 146/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1601 - acc: 0.9934 - val_loss: 1.8929 - val_acc: 0.6419\n",
            "Epoch 147/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2084 - acc: 0.9770 - val_loss: 2.5150 - val_acc: 0.5692\n",
            "Epoch 148/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3993 - acc: 0.9062 - val_loss: 3.1484 - val_acc: 0.4909\n",
            "Epoch 149/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6753 - acc: 0.8232 - val_loss: 2.1410 - val_acc: 0.5597\n",
            "Epoch 150/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.8492 - acc: 0.7652 - val_loss: 1.6973 - val_acc: 0.5739\n",
            "Epoch 151/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7328 - acc: 0.8189 - val_loss: 1.7210 - val_acc: 0.6190\n",
            "Epoch 152/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6418 - acc: 0.8606 - val_loss: 1.3624 - val_acc: 0.6538\n",
            "Epoch 153/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5138 - acc: 0.9147 - val_loss: 1.3963 - val_acc: 0.6617\n",
            "Epoch 154/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3897 - acc: 0.9642 - val_loss: 1.6114 - val_acc: 0.6545\n",
            "Epoch 155/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3382 - acc: 0.9852 - val_loss: 1.5638 - val_acc: 0.6664\n",
            "Epoch 156/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2966 - acc: 0.9973 - val_loss: 1.6148 - val_acc: 0.6759\n",
            "Epoch 157/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2723 - acc: 1.0000 - val_loss: 1.6680 - val_acc: 0.6640\n",
            "Epoch 158/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2581 - acc: 1.0000 - val_loss: 1.6024 - val_acc: 0.6846\n",
            "Epoch 159/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2466 - acc: 1.0000 - val_loss: 1.6017 - val_acc: 0.6822\n",
            "Epoch 160/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2365 - acc: 1.0000 - val_loss: 1.7742 - val_acc: 0.6593\n",
            "Epoch 161/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2269 - acc: 1.0000 - val_loss: 1.6039 - val_acc: 0.6846\n",
            "Epoch 162/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2177 - acc: 1.0000 - val_loss: 1.5349 - val_acc: 0.6830\n",
            "Epoch 163/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2089 - acc: 1.0000 - val_loss: 1.5981 - val_acc: 0.6798\n",
            "Epoch 164/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2008 - acc: 1.0000 - val_loss: 1.6466 - val_acc: 0.6696\n",
            "Epoch 165/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1926 - acc: 1.0000 - val_loss: 1.5523 - val_acc: 0.6901\n",
            "Epoch 166/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1847 - acc: 1.0000 - val_loss: 1.5816 - val_acc: 0.6909\n",
            "Epoch 167/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1773 - acc: 1.0000 - val_loss: 1.4289 - val_acc: 0.6909\n",
            "Epoch 168/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1701 - acc: 1.0000 - val_loss: 1.5295 - val_acc: 0.6791\n",
            "Epoch 169/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1633 - acc: 1.0000 - val_loss: 1.5581 - val_acc: 0.6798\n",
            "Epoch 170/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1568 - acc: 1.0000 - val_loss: 1.5805 - val_acc: 0.6727\n",
            "Epoch 171/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1504 - acc: 1.0000 - val_loss: 1.4308 - val_acc: 0.6791\n",
            "Epoch 172/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1445 - acc: 1.0000 - val_loss: 1.4852 - val_acc: 0.6917\n",
            "Epoch 173/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1389 - acc: 1.0000 - val_loss: 1.4789 - val_acc: 0.6862\n",
            "Epoch 174/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1333 - acc: 1.0000 - val_loss: 1.4892 - val_acc: 0.6830\n",
            "Epoch 175/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1280 - acc: 1.0000 - val_loss: 1.5990 - val_acc: 0.6862\n",
            "Epoch 176/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1242 - acc: 1.0000 - val_loss: 1.4706 - val_acc: 0.6893\n",
            "Epoch 177/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1210 - acc: 1.0000 - val_loss: 1.6397 - val_acc: 0.6735\n",
            "Epoch 178/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1178 - acc: 1.0000 - val_loss: 1.5450 - val_acc: 0.6751\n",
            "Epoch 179/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1167 - acc: 0.9988 - val_loss: 1.5012 - val_acc: 0.6711\n",
            "Epoch 180/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1242 - acc: 0.9977 - val_loss: 1.4734 - val_acc: 0.6585\n",
            "Epoch 181/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.1458 - acc: 0.9910 - val_loss: 1.8458 - val_acc: 0.5929\n",
            "Epoch 182/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4015 - acc: 0.8929 - val_loss: 2.3494 - val_acc: 0.5605\n",
            "Epoch 183/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6225 - acc: 0.8318 - val_loss: 2.8177 - val_acc: 0.4704\n",
            "Epoch 184/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.6548 - acc: 0.8232 - val_loss: 1.4258 - val_acc: 0.6134\n",
            "Epoch 185/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4107 - acc: 0.9241 - val_loss: 1.3419 - val_acc: 0.6538\n",
            "Epoch 186/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.4184 - acc: 0.9307 - val_loss: 1.5531 - val_acc: 0.6482\n",
            "Epoch 187/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3655 - acc: 0.9529 - val_loss: 1.7247 - val_acc: 0.6601\n",
            "Epoch 188/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3857 - acc: 0.9552 - val_loss: 2.1297 - val_acc: 0.6119\n",
            "Epoch 189/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.7203 - acc: 0.8322 - val_loss: 1.4205 - val_acc: 0.6593\n",
            "Epoch 190/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.5075 - acc: 0.9252 - val_loss: 1.3616 - val_acc: 0.6727\n",
            "Epoch 191/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3808 - acc: 0.9743 - val_loss: 1.5435 - val_acc: 0.6783\n",
            "Epoch 192/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.3253 - acc: 0.9938 - val_loss: 1.5560 - val_acc: 0.6767\n",
            "Epoch 193/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2950 - acc: 0.9977 - val_loss: 1.5642 - val_acc: 0.6846\n",
            "Epoch 194/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2753 - acc: 1.0000 - val_loss: 1.5362 - val_acc: 0.6743\n",
            "Epoch 195/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2622 - acc: 1.0000 - val_loss: 1.6314 - val_acc: 0.6791\n",
            "Epoch 196/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2510 - acc: 1.0000 - val_loss: 1.5362 - val_acc: 0.6862\n",
            "Epoch 197/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2407 - acc: 1.0000 - val_loss: 1.6227 - val_acc: 0.6767\n",
            "Epoch 198/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2308 - acc: 1.0000 - val_loss: 1.5562 - val_acc: 0.6664\n",
            "Epoch 199/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2214 - acc: 1.0000 - val_loss: 1.5685 - val_acc: 0.6759\n",
            "Epoch 200/200\n",
            "2568/2568 [==============================] - 18s 7ms/step - loss: 0.2124 - acc: 1.0000 - val_loss: 1.7797 - val_acc: 0.6451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrpg9KMplVKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "#wrn_28_10.fit_generator(aug.flow(X_train, to_categorical(y_train_df['New']), batch_size=BS),validation_data=(X_test, to_categorical(y_test_df['New'])), validation_steps=X_test.shape[0] // BS,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJysdIN3lVKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b65c3460-0b1d-485f-d8d9-c408f78430b7"
      },
      "source": [
        "import keras.utils.np_utils as kutils\n",
        "yPreds = wrn_28_10.predict(X_test)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yPred[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usjrLCh6lVKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bac657ca-6d76-4fec-d8f2-cf04c6a7b1cc"
      },
      "source": [
        "yPred[2]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4N7HDg0lVKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce93833e-db15-45c8-aa3d-d4f4f7598d9c"
      },
      "source": [
        "yTrue = y_test_df['New']\n",
        "yTrue[2]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gojN0iDIlVKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "acf20d24-a8dc-4401-a0ee-ca37c34997a6"
      },
      "source": [
        "yPreds"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.25724576e-03, 1.69853563e-03, 6.39387453e-03, 9.88650441e-01],\n",
              "       [4.32612467e-07, 9.99977827e-01, 2.16743283e-05, 1.26896493e-08],\n",
              "       [4.58722860e-01, 5.26561975e-01, 1.02345766e-02, 4.48056357e-03],\n",
              "       ...,\n",
              "       [1.05931445e-13, 1.57635766e-10, 1.00000000e+00, 2.27572058e-10],\n",
              "       [4.43276912e-02, 3.64337802e-01, 5.68621099e-01, 2.27134004e-02],\n",
              "       [1.48276554e-03, 1.05738651e-03, 9.97195363e-01, 2.64485716e-04]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4m2vMP7lVKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f930ab50-3058-4d96-bce2-50187dcc4fcc"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  66.49020645844362\n",
            "Error :  33.50979354155638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z8YVCPklVKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4a53fabe-05fe-4028-a875-8bbf2f914deb"
      },
      "source": [
        "from matplotlib import  pyplot\n",
        "\n",
        "\n",
        "pyplot.plot(hist.history[\"acc\"], label='train')\n",
        "pyplot.plot(hist.history['val_acc'], label='test')\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f05323cddd8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKg2w43tlVKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00e66114-861e-4681-c899-cabf05bf9061"
      },
      "source": [
        "hist.history['loss']\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4852685898635247,\n",
              " 2.3577441656700917,\n",
              " 2.2976779514384047,\n",
              " 2.2581057058316527,\n",
              " 2.172838179121879,\n",
              " 2.0995072338068597,\n",
              " 2.016986147265568,\n",
              " 1.9700154433740633,\n",
              " 1.9203159051520802,\n",
              " 1.8420374044376742,\n",
              " 1.7620685821010316,\n",
              " 1.7019386336068127,\n",
              " 1.633372816341317,\n",
              " 1.667760568987172,\n",
              " 1.54664027059561,\n",
              " 1.5650473420865068,\n",
              " 1.4599247951745244,\n",
              " 1.3653090003866273,\n",
              " 1.5064145014664838,\n",
              " 1.4436111754717484,\n",
              " 1.3039106955038053,\n",
              " 1.35924953537938,\n",
              " 1.2641059732140039,\n",
              " 1.2399832572892449,\n",
              " 1.1787623495699089,\n",
              " 1.1082134391659888,\n",
              " 1.1696957535461474,\n",
              " 1.0750275664611768,\n",
              " 1.0041106716494694,\n",
              " 1.0599869341122399,\n",
              " 1.049710791244685,\n",
              " 0.941314704692995,\n",
              " 0.9953079357325474,\n",
              " 0.8966918051057144,\n",
              " 0.8146872180644597,\n",
              " 0.7754658206229641,\n",
              " 0.7372594061670273,\n",
              " 0.7532974366458406,\n",
              " 0.6837602549252851,\n",
              " 0.7208685266823041,\n",
              " 0.5981945119728552,\n",
              " 0.552755093184587,\n",
              " 0.49170571807017577,\n",
              " 0.4938759761064595,\n",
              " 0.6820475323549312,\n",
              " 1.0600451145588052,\n",
              " 0.8172963107486381,\n",
              " 0.7703278302403626,\n",
              " 0.6646137738896307,\n",
              " 0.6401679872352386,\n",
              " 0.6040715985580397,\n",
              " 0.5557588114173985,\n",
              " 0.46797231591750527,\n",
              " 0.46007543542303403,\n",
              " 0.5193291113384044,\n",
              " 0.5982187551872753,\n",
              " 0.5026499375561687,\n",
              " 0.518007412135044,\n",
              " 0.4969692726001561,\n",
              " 0.42658350270856576,\n",
              " 0.3829443412963475,\n",
              " 0.35200318981925277,\n",
              " 0.3356572475017417,\n",
              " 0.31763398981540003,\n",
              " 0.2995649161182831,\n",
              " 0.2857485097888103,\n",
              " 0.27373132128210453,\n",
              " 0.2623550152852899,\n",
              " 0.2517214171425947,\n",
              " 0.2416743069794319,\n",
              " 0.2662116272427211,\n",
              " 0.3093713084670985,\n",
              " 0.8719228832892539,\n",
              " 0.8138889664801482,\n",
              " 0.8212870364991304,\n",
              " 0.6381546418614849,\n",
              " 0.4278596888450076,\n",
              " 0.34680516326167504,\n",
              " 0.3056828043542547,\n",
              " 0.3002414844489172,\n",
              " 0.2819660727480119,\n",
              " 0.264672643763254,\n",
              " 0.2523520163657881,\n",
              " 0.24305415636282474,\n",
              " 0.23265558202690054,\n",
              " 0.2227836142633563,\n",
              " 0.21369043048297134,\n",
              " 0.20649932946928565,\n",
              " 0.19752833015079438,\n",
              " 0.18916124973527368,\n",
              " 0.1816667771877901,\n",
              " 0.17460862628396054,\n",
              " 0.16888773534156823,\n",
              " 0.16149019131964984,\n",
              " 0.1548579114248448,\n",
              " 0.1484137680671668,\n",
              " 0.14330236664813628,\n",
              " 0.13720144187549935,\n",
              " 0.1316574064556312,\n",
              " 0.12645937265636764,\n",
              " 0.13681208864550723,\n",
              " 0.6328024011906063,\n",
              " 0.8919163088189479,\n",
              " 0.8638731382346228,\n",
              " 0.711080555603883,\n",
              " 0.6839930311170322,\n",
              " 0.5184448307548356,\n",
              " 0.6207522688625015,\n",
              " 0.597296137119008,\n",
              " 0.4881631557443803,\n",
              " 0.3970265345781392,\n",
              " 0.38789035737328814,\n",
              " 0.3169669793959347,\n",
              " 0.315468513891333,\n",
              " 0.5745615760485331,\n",
              " 0.6615479456672787,\n",
              " 0.8161347228047261,\n",
              " 0.6235427452582065,\n",
              " 0.43018379575366916,\n",
              " 0.41278180062213793,\n",
              " 0.34365154389651764,\n",
              " 0.3133853758421271,\n",
              " 0.2943869733550467,\n",
              " 0.28120465850532983,\n",
              " 0.269207330693337,\n",
              " 0.25813268658899446,\n",
              " 0.24769673856247995,\n",
              " 0.24993173903393967,\n",
              " 0.2366357585908468,\n",
              " 0.223169865034451,\n",
              " 0.21340073139125312,\n",
              " 0.20439139110648372,\n",
              " 0.20020063049696687,\n",
              " 0.19145769498244253,\n",
              " 0.20148551607986107,\n",
              " 0.2222137781689843,\n",
              " 0.2224308146958782,\n",
              " 0.21008346622049623,\n",
              " 0.18938875801838076,\n",
              " 0.18049460723764058,\n",
              " 0.17079555770132773,\n",
              " 0.16302356133208468,\n",
              " 0.1558013120637133,\n",
              " 0.14957403927762933,\n",
              " 0.14372689390665275,\n",
              " 0.1600512184457987,\n",
              " 0.20835378376122948,\n",
              " 0.399276676950425,\n",
              " 0.6753274343466833,\n",
              " 0.8492364352365892,\n",
              " 0.7327507660025006,\n",
              " 0.6417608758742193,\n",
              " 0.5137862266038438,\n",
              " 0.38974594568537774,\n",
              " 0.33822481979462216,\n",
              " 0.2966434050386197,\n",
              " 0.2722572915464918,\n",
              " 0.25809723955819913,\n",
              " 0.24664780195814057,\n",
              " 0.23647396948850044,\n",
              " 0.22692676635917475,\n",
              " 0.21765424682529544,\n",
              " 0.20885559990769978,\n",
              " 0.20076625893977573,\n",
              " 0.1925742750598634,\n",
              " 0.18468089341374572,\n",
              " 0.17725841246104315,\n",
              " 0.17012802434858876,\n",
              " 0.16331622510498559,\n",
              " 0.15680758111944823,\n",
              " 0.15042087667827667,\n",
              " 0.14450057036594438,\n",
              " 0.1389137505556564,\n",
              " 0.13334058740428675,\n",
              " 0.12798441524074827,\n",
              " 0.12417982504746625,\n",
              " 0.12097365397530553,\n",
              " 0.11782369913527527,\n",
              " 0.11672758069643721,\n",
              " 0.12421695572379222,\n",
              " 0.14578709721193878,\n",
              " 0.4015156639699253,\n",
              " 0.622450535542497,\n",
              " 0.6547642795839043,\n",
              " 0.4107158850657977,\n",
              " 0.41844572790686585,\n",
              " 0.36552326757217124,\n",
              " 0.3857127933115974,\n",
              " 0.7202832792592568,\n",
              " 0.5075109680865041,\n",
              " 0.38080025378417376,\n",
              " 0.3253413203906419,\n",
              " 0.2950261783005664,\n",
              " 0.27528417723201143,\n",
              " 0.26222188209076164,\n",
              " 0.2510450064578903,\n",
              " 0.24072160513787255,\n",
              " 0.230842066238231,\n",
              " 0.22138174764835203,\n",
              " 0.21244778686037688]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpzTXjevlVKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppO4rXhlVKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TnqtfIlVKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDXyYhWU24G5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UcghrO8lVKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}