{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "WResNet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM4Jt-bjlVI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974467ad-b92f-4445-de45-9d58c5d3590e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z75KUVuslVJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZqJ5pXWlVJH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2321e4f7-df78-448b-80f3-55e1269f2235"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7182129534661233486\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 16462655083208677224\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 5456555619530537685\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11150726272\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 4317554192042199169\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTNQ1F1hlVJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d19ece70-b95c-4ce2-b8fb-f2eba3c9b2ce"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0005\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.common.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.common.image_dim_ordering() == \"th\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      W_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.common.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, W_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from keras.utils import plot_model\n",
        "    from keras.layers import Input\n",
        "    from keras.models import Model\n",
        "\n",
        "    init = (68, 100,1)\n",
        "\n",
        "    wrn_28_10 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.0)\n",
        "\n",
        "    wrn_28_10.summary()\n",
        "\n",
        "   # plot_model(wrn_28_10, \"WRN-16-2.png\", show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wide Residual Network-16-2 created.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 68, 100, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 68, 100, 16)  144         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 68, 100, 16)  64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 68, 100, 16)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 68, 100, 32)  4608        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 68, 100, 32)  128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 68, 100, 32)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 68, 100, 32)  9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 68, 100, 32)  512         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 68, 100, 32)  0           conv2d_3[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 68, 100, 32)  128         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 68, 100, 32)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 68, 100, 32)  9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 68, 100, 32)  128         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 68, 100, 32)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 68, 100, 32)  9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 68, 100, 32)  0           add_1[0][0]                      \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 68, 100, 32)  128         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 68, 100, 32)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 34, 50, 64)   18432       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 34, 50, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 34, 50, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 34, 50, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 34, 50, 64)   2048        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 34, 50, 64)   0           conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 34, 50, 64)   256         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 34, 50, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 34, 50, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 34, 50, 64)   256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 34, 50, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 34, 50, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 34, 50, 64)   0           add_3[0][0]                      \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 34, 50, 64)   256         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 34, 50, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 17, 25, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 17, 25, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 17, 25, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 17, 25, 128)  147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 17, 25, 128)  8192        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 17, 25, 128)  0           conv2d_13[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 17, 25, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 17, 25, 128)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 17, 25, 128)  147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 17, 25, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 17, 25, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 17, 25, 128)  147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 17, 25, 128)  0           add_5[0][0]                      \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 17, 25, 128)  512         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 17, 25, 128)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 2, 3, 128)    0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 768)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4)            3076        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 694,996\n",
            "Trainable params: 693,172\n",
            "Non-trainable params: 1,824\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjrjOyAlVJR",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDBUYk7UlVJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVYpCeUrlVJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cffd38d9-7193-4322-f6eb-36ba40c2b408"
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(row['crop'])\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 68, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR9pAOHKlVJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_data_X, Y_data, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXkjB3OlVJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdneiMC7lVJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# creating initial dataframe\n",
        "\n",
        "y_train_df = pd.DataFrame(y_train, columns=['Label'])\n",
        "# creating instance of labelencoder\n",
        "labelencoder = LabelEncoder()\n",
        "# Assigning numerical values and storing in another column\n",
        "y_train_df['New'] = labelencoder.fit_transform(y_train_df['Label'])\n",
        "y_test_df = pd.DataFrame(y_test, columns=['Label'])\n",
        "# creating instance of labelencoder\n",
        "labelencoder = LabelEncoder()\n",
        "# Assigning numerical values and storing in another column\n",
        "y_test_df['New'] = labelencoder.fit_transform(y_test_df['Label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwbRj89tlVJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA9IwliGlVJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.callbacks as callbacks\n",
        "import keras.utils.np_utils as kutils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMgoWxaklVJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "BS = 32\n",
        "# construct the training image generator for data augmentation\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "horizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrX48KBplVJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X_train[0].shape\n",
        "\n",
        "\n",
        "# transform data set\n",
        "if K.common.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnAMOCV-lVJ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4c9cc9e-65e6-4c23-dbb0-13c9e7405886"
      },
      "source": [
        "opt = tf.optimizers.SGD(0.01)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD at 0x7fc3a90757b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-tuVr6lVJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9eb77606-07e5-4cf7-848b-2859f6297ede"
      },
      "source": [
        "wrn_28_10.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrpg9KMplVKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ca9d8e9-ba0f-4615-a862-255f1bddcb68"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "hist = wrn_28_10.fit_generator(aug.flow(X_train, to_categorical(y_train_df['New']), batch_size=BS),validation_data=(X_test, to_categorical(y_test_df['New'])), steps_per_epoch=len(X_train) // BS,epochs=EPOCHS)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "119/119 [==============================] - 35s 293ms/step - loss: 2.5152 - acc: 0.3344 - val_loss: 2.5049 - val_acc: 0.3843\n",
            "Epoch 2/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.4925 - acc: 0.3752 - val_loss: 2.4670 - val_acc: 0.3774\n",
            "Epoch 3/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.4597 - acc: 0.3717 - val_loss: 2.4537 - val_acc: 0.3722\n",
            "Epoch 4/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.4297 - acc: 0.3804 - val_loss: 2.4450 - val_acc: 0.3621\n",
            "Epoch 5/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.4089 - acc: 0.3746 - val_loss: 2.3993 - val_acc: 0.3949\n",
            "Epoch 6/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.3968 - acc: 0.3760 - val_loss: 2.3765 - val_acc: 0.4246\n",
            "Epoch 7/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.3809 - acc: 0.3902 - val_loss: 2.3673 - val_acc: 0.4187\n",
            "Epoch 8/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.3715 - acc: 0.3857 - val_loss: 2.3534 - val_acc: 0.4161\n",
            "Epoch 9/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.3638 - acc: 0.3933 - val_loss: 2.3418 - val_acc: 0.4182\n",
            "Epoch 10/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 2.3652 - acc: 0.3925 - val_loss: 2.3336 - val_acc: 0.4145\n",
            "Epoch 11/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.3577 - acc: 0.4081 - val_loss: 2.3622 - val_acc: 0.4187\n",
            "Epoch 12/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3461 - acc: 0.4062 - val_loss: 2.3456 - val_acc: 0.4277\n",
            "Epoch 13/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3459 - acc: 0.4117 - val_loss: 2.3215 - val_acc: 0.4320\n",
            "Epoch 14/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3253 - acc: 0.4238 - val_loss: 2.3048 - val_acc: 0.4209\n",
            "Epoch 15/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.3261 - acc: 0.4351 - val_loss: 2.3234 - val_acc: 0.3981\n",
            "Epoch 16/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3210 - acc: 0.4215 - val_loss: 2.3181 - val_acc: 0.4415\n",
            "Epoch 17/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3093 - acc: 0.4415 - val_loss: 2.2951 - val_acc: 0.4696\n",
            "Epoch 18/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.3055 - acc: 0.4486 - val_loss: 2.2761 - val_acc: 0.4839\n",
            "Epoch 19/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.2963 - acc: 0.4459 - val_loss: 2.2699 - val_acc: 0.4780\n",
            "Epoch 20/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.2884 - acc: 0.4494 - val_loss: 2.2856 - val_acc: 0.4701\n",
            "Epoch 21/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.2824 - acc: 0.4560 - val_loss: 2.2564 - val_acc: 0.4696\n",
            "Epoch 22/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.2715 - acc: 0.4659 - val_loss: 2.2330 - val_acc: 0.5087\n",
            "Epoch 23/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.2616 - acc: 0.4676 - val_loss: 2.2493 - val_acc: 0.4833\n",
            "Epoch 24/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.2457 - acc: 0.4694 - val_loss: 2.2379 - val_acc: 0.4966\n",
            "Epoch 25/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.2424 - acc: 0.4861 - val_loss: 2.2023 - val_acc: 0.5236\n",
            "Epoch 26/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.2277 - acc: 0.4884 - val_loss: 2.3277 - val_acc: 0.4426\n",
            "Epoch 27/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.2202 - acc: 0.4920 - val_loss: 2.2104 - val_acc: 0.4569\n",
            "Epoch 28/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1966 - acc: 0.5064 - val_loss: 2.1652 - val_acc: 0.5278\n",
            "Epoch 29/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.1826 - acc: 0.5155 - val_loss: 3.3061 - val_acc: 0.2965\n",
            "Epoch 30/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1818 - acc: 0.5180 - val_loss: 2.1118 - val_acc: 0.5627\n",
            "Epoch 31/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1633 - acc: 0.5264 - val_loss: 2.2550 - val_acc: 0.4272\n",
            "Epoch 32/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1444 - acc: 0.5456 - val_loss: 2.1599 - val_acc: 0.5087\n",
            "Epoch 33/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1233 - acc: 0.5467 - val_loss: 2.0856 - val_acc: 0.5834\n",
            "Epoch 34/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1131 - acc: 0.5533 - val_loss: 2.0606 - val_acc: 0.5887\n",
            "Epoch 35/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.1004 - acc: 0.5567 - val_loss: 2.1040 - val_acc: 0.5416\n",
            "Epoch 36/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.0833 - acc: 0.5677 - val_loss: 2.0721 - val_acc: 0.5580\n",
            "Epoch 37/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 2.0775 - acc: 0.5743 - val_loss: 2.0674 - val_acc: 0.5749\n",
            "Epoch 38/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.0531 - acc: 0.5893 - val_loss: 2.2092 - val_acc: 0.4854\n",
            "Epoch 39/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.0584 - acc: 0.5767 - val_loss: 2.0103 - val_acc: 0.5940\n",
            "Epoch 40/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 2.0233 - acc: 0.5980 - val_loss: 2.2716 - val_acc: 0.5056\n",
            "Epoch 41/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 2.0318 - acc: 0.5914 - val_loss: 2.1167 - val_acc: 0.5220\n",
            "Epoch 42/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 2.0132 - acc: 0.5977 - val_loss: 2.1764 - val_acc: 0.5167\n",
            "Epoch 43/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 2.0088 - acc: 0.6017 - val_loss: 2.1951 - val_acc: 0.5019\n",
            "Epoch 44/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.9809 - acc: 0.6150 - val_loss: 1.9712 - val_acc: 0.6098\n",
            "Epoch 45/200\n",
            "119/119 [==============================] - 30s 249ms/step - loss: 1.9638 - acc: 0.6241 - val_loss: 1.9747 - val_acc: 0.6204\n",
            "Epoch 46/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.9664 - acc: 0.6184 - val_loss: 2.1468 - val_acc: 0.5405\n",
            "Epoch 47/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.9479 - acc: 0.6246 - val_loss: 2.0481 - val_acc: 0.5691\n",
            "Epoch 48/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.9374 - acc: 0.6205 - val_loss: 2.0035 - val_acc: 0.5934\n",
            "Epoch 49/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.9209 - acc: 0.6343 - val_loss: 2.0116 - val_acc: 0.6061\n",
            "Epoch 50/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.9181 - acc: 0.6324 - val_loss: 2.1150 - val_acc: 0.5416\n",
            "Epoch 51/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.9180 - acc: 0.6202 - val_loss: 1.9121 - val_acc: 0.6390\n",
            "Epoch 52/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.9052 - acc: 0.6372 - val_loss: 1.9269 - val_acc: 0.6379\n",
            "Epoch 53/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8974 - acc: 0.6373 - val_loss: 1.8945 - val_acc: 0.6601\n",
            "Epoch 54/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.8970 - acc: 0.6469 - val_loss: 2.0901 - val_acc: 0.5331\n",
            "Epoch 55/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8543 - acc: 0.6603 - val_loss: 2.7980 - val_acc: 0.4457\n",
            "Epoch 56/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.8558 - acc: 0.6584 - val_loss: 2.0558 - val_acc: 0.5786\n",
            "Epoch 57/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8568 - acc: 0.6548 - val_loss: 1.9146 - val_acc: 0.6231\n",
            "Epoch 58/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8522 - acc: 0.6588 - val_loss: 1.9066 - val_acc: 0.6305\n",
            "Epoch 59/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8474 - acc: 0.6558 - val_loss: 1.8650 - val_acc: 0.6713\n",
            "Epoch 60/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8324 - acc: 0.6610 - val_loss: 2.6245 - val_acc: 0.4648\n",
            "Epoch 61/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8079 - acc: 0.6726 - val_loss: 1.8095 - val_acc: 0.6734\n",
            "Epoch 62/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8067 - acc: 0.6772 - val_loss: 1.7856 - val_acc: 0.6930\n",
            "Epoch 63/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8041 - acc: 0.6807 - val_loss: 1.8072 - val_acc: 0.6760\n",
            "Epoch 64/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7875 - acc: 0.6843 - val_loss: 1.9497 - val_acc: 0.6109\n",
            "Epoch 65/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.8087 - acc: 0.6740 - val_loss: 1.7642 - val_acc: 0.6951\n",
            "Epoch 66/200\n",
            "119/119 [==============================] - 30s 249ms/step - loss: 1.7557 - acc: 0.6993 - val_loss: 1.7362 - val_acc: 0.7221\n",
            "Epoch 67/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.7760 - acc: 0.6859 - val_loss: 1.8840 - val_acc: 0.6358\n",
            "Epoch 68/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7605 - acc: 0.6953 - val_loss: 1.8020 - val_acc: 0.6803\n",
            "Epoch 69/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7736 - acc: 0.6943 - val_loss: 1.7676 - val_acc: 0.6893\n",
            "Epoch 70/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7468 - acc: 0.6917 - val_loss: 1.9105 - val_acc: 0.6337\n",
            "Epoch 71/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7327 - acc: 0.7096 - val_loss: 1.7169 - val_acc: 0.7184\n",
            "Epoch 72/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7314 - acc: 0.7003 - val_loss: 2.0365 - val_acc: 0.5797\n",
            "Epoch 73/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7452 - acc: 0.6977 - val_loss: 1.7676 - val_acc: 0.6824\n",
            "Epoch 74/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7329 - acc: 0.6995 - val_loss: 1.7858 - val_acc: 0.6718\n",
            "Epoch 75/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.7292 - acc: 0.6893 - val_loss: 1.6852 - val_acc: 0.7194\n",
            "Epoch 76/200\n",
            "119/119 [==============================] - 30s 249ms/step - loss: 1.7180 - acc: 0.7016 - val_loss: 1.8839 - val_acc: 0.6453\n",
            "Epoch 77/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.7096 - acc: 0.7098 - val_loss: 1.7574 - val_acc: 0.6977\n",
            "Epoch 78/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6853 - acc: 0.7288 - val_loss: 1.7960 - val_acc: 0.6871\n",
            "Epoch 79/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6903 - acc: 0.7096 - val_loss: 1.7744 - val_acc: 0.6908\n",
            "Epoch 80/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6898 - acc: 0.7022 - val_loss: 1.7984 - val_acc: 0.6702\n",
            "Epoch 81/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6726 - acc: 0.7225 - val_loss: 1.9748 - val_acc: 0.6030\n",
            "Epoch 82/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6721 - acc: 0.7256 - val_loss: 1.9677 - val_acc: 0.6464\n",
            "Epoch 83/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6617 - acc: 0.7180 - val_loss: 1.6534 - val_acc: 0.7163\n",
            "Epoch 84/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6376 - acc: 0.7340 - val_loss: 1.7058 - val_acc: 0.7115\n",
            "Epoch 85/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6672 - acc: 0.7198 - val_loss: 1.9736 - val_acc: 0.6136\n",
            "Epoch 86/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6359 - acc: 0.7306 - val_loss: 1.8149 - val_acc: 0.6554\n",
            "Epoch 87/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6386 - acc: 0.7264 - val_loss: 1.7024 - val_acc: 0.7020\n",
            "Epoch 88/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 1.6459 - acc: 0.7261 - val_loss: 1.9614 - val_acc: 0.6400\n",
            "Epoch 89/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6450 - acc: 0.7217 - val_loss: 1.8616 - val_acc: 0.6368\n",
            "Epoch 90/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6131 - acc: 0.7369 - val_loss: 1.7006 - val_acc: 0.7099\n",
            "Epoch 91/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6384 - acc: 0.7243 - val_loss: 1.7767 - val_acc: 0.6596\n",
            "Epoch 92/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6238 - acc: 0.7293 - val_loss: 1.7289 - val_acc: 0.6903\n",
            "Epoch 93/200\n",
            "119/119 [==============================] - 30s 249ms/step - loss: 1.6088 - acc: 0.7364 - val_loss: 1.6191 - val_acc: 0.7321\n",
            "Epoch 94/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.5963 - acc: 0.7434 - val_loss: 1.6327 - val_acc: 0.7152\n",
            "Epoch 95/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.6076 - acc: 0.7409 - val_loss: 1.9124 - val_acc: 0.6019\n",
            "Epoch 96/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.6062 - acc: 0.7374 - val_loss: 1.6698 - val_acc: 0.7178\n",
            "Epoch 97/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.5972 - acc: 0.7411 - val_loss: 1.7330 - val_acc: 0.6914\n",
            "Epoch 98/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.5838 - acc: 0.7478 - val_loss: 1.7685 - val_acc: 0.6893\n",
            "Epoch 99/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5875 - acc: 0.7471 - val_loss: 1.6405 - val_acc: 0.7141\n",
            "Epoch 100/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5673 - acc: 0.7509 - val_loss: 1.5755 - val_acc: 0.7565\n",
            "Epoch 101/200\n",
            "119/119 [==============================] - 30s 249ms/step - loss: 1.5902 - acc: 0.7422 - val_loss: 1.6431 - val_acc: 0.7406\n",
            "Epoch 102/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5549 - acc: 0.7482 - val_loss: 1.6092 - val_acc: 0.7327\n",
            "Epoch 103/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.5758 - acc: 0.7406 - val_loss: 1.5770 - val_acc: 0.7364\n",
            "Epoch 104/200\n",
            "119/119 [==============================] - 30s 255ms/step - loss: 1.5447 - acc: 0.7538 - val_loss: 1.8867 - val_acc: 0.6337\n",
            "Epoch 105/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5445 - acc: 0.7545 - val_loss: 1.7079 - val_acc: 0.7125\n",
            "Epoch 106/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5630 - acc: 0.7498 - val_loss: 1.7912 - val_acc: 0.6675\n",
            "Epoch 107/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5479 - acc: 0.7482 - val_loss: 1.7328 - val_acc: 0.6649\n",
            "Epoch 108/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 1.5487 - acc: 0.7490 - val_loss: 1.6094 - val_acc: 0.7274\n",
            "Epoch 109/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 1.5288 - acc: 0.7583 - val_loss: 1.6767 - val_acc: 0.6956\n",
            "Epoch 110/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5290 - acc: 0.7597 - val_loss: 1.6279 - val_acc: 0.7237\n",
            "Epoch 111/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5237 - acc: 0.7603 - val_loss: 1.5478 - val_acc: 0.7475\n",
            "Epoch 112/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5270 - acc: 0.7511 - val_loss: 1.5605 - val_acc: 0.7459\n",
            "Epoch 113/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5285 - acc: 0.7522 - val_loss: 1.6070 - val_acc: 0.7279\n",
            "Epoch 114/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.5065 - acc: 0.7587 - val_loss: 1.5490 - val_acc: 0.7528\n",
            "Epoch 115/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4924 - acc: 0.7685 - val_loss: 1.5656 - val_acc: 0.7517\n",
            "Epoch 116/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.5072 - acc: 0.7636 - val_loss: 1.7469 - val_acc: 0.6585\n",
            "Epoch 117/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4860 - acc: 0.7707 - val_loss: 1.5966 - val_acc: 0.7263\n",
            "Epoch 118/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4913 - acc: 0.7669 - val_loss: 1.6546 - val_acc: 0.7136\n",
            "Epoch 119/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4802 - acc: 0.7695 - val_loss: 1.7313 - val_acc: 0.6686\n",
            "Epoch 120/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4920 - acc: 0.7637 - val_loss: 2.0311 - val_acc: 0.6008\n",
            "Epoch 121/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4814 - acc: 0.7651 - val_loss: 1.6084 - val_acc: 0.7152\n",
            "Epoch 122/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4921 - acc: 0.7672 - val_loss: 1.5118 - val_acc: 0.7533\n",
            "Epoch 123/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4758 - acc: 0.7732 - val_loss: 1.5250 - val_acc: 0.7544\n",
            "Epoch 124/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4777 - acc: 0.7716 - val_loss: 1.5591 - val_acc: 0.7433\n",
            "Epoch 125/200\n",
            "119/119 [==============================] - 30s 253ms/step - loss: 1.4616 - acc: 0.7687 - val_loss: 1.5495 - val_acc: 0.7374\n",
            "Epoch 126/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4571 - acc: 0.7727 - val_loss: 1.5623 - val_acc: 0.7295\n",
            "Epoch 127/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4515 - acc: 0.7735 - val_loss: 1.5866 - val_acc: 0.7327\n",
            "Epoch 128/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4454 - acc: 0.7770 - val_loss: 1.5144 - val_acc: 0.7491\n",
            "Epoch 129/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4459 - acc: 0.7754 - val_loss: 1.6556 - val_acc: 0.7099\n",
            "Epoch 130/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4414 - acc: 0.7719 - val_loss: 1.5239 - val_acc: 0.7501\n",
            "Epoch 131/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4536 - acc: 0.7693 - val_loss: 1.5086 - val_acc: 0.7443\n",
            "Epoch 132/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4468 - acc: 0.7708 - val_loss: 1.5001 - val_acc: 0.7602\n",
            "Epoch 133/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4110 - acc: 0.7962 - val_loss: 1.4770 - val_acc: 0.7570\n",
            "Epoch 134/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4446 - acc: 0.7704 - val_loss: 1.5671 - val_acc: 0.7274\n",
            "Epoch 135/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4263 - acc: 0.7853 - val_loss: 1.4953 - val_acc: 0.7454\n",
            "Epoch 136/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 1.4108 - acc: 0.7784 - val_loss: 1.5649 - val_acc: 0.7221\n",
            "Epoch 137/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4115 - acc: 0.7818 - val_loss: 1.5012 - val_acc: 0.7454\n",
            "Epoch 138/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4019 - acc: 0.7887 - val_loss: 1.5863 - val_acc: 0.7046\n",
            "Epoch 139/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4014 - acc: 0.7843 - val_loss: 1.5626 - val_acc: 0.7274\n",
            "Epoch 140/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3985 - acc: 0.7874 - val_loss: 1.6469 - val_acc: 0.6961\n",
            "Epoch 141/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.4404 - acc: 0.7756 - val_loss: 1.5462 - val_acc: 0.7343\n",
            "Epoch 142/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3655 - acc: 0.8058 - val_loss: 1.4737 - val_acc: 0.7528\n",
            "Epoch 143/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4003 - acc: 0.7856 - val_loss: 1.5811 - val_acc: 0.7380\n",
            "Epoch 144/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3967 - acc: 0.7836 - val_loss: 1.5821 - val_acc: 0.7231\n",
            "Epoch 145/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3840 - acc: 0.7920 - val_loss: 1.5760 - val_acc: 0.7353\n",
            "Epoch 146/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3860 - acc: 0.7840 - val_loss: 1.5313 - val_acc: 0.7369\n",
            "Epoch 147/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.4035 - acc: 0.7816 - val_loss: 1.5153 - val_acc: 0.7205\n",
            "Epoch 148/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3539 - acc: 0.7956 - val_loss: 1.4128 - val_acc: 0.7745\n",
            "Epoch 149/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3695 - acc: 0.7919 - val_loss: 1.4556 - val_acc: 0.7687\n",
            "Epoch 150/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3637 - acc: 0.7974 - val_loss: 1.7323 - val_acc: 0.6760\n",
            "Epoch 151/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3700 - acc: 0.7911 - val_loss: 1.4694 - val_acc: 0.7485\n",
            "Epoch 152/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3688 - acc: 0.7906 - val_loss: 1.4606 - val_acc: 0.7623\n",
            "Epoch 153/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3605 - acc: 0.7985 - val_loss: 1.4996 - val_acc: 0.7401\n",
            "Epoch 154/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3641 - acc: 0.7927 - val_loss: 1.5296 - val_acc: 0.7374\n",
            "Epoch 155/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3469 - acc: 0.7993 - val_loss: 1.4617 - val_acc: 0.7470\n",
            "Epoch 156/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3460 - acc: 0.7940 - val_loss: 1.4971 - val_acc: 0.7374\n",
            "Epoch 157/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3396 - acc: 0.7998 - val_loss: 1.6351 - val_acc: 0.7004\n",
            "Epoch 158/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3404 - acc: 0.7953 - val_loss: 1.5117 - val_acc: 0.7507\n",
            "Epoch 159/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3205 - acc: 0.8032 - val_loss: 1.4143 - val_acc: 0.7745\n",
            "Epoch 160/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3122 - acc: 0.8128 - val_loss: 1.5535 - val_acc: 0.7210\n",
            "Epoch 161/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3427 - acc: 0.7926 - val_loss: 1.4214 - val_acc: 0.7623\n",
            "Epoch 162/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3233 - acc: 0.7974 - val_loss: 1.4997 - val_acc: 0.7507\n",
            "Epoch 163/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3276 - acc: 0.8059 - val_loss: 1.6875 - val_acc: 0.7009\n",
            "Epoch 164/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3211 - acc: 0.7999 - val_loss: 1.4651 - val_acc: 0.7485\n",
            "Epoch 165/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3090 - acc: 0.8048 - val_loss: 1.5184 - val_acc: 0.7433\n",
            "Epoch 166/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3089 - acc: 0.8079 - val_loss: 1.4638 - val_acc: 0.7565\n",
            "Epoch 167/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3132 - acc: 0.8017 - val_loss: 1.4749 - val_acc: 0.7491\n",
            "Epoch 168/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2978 - acc: 0.8116 - val_loss: 1.5141 - val_acc: 0.7205\n",
            "Epoch 169/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.3234 - acc: 0.8037 - val_loss: 1.5533 - val_acc: 0.7321\n",
            "Epoch 170/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2711 - acc: 0.8185 - val_loss: 1.5102 - val_acc: 0.7512\n",
            "Epoch 171/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2876 - acc: 0.8065 - val_loss: 1.4242 - val_acc: 0.7554\n",
            "Epoch 172/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2914 - acc: 0.8143 - val_loss: 1.5222 - val_acc: 0.7470\n",
            "Epoch 173/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2799 - acc: 0.8090 - val_loss: 1.6529 - val_acc: 0.6935\n",
            "Epoch 174/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.3030 - acc: 0.8058 - val_loss: 1.5432 - val_acc: 0.7279\n",
            "Epoch 175/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2866 - acc: 0.8106 - val_loss: 1.3958 - val_acc: 0.7687\n",
            "Epoch 176/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2525 - acc: 0.8206 - val_loss: 1.5171 - val_acc: 0.7274\n",
            "Epoch 177/200\n",
            "119/119 [==============================] - 30s 252ms/step - loss: 1.2820 - acc: 0.8044 - val_loss: 1.4023 - val_acc: 0.7644\n",
            "Epoch 178/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2549 - acc: 0.8193 - val_loss: 1.4072 - val_acc: 0.7692\n",
            "Epoch 179/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2704 - acc: 0.8094 - val_loss: 1.3978 - val_acc: 0.7676\n",
            "Epoch 180/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2491 - acc: 0.8199 - val_loss: 1.4051 - val_acc: 0.7570\n",
            "Epoch 181/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2425 - acc: 0.8205 - val_loss: 1.5358 - val_acc: 0.7120\n",
            "Epoch 182/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2697 - acc: 0.8056 - val_loss: 1.4682 - val_acc: 0.7485\n",
            "Epoch 183/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2523 - acc: 0.8162 - val_loss: 1.6142 - val_acc: 0.7041\n",
            "Epoch 184/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2528 - acc: 0.8152 - val_loss: 1.5016 - val_acc: 0.7332\n",
            "Epoch 185/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2534 - acc: 0.8074 - val_loss: 1.5371 - val_acc: 0.7194\n",
            "Epoch 186/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2460 - acc: 0.8222 - val_loss: 1.3922 - val_acc: 0.7697\n",
            "Epoch 187/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2391 - acc: 0.8197 - val_loss: 1.3738 - val_acc: 0.7713\n",
            "Epoch 188/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2195 - acc: 0.8258 - val_loss: 1.3983 - val_acc: 0.7697\n",
            "Epoch 189/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2444 - acc: 0.8150 - val_loss: 1.4545 - val_acc: 0.7575\n",
            "Epoch 190/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2174 - acc: 0.8300 - val_loss: 1.4537 - val_acc: 0.7565\n",
            "Epoch 191/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2348 - acc: 0.8196 - val_loss: 1.4971 - val_acc: 0.7263\n",
            "Epoch 192/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2348 - acc: 0.8224 - val_loss: 1.4273 - val_acc: 0.7607\n",
            "Epoch 193/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2149 - acc: 0.8237 - val_loss: 1.4884 - val_acc: 0.7390\n",
            "Epoch 194/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2047 - acc: 0.8297 - val_loss: 1.4706 - val_acc: 0.7231\n",
            "Epoch 195/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2318 - acc: 0.8177 - val_loss: 1.4618 - val_acc: 0.7358\n",
            "Epoch 196/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.2032 - acc: 0.8314 - val_loss: 1.3754 - val_acc: 0.7538\n",
            "Epoch 197/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.1962 - acc: 0.8324 - val_loss: 1.5602 - val_acc: 0.7300\n",
            "Epoch 198/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2023 - acc: 0.8250 - val_loss: 1.4273 - val_acc: 0.7427\n",
            "Epoch 199/200\n",
            "119/119 [==============================] - 30s 250ms/step - loss: 1.2091 - acc: 0.8195 - val_loss: 1.3047 - val_acc: 0.7941\n",
            "Epoch 200/200\n",
            "119/119 [==============================] - 30s 251ms/step - loss: 1.1902 - acc: 0.8335 - val_loss: 1.4878 - val_acc: 0.7417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJysdIN3lVKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "921d5cf6-62eb-4046-a278-8b62ef674464"
      },
      "source": [
        "import keras.utils.np_utils as kutils\n",
        "yPreds = wrn_28_10.predict(X_test)\n",
        "yPred = np.argmax(yPreds, axis=1)\n",
        "yPred[0]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usjrLCh6lVKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24d4580b-b5ed-4e7b-ace4-a2032dba93ad"
      },
      "source": [
        "yPred[2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4N7HDg0lVKP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e12f8db-0687-49f4-cc92-9ed1af595742"
      },
      "source": [
        "yTrue = y_test_df['New']\n",
        "yTrue[2]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gojN0iDIlVKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a127a91e-48f8-47bc-e723-6e9fae0c14ae"
      },
      "source": [
        "yPreds"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.7758693e-01, 9.4872536e-03, 8.8735390e-03, 4.0523075e-03],\n",
              "       [8.0385869e-03, 9.7960770e-01, 1.1277177e-02, 1.0765449e-03],\n",
              "       [6.4494306e-01, 3.1119347e-01, 3.4909833e-02, 8.9536263e-03],\n",
              "       ...,\n",
              "       [7.9813333e-05, 5.6717703e-03, 9.9110353e-01, 3.1449250e-03],\n",
              "       [2.9995468e-01, 6.7772800e-01, 1.5793923e-02, 6.5234290e-03],\n",
              "       [9.9878794e-01, 2.4846685e-04, 2.8403150e-04, 6.7955139e-04]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4m2vMP7lVKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b906e8c4-f905-4e3c-c26e-e4c5451a14b4"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "\n",
        "accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
        "error = 100 - accuracy\n",
        "print(\"Accuracy : \", accuracy)\n",
        "print(\"Error : \", error)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy :  74.16622551614611\n",
            "Error :  25.833774483853887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z8YVCPklVKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b66efe45-ef3f-48cf-93f4-44ac0ce80380"
      },
      "source": [
        "from matplotlib import  pyplot\n",
        "pyplot.plot(hist.history['acc'], label='train')\n",
        "pyplot.plot(hist.history['val_acc'], label='test')\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc3aae48cc0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKg2w43tlVKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpzTXjevlVKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tppO4rXhlVKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TnqtfIlVKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UcghrO8lVKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}