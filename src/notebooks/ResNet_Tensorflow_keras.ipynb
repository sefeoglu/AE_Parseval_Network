{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Tensorflow_keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/AE_Parseval_Network/blob/master/src/notebooks/ResNet_Tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cczYDRrfFlDx",
        "colab_type": "text"
      },
      "source": [
        "# Wide ResNet 16_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWvd9YADGtMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81b74e58-3c75-4353-bdeb-44bdb15a44ba"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
        "import tensorflow\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tensorflow Version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqbIFJTwXLH",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRdSMgRjG8ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2dfb5edf-2f73-41a6-ef1b-6842b6c28b8c"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0001\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "  \n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv2:channel:  {}\".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv3 channel_axis:{} \".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "init = (32, 32,1)\n",
        "wrn_16_2 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNo5x-Ft9Fe",
        "colab_type": "text"
      },
      "source": [
        "# Data Prepare and Processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqH742XcPQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNBI_SkvuzgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euxwMe2jIoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b057e2f4-79a3-4f53-d225-b8c5ce58b3b9"
      },
      "source": [
        "import cv2\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNBsNVDNu6Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ec77dc2-ba39-4fa9-b745-45c96c37c3e9"
      },
      "source": [
        "X = new_data_X.astype('float32')\n",
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQdrnTKuM8c",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqf-dZOrvC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X[0].shape\n",
        "\n",
        "# transform data set\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEHVf2Bu9xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "y_df = pd.DataFrame(Y_data, columns=['Label'])\n",
        "y_df['Encoded'] = labelencoder.fit_transform(y_df['Label'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdkpb2Jkqu6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_cat = to_categorical(y_df['Encoded'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kif3Li9NuSnV",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88yOqhbSwjPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_sch(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.1\n",
        "    elif epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_sch)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpiWMEgRpWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-W3MPorKESw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,X_train,y_train):\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)\n",
        "  hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVs_QNHoEKji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def train_KFold(pretrained_model, X_train, Y_train, X_test, y_test, EPOCHS, BS, sgd, epsilon_list):\n",
        "  init = (32, 32,1)\n",
        "  \n",
        "  res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "  kf = KFold(n_splits=3, random_state=42, shuffle=False)\n",
        "  # enumerate splits\n",
        "  for j, (train, test) in enumerate(kf.split(X_train)):\n",
        "    model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "    print(\"Finished compiling\")\n",
        "    x_train, y_train, x_val,y_val = X_train[train],Y_train[train], X_train[test],Y_train[test]\n",
        "    hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=EPOCHS,\n",
        "                    callbacks=[lr_scheduler],\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    validation_steps=x_val.shape[0] // BS,)\n",
        "    loss, acc = model.evaluate(X_test, y_test)\n",
        "    adv1 = get_adversarial_examples(pretrained_model, X_test, y_test, 0.003)\n",
        "    loss1, acc1 = model.evaluate(adv1,y_test)\n",
        "    adv2 = get_adversarial_examples(pretrained_model, X_test, y_test, 0.005)\n",
        "    loss2, acc2 = model.evaluate(adv2, y_test)\n",
        "    adv3 = get_adversarial_examples(pretrained_model, X_test, y_test, 0.01)\n",
        "    loss3, acc3 = model.evaluate(adv3,y_test)\n",
        "    adv4 = get_adversarial_examples(pretrained_model, X_test, y_test, 0.02)\n",
        "    loss4, acc4 = model.evaluate(adv4,y_test)\n",
        "    \n",
        "    row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "           'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "    res_df = res_df.append(row , ignore_index=True)\n",
        "\n",
        "  return res_df\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFgKVWiHKYsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "d3455b04-5645-4088-af65-9c4659c02641"
      },
      "source": [
        "\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)\n",
        "print(\"Cleverhans Version: \" + cleverhans.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cleverhans\n",
            "  Cloning https://github.com/tensorflow/cleverhans.git to /tmp/pip-install-815fskzm/cleverhans\n",
            "  Running command git clone -q https://github.com/tensorflow/cleverhans.git /tmp/pip-install-815fskzm/cleverhans\n",
            "Collecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 10.4MB/s \n",
            "\u001b[?25hCollecting pycodestyle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/5b/88879fb861ab79aef45c7e199cae3ef7af487b5603dcb363517a50602dd7/pycodestyle-2.6.0-py2.py3-none-any.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (3.2.1)\n",
            "Collecting mnist~=0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from cleverhans) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from cleverhans) (0.15.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->cleverhans) (1.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (0.3.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability->cleverhans) (1.12.0)\n",
            "Building wheels for collected packages: cleverhans\n",
            "  Building wheel for cleverhans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cleverhans: filename=cleverhans-3.0.1-cp36-none-any.whl size=262572 sha256=32176e95a06313ae11dfd12cae008f384f241d87a1e63366e0e86772511dcdff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oowcgfrn/wheels/6e/59/ec/723a6f654aaf62c8c40f0f0850fdf71a4948598697f56c3bfa\n",
            "Successfully built cleverhans\n",
            "Installing collected packages: nose, pycodestyle, mnist, cleverhans\n",
            "Successfully installed cleverhans-3.0.1 mnist-0.2.2 nose-1.3.7 pycodestyle-2.6.0\n",
            "\n",
            "Tensorflow Version: 2.2.0\n",
            "Cleverhans Version: 3.0.1-fc7b7c7ec903258e0e3fb88503fa629f\n",
            "WARNING:tensorflow:From <ipython-input-15-21b906498791>:8: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU Available:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6HjbpvKslU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "def get_adversarial_examples(pretrained_model, X_true, y_true, epsilon):\n",
        "  #The attack requires the model to ouput the logits\n",
        "   \n",
        "  logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
        "  X_adv = []\n",
        "  for i in range(len(X_true)):\n",
        "    random_index = i\n",
        "    original_image = X_true[random_index]\n",
        "    original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "    original_label = y_true[random_index]\n",
        "    original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "    adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "    X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "  X_adv = np.array(X_adv)\n",
        "  return X_adv\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_quzDGwKGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_graph(hist):\n",
        "  history = hist\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"wrn_tensor.png\")\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"deneme.png\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbbLi83NyVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_test(model,X_adv, X_test, y_test, epsilon):\n",
        "  loss, acc = model.evaluate(X_adv,y_test)\n",
        "  print(\"epsilon: {} and test evaluation : {}, {}\".format(epsilon,loss, acc))\n",
        "  SNR = 20*np.log10(np.linalg.norm(X_test)/np.linalg.norm(X_test-X_adv))\n",
        "  print(\"SNR: {}\".format(SNR))\n",
        "  return loss, acc"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxNDNAa6MWu7",
        "colab_type": "text"
      },
      "source": [
        "**Train a Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghSgp3NvhhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "BS = 128\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBnqXaiNwHGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "220dab80-ea29-4305-d03b-6e6dd0fb5a59"
      },
      "source": [
        "#wrn_16_2.summary()\n",
        "wrn_16_2.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size = 0.1)\n",
        "train(wrn_16_2, X_train,y_train)\n",
        "wrn_16_2.save(\"wrn_model.h5\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 3s 72ms/step - loss: 1.5967 - acc: 0.3111 - val_loss: 1.5236 - val_acc: 0.3709 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5124 - acc: 0.3668 - val_loss: 1.5397 - val_acc: 0.3340 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4861 - acc: 0.3706 - val_loss: 1.5253 - val_acc: 0.3650 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4765 - acc: 0.3810 - val_loss: 1.4519 - val_acc: 0.3767 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4676 - acc: 0.3799 - val_loss: 1.4501 - val_acc: 0.4155 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4550 - acc: 0.4024 - val_loss: 1.4570 - val_acc: 0.4136 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4392 - acc: 0.4123 - val_loss: 1.4437 - val_acc: 0.3883 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4437 - acc: 0.4210 - val_loss: 1.4404 - val_acc: 0.4078 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4176 - acc: 0.4521 - val_loss: 1.6832 - val_acc: 0.3029 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3891 - acc: 0.4842 - val_loss: 1.3892 - val_acc: 0.4990 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3647 - acc: 0.4882 - val_loss: 1.3693 - val_acc: 0.4951 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3325 - acc: 0.5249 - val_loss: 1.3454 - val_acc: 0.5146 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3032 - acc: 0.5444 - val_loss: 1.7127 - val_acc: 0.4136 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2814 - acc: 0.5473 - val_loss: 1.5092 - val_acc: 0.4621 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3004 - acc: 0.5382 - val_loss: 1.3092 - val_acc: 0.5437 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2757 - acc: 0.5497 - val_loss: 1.2897 - val_acc: 0.5515 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2252 - acc: 0.5908 - val_loss: 1.1845 - val_acc: 0.6058 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1827 - acc: 0.5928 - val_loss: 1.1859 - val_acc: 0.6311 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.1622 - acc: 0.6123 - val_loss: 1.3036 - val_acc: 0.5573 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1340 - acc: 0.6209 - val_loss: 1.0621 - val_acc: 0.6641 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1156 - acc: 0.6258 - val_loss: 1.0613 - val_acc: 0.6641 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1132 - acc: 0.6378 - val_loss: 1.2700 - val_acc: 0.5903 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0908 - acc: 0.6414 - val_loss: 1.0271 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0612 - acc: 0.6447 - val_loss: 1.0173 - val_acc: 0.6854 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0378 - acc: 0.6651 - val_loss: 1.0101 - val_acc: 0.7262 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0410 - acc: 0.6647 - val_loss: 0.9890 - val_acc: 0.7068 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0072 - acc: 0.6743 - val_loss: 1.0977 - val_acc: 0.6796 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.0176 - acc: 0.6698 - val_loss: 1.0034 - val_acc: 0.7107 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9997 - acc: 0.6813 - val_loss: 1.0568 - val_acc: 0.6893 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.9862 - acc: 0.6833 - val_loss: 0.9416 - val_acc: 0.7398 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9488 - acc: 0.7029 - val_loss: 0.9175 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9314 - acc: 0.7115 - val_loss: 0.9081 - val_acc: 0.7515 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.9140 - acc: 0.7244 - val_loss: 0.9154 - val_acc: 0.7398 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9162 - acc: 0.7250 - val_loss: 0.8837 - val_acc: 0.7534 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9066 - acc: 0.7230 - val_loss: 0.8894 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9101 - acc: 0.7239 - val_loss: 0.9002 - val_acc: 0.7456 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9061 - acc: 0.7253 - val_loss: 0.9053 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9021 - acc: 0.7268 - val_loss: 0.8710 - val_acc: 0.7534 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9057 - acc: 0.7339 - val_loss: 0.8811 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8977 - acc: 0.7315 - val_loss: 0.8720 - val_acc: 0.7709 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8989 - acc: 0.7348 - val_loss: 0.9108 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8935 - acc: 0.7315 - val_loss: 0.9051 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8980 - acc: 0.7315 - val_loss: 0.9095 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8923 - acc: 0.7277 - val_loss: 0.8708 - val_acc: 0.7612 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8835 - acc: 0.7428 - val_loss: 0.8803 - val_acc: 0.7437 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8933 - acc: 0.7328 - val_loss: 0.8873 - val_acc: 0.7553 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8865 - acc: 0.7319 - val_loss: 0.9317 - val_acc: 0.7359 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8950 - acc: 0.7324 - val_loss: 0.8944 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8940 - acc: 0.7332 - val_loss: 0.8761 - val_acc: 0.7398 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8772 - acc: 0.7421 - val_loss: 0.8898 - val_acc: 0.7437 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8900 - acc: 0.7341 - val_loss: 0.8705 - val_acc: 0.7456 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8890 - acc: 0.7381 - val_loss: 0.8839 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8889 - acc: 0.7379 - val_loss: 0.8966 - val_acc: 0.7398 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8996 - acc: 0.7337 - val_loss: 0.9012 - val_acc: 0.7573 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8774 - acc: 0.7421 - val_loss: 0.9171 - val_acc: 0.7398 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8824 - acc: 0.7292 - val_loss: 0.8831 - val_acc: 0.7456 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8866 - acc: 0.7355 - val_loss: 0.8776 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8885 - acc: 0.7386 - val_loss: 0.8946 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8728 - acc: 0.7446 - val_loss: 0.8849 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8830 - acc: 0.7337 - val_loss: 0.8762 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8865 - acc: 0.7386 - val_loss: 0.8709 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8812 - acc: 0.7339 - val_loss: 1.0037 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8745 - acc: 0.7332 - val_loss: 0.9153 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7450 - val_loss: 0.9051 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8866 - acc: 0.7292 - val_loss: 0.8650 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8880 - acc: 0.7315 - val_loss: 0.9407 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8824 - acc: 0.7324 - val_loss: 0.8908 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8901 - acc: 0.7370 - val_loss: 0.8890 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8758 - acc: 0.7397 - val_loss: 0.9021 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8785 - acc: 0.7406 - val_loss: 0.9096 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8765 - acc: 0.7370 - val_loss: 0.8713 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8872 - acc: 0.7299 - val_loss: 0.8722 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8688 - acc: 0.7388 - val_loss: 0.8958 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8883 - acc: 0.7346 - val_loss: 0.8946 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8902 - acc: 0.7376 - val_loss: 0.8731 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8813 - acc: 0.7344 - val_loss: 0.8973 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8804 - acc: 0.7324 - val_loss: 0.8913 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8792 - acc: 0.7346 - val_loss: 0.8895 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8798 - acc: 0.7379 - val_loss: 0.8800 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8767 - acc: 0.7408 - val_loss: 0.9082 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8708 - acc: 0.7437 - val_loss: 0.8783 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8929 - acc: 0.7310 - val_loss: 0.9112 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8727 - acc: 0.7455 - val_loss: 0.8962 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8784 - acc: 0.7400 - val_loss: 0.8816 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8874 - acc: 0.7361 - val_loss: 0.8784 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8820 - acc: 0.7330 - val_loss: 0.8727 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8754 - acc: 0.7392 - val_loss: 0.8894 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8756 - acc: 0.7397 - val_loss: 0.8977 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8738 - acc: 0.7419 - val_loss: 0.8741 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8812 - acc: 0.7359 - val_loss: 0.8839 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8890 - acc: 0.7321 - val_loss: 0.8935 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8809 - acc: 0.7410 - val_loss: 0.8874 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8883 - acc: 0.7337 - val_loss: 0.9185 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8758 - acc: 0.7375 - val_loss: 0.8700 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8773 - acc: 0.7352 - val_loss: 0.9082 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8900 - acc: 0.7395 - val_loss: 0.8752 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8801 - acc: 0.7352 - val_loss: 0.9000 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8854 - acc: 0.7368 - val_loss: 0.8659 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8744 - acc: 0.7441 - val_loss: 0.8884 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8742 - acc: 0.7406 - val_loss: 0.8773 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8802 - acc: 0.7348 - val_loss: 0.8911 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8775 - acc: 0.7359 - val_loss: 0.8841 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8901 - acc: 0.7270 - val_loss: 0.8749 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8690 - acc: 0.7381 - val_loss: 0.8932 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8773 - acc: 0.7417 - val_loss: 0.9705 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8826 - acc: 0.7355 - val_loss: 0.8735 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8813 - acc: 0.7344 - val_loss: 0.8662 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8731 - acc: 0.7337 - val_loss: 0.8980 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8774 - acc: 0.7412 - val_loss: 0.8743 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8795 - acc: 0.7386 - val_loss: 0.8661 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8763 - acc: 0.7355 - val_loss: 0.8802 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8816 - acc: 0.7379 - val_loss: 0.8913 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8780 - acc: 0.7406 - val_loss: 0.8751 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8863 - acc: 0.7332 - val_loss: 0.8997 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8764 - acc: 0.7372 - val_loss: 0.8769 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8821 - acc: 0.7399 - val_loss: 0.8814 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8766 - acc: 0.7377 - val_loss: 0.9018 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8739 - acc: 0.7406 - val_loss: 0.8841 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8817 - acc: 0.7352 - val_loss: 0.8611 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8767 - acc: 0.7368 - val_loss: 0.8940 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8899 - acc: 0.7381 - val_loss: 0.9049 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8809 - acc: 0.7308 - val_loss: 0.8723 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8804 - acc: 0.7368 - val_loss: 0.8820 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8850 - acc: 0.7359 - val_loss: 0.9103 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8721 - acc: 0.7366 - val_loss: 0.9178 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8758 - acc: 0.7439 - val_loss: 0.8914 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7368 - val_loss: 0.8873 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8836 - acc: 0.7403 - val_loss: 0.9510 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8715 - acc: 0.7430 - val_loss: 0.8849 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8919 - acc: 0.7295 - val_loss: 0.9105 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8658 - acc: 0.7450 - val_loss: 0.8786 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8779 - acc: 0.7403 - val_loss: 0.8697 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8786 - acc: 0.7399 - val_loss: 0.8809 - val_acc: 0.7592 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8869 - acc: 0.7344 - val_loss: 0.9178 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8813 - acc: 0.7370 - val_loss: 0.8647 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8850 - acc: 0.7350 - val_loss: 0.8949 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8756 - acc: 0.7372 - val_loss: 0.9145 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8793 - acc: 0.7419 - val_loss: 0.8966 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8850 - acc: 0.7348 - val_loss: 0.8745 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8862 - acc: 0.7392 - val_loss: 0.8891 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8828 - acc: 0.7310 - val_loss: 0.8939 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8742 - acc: 0.7368 - val_loss: 0.8723 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8747 - acc: 0.7459 - val_loss: 0.8659 - val_acc: 0.7553 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8836 - acc: 0.7403 - val_loss: 0.8940 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8700 - acc: 0.7397 - val_loss: 0.9189 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8900 - acc: 0.7364 - val_loss: 0.8935 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8814 - acc: 0.7328 - val_loss: 0.8792 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8802 - acc: 0.7344 - val_loss: 0.9472 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8775 - acc: 0.7415 - val_loss: 0.8810 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8697 - acc: 0.7428 - val_loss: 0.8814 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8804 - acc: 0.7428 - val_loss: 0.9021 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8802 - acc: 0.7337 - val_loss: 0.8801 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8771 - acc: 0.7330 - val_loss: 0.9073 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7428 - val_loss: 0.8775 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7375 - val_loss: 0.8865 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8826 - acc: 0.7432 - val_loss: 0.8878 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8840 - acc: 0.7346 - val_loss: 0.9177 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8735 - acc: 0.7372 - val_loss: 0.8786 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8771 - acc: 0.7441 - val_loss: 0.8770 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8849 - acc: 0.7284 - val_loss: 0.8891 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8819 - acc: 0.7401 - val_loss: 0.8883 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8765 - acc: 0.7452 - val_loss: 0.8689 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8823 - acc: 0.7297 - val_loss: 0.8713 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8856 - acc: 0.7366 - val_loss: 0.8970 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8850 - acc: 0.7364 - val_loss: 0.8820 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8801 - acc: 0.7330 - val_loss: 0.8829 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8786 - acc: 0.7339 - val_loss: 0.9034 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8764 - acc: 0.7430 - val_loss: 0.8864 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8748 - acc: 0.7392 - val_loss: 0.8711 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8767 - acc: 0.7426 - val_loss: 0.8646 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8806 - acc: 0.7381 - val_loss: 0.8717 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8941 - acc: 0.7259 - val_loss: 0.8765 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8766 - acc: 0.7355 - val_loss: 0.8890 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8745 - acc: 0.7470 - val_loss: 0.8987 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8843 - acc: 0.7350 - val_loss: 0.8931 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8821 - acc: 0.7417 - val_loss: 0.8741 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8777 - acc: 0.7443 - val_loss: 0.8971 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8728 - acc: 0.7399 - val_loss: 0.8692 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8818 - acc: 0.7377 - val_loss: 0.8629 - val_acc: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8805 - acc: 0.7395 - val_loss: 0.8750 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8843 - acc: 0.7335 - val_loss: 0.8918 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8748 - acc: 0.7472 - val_loss: 0.9424 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8756 - acc: 0.7341 - val_loss: 0.8883 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8782 - acc: 0.7366 - val_loss: 0.9019 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8726 - acc: 0.7435 - val_loss: 0.8885 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8689 - acc: 0.7392 - val_loss: 0.8985 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8825 - acc: 0.7330 - val_loss: 0.8860 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8817 - acc: 0.7326 - val_loss: 0.8983 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8821 - acc: 0.7397 - val_loss: 0.8784 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8779 - acc: 0.7419 - val_loss: 0.8973 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8812 - acc: 0.7332 - val_loss: 0.9053 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8787 - acc: 0.7359 - val_loss: 0.8837 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8822 - acc: 0.7357 - val_loss: 0.8681 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8788 - acc: 0.7383 - val_loss: 0.8783 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8771 - acc: 0.7397 - val_loss: 0.8754 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8676 - acc: 0.7459 - val_loss: 0.8849 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8838 - acc: 0.7326 - val_loss: 0.9196 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8811 - acc: 0.7359 - val_loss: 0.8982 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8906 - acc: 0.7319 - val_loss: 0.8883 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8782 - acc: 0.7361 - val_loss: 0.8706 - val_acc: 0.7476 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCdsaWdoWLSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4c7f74c4-8768-49b4-8ca3-87c2ee611d7d"
      },
      "source": [
        "res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "loss, acc = wrn_16_2.evaluate(X_test, y_test)\n",
        "adv1 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.003)\n",
        "loss1, acc1 = wrn_16_2.evaluate(adv1,y_test)\n",
        "adv2 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.005)\n",
        "loss2, acc2 = wrn_16_2.evaluate(adv2, y_test)\n",
        "adv3 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.01)\n",
        "loss3, acc3 = wrn_16_2.evaluate(adv3,y_test)\n",
        "adv4 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.02)\n",
        "loss4, acc4 = wrn_16_2.evaluate(adv4,y_test)\n",
        "    \n",
        "row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "           'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "res_df = res_df.append(row , ignore_index=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8181 - acc: 0.7749\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9102 - acc: 0.7330\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9762 - acc: 0.7033\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.1539 - acc: 0.6283\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.5343 - acc: 0.4764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw8RqG_AWftH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "a9dd55d4-2497-4400-afaf-4ec7bb1fba35"
      },
      "source": [
        "res_df"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.818086</td>\n",
              "      <td>0.774869</td>\n",
              "      <td>0.91019</td>\n",
              "      <td>0.732984</td>\n",
              "      <td>0.976165</td>\n",
              "      <td>0.703316</td>\n",
              "      <td>1.153864</td>\n",
              "      <td>0.628272</td>\n",
              "      <td>1.534295</td>\n",
              "      <td>0.47644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean    loss1  ...      acc3     loss4     acc4\n",
              "0    0.818086   0.774869  0.91019  ...  0.628272  1.534295  0.47644\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDHJIheU8bm",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydHkZ8pauMe",
        "colab_type": "text"
      },
      "source": [
        "**Non_Adversarial Training Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkjbXCV6KFcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "result_df = train_KFold(wrn_16_2,X_train,y_train, X_test,y_test, 50,BS, sgd, epsilon_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3iEL9P9Tapn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "1c1f2798-10e2-4128-aedb-7c79cfe00c31"
      },
      "source": [
        "result_df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.058956</td>\n",
              "      <td>0.663176</td>\n",
              "      <td>1.073644</td>\n",
              "      <td>0.652705</td>\n",
              "      <td>1.083692</td>\n",
              "      <td>0.649215</td>\n",
              "      <td>1.109767</td>\n",
              "      <td>0.645724</td>\n",
              "      <td>1.164680</td>\n",
              "      <td>0.624782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.867004</td>\n",
              "      <td>0.720768</td>\n",
              "      <td>0.881926</td>\n",
              "      <td>0.712042</td>\n",
              "      <td>0.892072</td>\n",
              "      <td>0.710297</td>\n",
              "      <td>0.918150</td>\n",
              "      <td>0.699825</td>\n",
              "      <td>0.973887</td>\n",
              "      <td>0.671902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.933523</td>\n",
              "      <td>0.724258</td>\n",
              "      <td>0.945634</td>\n",
              "      <td>0.719023</td>\n",
              "      <td>0.953873</td>\n",
              "      <td>0.713787</td>\n",
              "      <td>0.975151</td>\n",
              "      <td>0.703316</td>\n",
              "      <td>1.020294</td>\n",
              "      <td>0.678883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean     loss1  ...      acc3     loss4      acc4\n",
              "0    1.058956   0.663176  1.073644  ...  0.645724  1.164680  0.624782\n",
              "1    0.867004   0.720768  0.881926  ...  0.699825  0.973887  0.671902\n",
              "2    0.933523   0.724258  0.945634  ...  0.703316  1.020294  0.678883\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkS-pL3EkeO",
        "colab_type": "text"
      },
      "source": [
        "# **Show Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT6i2DxWEjyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "result_df['clean_acc_mean'] = np.sum(result_df['acc_clean'])/3.0\n",
        "result_df['0.003_acc_mean'] = np.sum(result_df['acc1'])/3.0\n",
        "result_df['0.005_acc_mean'] = np.sum(result_df['acc2'])/3.0\n",
        "result_df['0.01_acc_mean'] = np.sum(result_df['acc3'])/3.0\n",
        "result_df['0.02_acc_mean'] = np.sum(result_df['acc4'])/3.0\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZQavZ-2Ighr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "45503bb9-c229-46b2-9011-21f5b306bf52"
      },
      "source": [
        "result_df.head(1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>clean_acc_mean</th>\n",
              "      <th>0.003_acc_mean</th>\n",
              "      <th>0.005_acc_mean</th>\n",
              "      <th>0.01_acc_mean</th>\n",
              "      <th>0.02_acc_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.058956</td>\n",
              "      <td>0.663176</td>\n",
              "      <td>1.073644</td>\n",
              "      <td>0.652705</td>\n",
              "      <td>1.083692</td>\n",
              "      <td>0.649215</td>\n",
              "      <td>1.109767</td>\n",
              "      <td>0.645724</td>\n",
              "      <td>1.16468</td>\n",
              "      <td>0.624782</td>\n",
              "      <td>0.702734</td>\n",
              "      <td>0.69459</td>\n",
              "      <td>0.691099</td>\n",
              "      <td>0.682955</td>\n",
              "      <td>0.658522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  0.01_acc_mean  0.02_acc_mean\n",
              "0    1.058956   0.663176  ...       0.682955       0.658522\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWpwrKuwmsc",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxVNgZvPRHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "class AdversarialTraining(object):\n",
        "    \"\"\"Adversarial Training  \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def train(self, pretrained_model, X_train, Y_train, X_test, Y_test, epochs, batch_size, epsilon_list):\n",
        "        init = (32, 32,1)\n",
        "        res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)\n",
        "        x_train, y_train = self.data_augmentation(X_train, y_train, BS, pretrained_model, epsilon_list)\n",
        "        x_val, y_val = self.data_augmentation(X_val, y_val, BS, pretrained_model, epsilon_list)\n",
        "        model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "        model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "        hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                         callbacks = [lr_scheduler],\n",
        "                         validation_data=(x_val, y_val),\n",
        "                         validation_steps=x_val.shape[0] // BS,)\n",
        "        loss, acc = model.evaluate(X_test, y_test)\n",
        "        loss1, acc1 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "        loss2, acc2 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "        loss3, acc3 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "        loss4, acc4 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "        row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "        res_df = res_df.append(row , ignore_index=True)\n",
        "          \n",
        "        return res_df\n",
        "    def mini_batch_train(self, model, X_train,y_train, x_val, y_val, BS, pretrained_model, epsilon):\n",
        "\n",
        "\n",
        "        hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=1,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   validation_steps=x_val.shape[0] // BS, shuffle = True)\n",
        "        \n",
        "        ### TODO ###\n",
        "        ## Save hist on file.###\n",
        "\n",
        "\n",
        "    def data_augmentation(self, X_train, Y_train, batch_size, pretrained_model, epsilon_list):\n",
        "      ### divide data 16,16,16,16 for 4 different epsilons and 64 is true image. ### \n",
        "        #start_index = self.data_iteration(X_train, batch_size)\n",
        "        first_half_end = int(len(X_train)/2)\n",
        "        second_half_end = int(len(X_train))\n",
        "        x_clean = X_train[0:first_half_end,:,:,:]\n",
        "        x_adv = self.get_adversarial(X_train[first_half_end:second_half_end,:,:,:], Y_train[first_half_end:second_half_end], epsilon_list)\n",
        "        x_mix = self.merge_data(x_clean, x_adv)\n",
        "        y_mix = Y_train[0:second_half_end]\n",
        "        ### TODO###\n",
        "        # Mixture data for 4 epsilon values\n",
        "\n",
        "        return x_mix, y_mix\n",
        "\n",
        "    def data_iteration(self, X_train, batch_size):\n",
        "        N = X_train.shape[0]\n",
        "        start = np.random.randint(0, N-batch_size)\n",
        "        return start\n",
        "\n",
        "    def merge_data(self, x_clean, x_adv):\n",
        "        x_mix = []\n",
        "        for i in range(len(x_clean)):\n",
        "          x_mix.append(x_clean[i])\n",
        "        for j in range(len(x_adv)):\n",
        "          x_mix.append(x_adv[j])\n",
        "        x_mix = np.array(x_mix)\n",
        "        print(x_mix.shape)\n",
        "\n",
        "        return x_mix\n",
        "\n",
        "\n",
        "    def get_adversarial(self, X_true, y_true, epsilon_list):\n",
        "\n",
        "        return self.adversarial_example(X_true, y_true, epsilon_list)\n",
        "\n",
        "    def adversarial_example(self, X_true, Y_true, epsilon_list):\n",
        "        size = len(X_true)\n",
        "        X_adv = []\n",
        "        interval = int(size/4)\n",
        "        index_list = [0,interval, interval*2, interval*3, size]\n",
        "        index = 0\n",
        "        for epsilon in epsilon_list:\n",
        "          print(index)\n",
        "          if index == 4:\n",
        "            break\n",
        "          x_true = X_true[index_list[index]:index_list[index+1],:,:,:]\n",
        "          y_true = Y_true[index_list[index]:index_list[index+1]]\n",
        "\n",
        "          index = index + 1\n",
        "\n",
        "          for i in range(len(x_true)):\n",
        "            random_index = i\n",
        "            original_image = x_true[random_index]\n",
        "            original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "            original_label = y_true[random_index]\n",
        "            original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "            adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "            X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "          \n",
        "        X_adv = np.array(X_adv)\n",
        "        return X_adv\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5vG0s9PAmw",
        "colab_type": "text"
      },
      "source": [
        "Adversarial Training Second Wide ResNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR3373MWPvSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adversarial_training =  AdversarialTraining()"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2LxFwajOiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_model = tf.keras.Model(wrn_16_2.input, wrn_16_2.layers[-1].output)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i82EfjWHP2mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7529842-00e5-41e8-bf05-0229cd217edc"
      },
      "source": [
        "result_adv_df = adversarial_training.train(logits_model, X_train, y_train, X_test, y_test, EPOCHS, BS, epsilon_list)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "(4634, 32, 32, 1)\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "(515, 32, 32, 1)\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 61ms/step - loss: 1.5838 - acc: 0.3291 - val_loss: 1.5569 - val_acc: 0.3301 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5107 - acc: 0.3628 - val_loss: 1.5177 - val_acc: 0.3165 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4867 - acc: 0.3693 - val_loss: 1.4866 - val_acc: 0.3767 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4631 - acc: 0.3782 - val_loss: 1.4681 - val_acc: 0.3650 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4543 - acc: 0.4008 - val_loss: 1.4491 - val_acc: 0.3806 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4443 - acc: 0.4086 - val_loss: 1.4281 - val_acc: 0.3961 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4384 - acc: 0.4283 - val_loss: 1.4247 - val_acc: 0.4311 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4038 - acc: 0.4525 - val_loss: 1.4782 - val_acc: 0.3942 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3967 - acc: 0.4589 - val_loss: 1.3948 - val_acc: 0.4777 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3736 - acc: 0.4953 - val_loss: 1.3369 - val_acc: 0.5107 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.3403 - acc: 0.5164 - val_loss: 1.3202 - val_acc: 0.5243 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3324 - acc: 0.5224 - val_loss: 1.3151 - val_acc: 0.5068 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2870 - acc: 0.5455 - val_loss: 1.3468 - val_acc: 0.5126 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.2574 - acc: 0.5653 - val_loss: 1.3605 - val_acc: 0.5049 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2503 - acc: 0.5757 - val_loss: 1.3206 - val_acc: 0.5534 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2186 - acc: 0.5797 - val_loss: 1.2439 - val_acc: 0.5709 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1975 - acc: 0.5970 - val_loss: 1.2647 - val_acc: 0.5709 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.1875 - acc: 0.5959 - val_loss: 1.1701 - val_acc: 0.6000 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1468 - acc: 0.6158 - val_loss: 1.3180 - val_acc: 0.5320 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1435 - acc: 0.6178 - val_loss: 1.0913 - val_acc: 0.6466 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1021 - acc: 0.6396 - val_loss: 1.0580 - val_acc: 0.6447 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0847 - acc: 0.6438 - val_loss: 1.0557 - val_acc: 0.6680 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.0697 - acc: 0.6625 - val_loss: 1.1106 - val_acc: 0.6602 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.0525 - acc: 0.6605 - val_loss: 1.1537 - val_acc: 0.6194 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0268 - acc: 0.6802 - val_loss: 0.9924 - val_acc: 0.7010 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0210 - acc: 0.6795 - val_loss: 1.1417 - val_acc: 0.6427 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0185 - acc: 0.6773 - val_loss: 1.0448 - val_acc: 0.6505 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0060 - acc: 0.6924 - val_loss: 1.0004 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9811 - acc: 0.6906 - val_loss: 1.0390 - val_acc: 0.6932 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9722 - acc: 0.6962 - val_loss: 1.0928 - val_acc: 0.6835 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9458 - acc: 0.7119 - val_loss: 1.0023 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9337 - acc: 0.7159 - val_loss: 0.9927 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9252 - acc: 0.7197 - val_loss: 0.9747 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9250 - acc: 0.7248 - val_loss: 0.9775 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9177 - acc: 0.7237 - val_loss: 0.9741 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9127 - acc: 0.7246 - val_loss: 0.9851 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9022 - acc: 0.7350 - val_loss: 0.9705 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9068 - acc: 0.7359 - val_loss: 0.9650 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9065 - acc: 0.7335 - val_loss: 1.0182 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8907 - acc: 0.7390 - val_loss: 0.9658 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8957 - acc: 0.7352 - val_loss: 0.9779 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8999 - acc: 0.7324 - val_loss: 0.9568 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8988 - acc: 0.7381 - val_loss: 0.9646 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8992 - acc: 0.7377 - val_loss: 1.0172 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8852 - acc: 0.7459 - val_loss: 0.9677 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8923 - acc: 0.7383 - val_loss: 0.9669 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8952 - acc: 0.7388 - val_loss: 0.9849 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8864 - acc: 0.7368 - val_loss: 0.9597 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8883 - acc: 0.7372 - val_loss: 0.9701 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8761 - acc: 0.7463 - val_loss: 0.9669 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8891 - acc: 0.7364 - val_loss: 0.9722 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8876 - acc: 0.7410 - val_loss: 0.9602 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8811 - acc: 0.7410 - val_loss: 0.9610 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8768 - acc: 0.7450 - val_loss: 0.9624 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8827 - acc: 0.7461 - val_loss: 0.9695 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8787 - acc: 0.7426 - val_loss: 0.9733 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8826 - acc: 0.7397 - val_loss: 0.9650 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8815 - acc: 0.7492 - val_loss: 0.9813 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8807 - acc: 0.7439 - val_loss: 0.9892 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8829 - acc: 0.7386 - val_loss: 0.9545 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8740 - acc: 0.7443 - val_loss: 0.9955 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8811 - acc: 0.7408 - val_loss: 0.9478 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8740 - acc: 0.7514 - val_loss: 0.9585 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8814 - acc: 0.7399 - val_loss: 0.9566 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8875 - acc: 0.7339 - val_loss: 1.0067 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8788 - acc: 0.7443 - val_loss: 0.9514 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8768 - acc: 0.7428 - val_loss: 0.9629 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8679 - acc: 0.7506 - val_loss: 0.9790 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8745 - acc: 0.7472 - val_loss: 0.9601 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8662 - acc: 0.7492 - val_loss: 0.9732 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8768 - acc: 0.7426 - val_loss: 0.9595 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8767 - acc: 0.7472 - val_loss: 0.9850 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8728 - acc: 0.7537 - val_loss: 0.9494 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8773 - acc: 0.7441 - val_loss: 0.9664 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8795 - acc: 0.7432 - val_loss: 0.9662 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8728 - acc: 0.7423 - val_loss: 0.9815 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8799 - acc: 0.7395 - val_loss: 0.9826 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8790 - acc: 0.7497 - val_loss: 0.9633 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8733 - acc: 0.7397 - val_loss: 0.9438 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8737 - acc: 0.7417 - val_loss: 0.9511 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8743 - acc: 0.7410 - val_loss: 0.9584 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8709 - acc: 0.7488 - val_loss: 0.9550 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8772 - acc: 0.7472 - val_loss: 0.9594 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8665 - acc: 0.7455 - val_loss: 0.9804 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8752 - acc: 0.7443 - val_loss: 0.9713 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8804 - acc: 0.7423 - val_loss: 0.9584 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8723 - acc: 0.7455 - val_loss: 0.9615 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8788 - acc: 0.7426 - val_loss: 0.9840 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8745 - acc: 0.7474 - val_loss: 0.9466 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8783 - acc: 0.7419 - val_loss: 1.1046 - val_acc: 0.6583 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8790 - acc: 0.7450 - val_loss: 0.9597 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7437 - val_loss: 0.9576 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8779 - acc: 0.7419 - val_loss: 0.9489 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8704 - acc: 0.7508 - val_loss: 0.9605 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8775 - acc: 0.7463 - val_loss: 0.9469 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8674 - acc: 0.7497 - val_loss: 0.9706 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8851 - acc: 0.7430 - val_loss: 0.9599 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8806 - acc: 0.7419 - val_loss: 0.9644 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8761 - acc: 0.7432 - val_loss: 1.0330 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8728 - acc: 0.7488 - val_loss: 0.9452 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8805 - acc: 0.7423 - val_loss: 0.9491 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8724 - acc: 0.7492 - val_loss: 0.9852 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8767 - acc: 0.7437 - val_loss: 0.9800 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8774 - acc: 0.7461 - val_loss: 1.0125 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8657 - acc: 0.7499 - val_loss: 0.9578 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8724 - acc: 0.7397 - val_loss: 0.9610 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8772 - acc: 0.7419 - val_loss: 0.9511 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8760 - acc: 0.7459 - val_loss: 0.9651 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8775 - acc: 0.7472 - val_loss: 0.9795 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8869 - acc: 0.7408 - val_loss: 0.9470 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8788 - acc: 0.7474 - val_loss: 0.9692 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8688 - acc: 0.7443 - val_loss: 0.9663 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8674 - acc: 0.7466 - val_loss: 0.9538 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8717 - acc: 0.7486 - val_loss: 0.9706 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8683 - acc: 0.7492 - val_loss: 0.9454 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8703 - acc: 0.7423 - val_loss: 0.9634 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8788 - acc: 0.7446 - val_loss: 0.9493 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8766 - acc: 0.7432 - val_loss: 0.9491 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8756 - acc: 0.7450 - val_loss: 0.9545 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8766 - acc: 0.7388 - val_loss: 0.9660 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8720 - acc: 0.7499 - val_loss: 0.9645 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8718 - acc: 0.7490 - val_loss: 0.9536 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8785 - acc: 0.7437 - val_loss: 0.9533 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8765 - acc: 0.7441 - val_loss: 0.9556 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8783 - acc: 0.7466 - val_loss: 0.9577 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8725 - acc: 0.7483 - val_loss: 0.9496 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8781 - acc: 0.7392 - val_loss: 0.9663 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8809 - acc: 0.7419 - val_loss: 0.9543 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8708 - acc: 0.7441 - val_loss: 0.9673 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8710 - acc: 0.7463 - val_loss: 0.9735 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8780 - acc: 0.7474 - val_loss: 0.9463 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8682 - acc: 0.7468 - val_loss: 0.9563 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8730 - acc: 0.7430 - val_loss: 0.9808 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8686 - acc: 0.7461 - val_loss: 0.9559 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8747 - acc: 0.7428 - val_loss: 0.9561 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8746 - acc: 0.7461 - val_loss: 1.0021 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8873 - acc: 0.7403 - val_loss: 0.9766 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8723 - acc: 0.7448 - val_loss: 0.9507 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8746 - acc: 0.7481 - val_loss: 0.9598 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8755 - acc: 0.7463 - val_loss: 0.9833 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8728 - acc: 0.7463 - val_loss: 0.9499 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8761 - acc: 0.7443 - val_loss: 0.9501 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8746 - acc: 0.7426 - val_loss: 0.9477 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8709 - acc: 0.7401 - val_loss: 0.9869 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8719 - acc: 0.7465 - val_loss: 0.9821 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8784 - acc: 0.7399 - val_loss: 0.9558 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8724 - acc: 0.7466 - val_loss: 0.9454 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8754 - acc: 0.7426 - val_loss: 0.9730 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8748 - acc: 0.7481 - val_loss: 0.9639 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8746 - acc: 0.7508 - val_loss: 0.9608 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8791 - acc: 0.7392 - val_loss: 0.9651 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8712 - acc: 0.7492 - val_loss: 0.9679 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8838 - acc: 0.7388 - val_loss: 0.9654 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8807 - acc: 0.7408 - val_loss: 0.9729 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8737 - acc: 0.7483 - val_loss: 0.9657 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8699 - acc: 0.7503 - val_loss: 0.9617 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8750 - acc: 0.7441 - val_loss: 0.9506 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8741 - acc: 0.7450 - val_loss: 0.9556 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8781 - acc: 0.7481 - val_loss: 0.9923 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8823 - acc: 0.7446 - val_loss: 0.9754 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8751 - acc: 0.7428 - val_loss: 0.9572 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8747 - acc: 0.7452 - val_loss: 0.9683 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8697 - acc: 0.7501 - val_loss: 0.9468 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8682 - acc: 0.7486 - val_loss: 0.9935 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8799 - acc: 0.7470 - val_loss: 0.9625 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8752 - acc: 0.7428 - val_loss: 0.9595 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8737 - acc: 0.7410 - val_loss: 0.9651 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8725 - acc: 0.7443 - val_loss: 0.9728 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8722 - acc: 0.7457 - val_loss: 0.9732 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8684 - acc: 0.7494 - val_loss: 0.9543 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8670 - acc: 0.7483 - val_loss: 0.9700 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8729 - acc: 0.7483 - val_loss: 0.9687 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8749 - acc: 0.7468 - val_loss: 0.9734 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8784 - acc: 0.7430 - val_loss: 0.9804 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8658 - acc: 0.7523 - val_loss: 0.9585 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8725 - acc: 0.7483 - val_loss: 0.9598 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8707 - acc: 0.7512 - val_loss: 0.9667 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8697 - acc: 0.7472 - val_loss: 0.9653 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8790 - acc: 0.7490 - val_loss: 0.9751 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8759 - acc: 0.7448 - val_loss: 0.9658 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8775 - acc: 0.7472 - val_loss: 0.9517 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8773 - acc: 0.7430 - val_loss: 0.9552 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8709 - acc: 0.7472 - val_loss: 0.9755 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8767 - acc: 0.7415 - val_loss: 0.9876 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8793 - acc: 0.7428 - val_loss: 0.9438 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8761 - acc: 0.7519 - val_loss: 0.9926 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8674 - acc: 0.7452 - val_loss: 0.9470 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8753 - acc: 0.7421 - val_loss: 0.9550 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8813 - acc: 0.7446 - val_loss: 0.9806 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8758 - acc: 0.7455 - val_loss: 0.9584 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8665 - acc: 0.7457 - val_loss: 0.9521 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8770 - acc: 0.7470 - val_loss: 0.9451 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8676 - acc: 0.7472 - val_loss: 0.9621 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8755 - acc: 0.7466 - val_loss: 0.9592 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8724 - acc: 0.7474 - val_loss: 0.9634 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8739 - acc: 0.7472 - val_loss: 0.9740 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8645 - acc: 0.7512 - val_loss: 0.9559 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8777 - acc: 0.7443 - val_loss: 0.9537 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8701 - acc: 0.7501 - val_loss: 0.9586 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8731 - acc: 0.7437 - val_loss: 0.9772 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8098 - acc: 0.7592\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8272 - acc: 0.7522\n",
            "epsilon: 0.003 and test evaluation : 0.8272483348846436, 0.7521815299987793\n",
            "SNR: 50.221214294433594\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.8392 - acc: 0.7435\n",
            "epsilon: 0.005 and test evaluation : 0.8391525745391846, 0.7434554696083069\n",
            "SNR: 45.78392028808594\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.8698 - acc: 0.7312\n",
            "epsilon: 0.01 and test evaluation : 0.8698340654373169, 0.7312390804290771\n",
            "SNR: 39.76332426071167\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9367 - acc: 0.7033\n",
            "epsilon: 0.02 and test evaluation : 0.936705470085144, 0.7033158540725708\n",
            "SNR: 33.74272346496582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBzRKS8IoCR",
        "colab_type": "text"
      },
      "source": [
        "# **Show Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KYCUfIFW6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "ebb26b45-bc8c-4d51-d18b-e73fcb1be326"
      },
      "source": [
        "result_adv_df"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.809817</td>\n",
              "      <td>0.759162</td>\n",
              "      <td>0.827248</td>\n",
              "      <td>0.752182</td>\n",
              "      <td>0.839153</td>\n",
              "      <td>0.743455</td>\n",
              "      <td>0.869834</td>\n",
              "      <td>0.731239</td>\n",
              "      <td>0.936705</td>\n",
              "      <td>0.703316</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean     loss1  ...      acc3     loss4      acc4\n",
              "0    0.809817   0.759162  0.827248  ...  0.731239  0.936705  0.703316\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_mZX3HVU1G1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "e1a5ea9b-ee80-4daf-c561-681f7b99f779"
      },
      "source": [
        "result_adv_df.head(1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>clean_acc_mean</th>\n",
              "      <th>0.003_acc_mean</th>\n",
              "      <th>0.005_acc_mean</th>\n",
              "      <th>0.01_acc_mean</th>\n",
              "      <th>0.02_acc_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.97935</td>\n",
              "      <td>0.691099</td>\n",
              "      <td>0.991982</td>\n",
              "      <td>0.685864</td>\n",
              "      <td>1.000634</td>\n",
              "      <td>0.678883</td>\n",
              "      <td>1.023155</td>\n",
              "      <td>0.675393</td>\n",
              "      <td>1.07026</td>\n",
              "      <td>0.636998</td>\n",
              "      <td>0.696917</td>\n",
              "      <td>0.690518</td>\n",
              "      <td>0.686446</td>\n",
              "      <td>0.677138</td>\n",
              "      <td>0.649215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  0.01_acc_mean  0.02_acc_mean\n",
              "0     0.97935   0.691099  ...       0.677138       0.649215\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}