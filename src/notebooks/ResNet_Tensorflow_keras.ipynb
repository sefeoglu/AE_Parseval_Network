{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Tensorflow_keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/AE_Parseval_Network/blob/master/src/notebooks/ResNet_Tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cczYDRrfFlDx",
        "colab_type": "text"
      },
      "source": [
        "# Wide ResNet 16_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWvd9YADGtMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbe35240-7cd0-40d8-a1c7-5848d8869b75"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
        "import tensorflow\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tensorflow Version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqbIFJTwXLH",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRdSMgRjG8ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b4f380b2-4063-44fa-eec6-67d7719c5fe0"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0001\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "  \n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv2:channel:  {}\".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv3 channel_axis:{} \".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "if __name__ == \"__main__\":\n",
        "  init = (32, 32,1)\n",
        "  wrn_16_2 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNo5x-Ft9Fe",
        "colab_type": "text"
      },
      "source": [
        "# Data Prepare and Processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqH742XcPQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNBI_SkvuzgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euxwMe2jIoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7009e35-a837-4c73-ce9a-20e775660dbe"
      },
      "source": [
        "import cv2\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNBsNVDNu6Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53dc2ac4-e54a-4426-b697-d8550a679892"
      },
      "source": [
        "X = new_data_X.astype('float32')\n",
        "X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQdrnTKuM8c",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqf-dZOrvC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X[0].shape\n",
        "\n",
        "# transform data set\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEHVf2Bu9xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "y_df = pd.DataFrame(Y_data, columns=['Label'])\n",
        "y_df['Encoded'] = labelencoder.fit_transform(y_df['Label'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdkpb2Jkqu6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_cat = to_categorical(y_df['Encoded'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb5M1kDQnX5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size = 0.1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k70n9nAnO24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data_set.pickle', 'rb') as f:\n",
        "    x = pickle.load(f)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBUCSuWOnP5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train,X_test, y_test, X_val, y_val = x['X_train'], x['y_train'], x['X_test'], x['y_test'], x['X_val'], x['y_val']"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kif3Li9NuSnV",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88yOqhbSwjPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_sch(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.1\n",
        "    elif epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_sch)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpiWMEgRpWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-W3MPorKESw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,X_train,y_train, X_val, y_val):\n",
        "  \n",
        "  hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "  show_graph(hist)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVs_QNHoEKji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Non_adversarial(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train_iterate(self, X_train, y_train, X_val, y_val, X_test, y_test, epochs, BS,sgd, epsilon_list):\n",
        "          init = (32, 32,1)\n",
        "          res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                  'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                    'acc3','loss4', 'acc4'])\n",
        "          for j in range(3):\n",
        "            model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "            model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "            hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=epochs,\n",
        "                            callbacks = [lr_scheduler],\n",
        "                            validation_data=(X_val, y_val),\n",
        "                            validation_steps=X_val.shape[0] // BS,)\n",
        "            loss, acc = model.evaluate(X_test, y_test)\n",
        "            loss1, acc1 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "            loss2, acc2 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "            loss3, acc3 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "            loss4, acc4 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "            row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                    'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "            res_df = res_df.append(row , ignore_index=True)\n",
        "            \n",
        "          return res_df"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFgKVWiHKYsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)\n",
        "print(\"Cleverhans Version: \" + cleverhans.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6HjbpvKslU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "def get_adversarial_examples(pretrained_model, X_true, y_true, epsilon):\n",
        "  #The attack requires the model to ouput the logits\n",
        "   \n",
        "  logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
        "  X_adv = []\n",
        "  for i in range(len(X_true)):\n",
        "    random_index = i\n",
        "    original_image = X_true[random_index]\n",
        "    original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "    original_label = y_true[random_index]\n",
        "    original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "    adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "    X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "  X_adv = np.array(X_adv)\n",
        "  return X_adv\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_quzDGwKGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_graph(hist):\n",
        "  history = hist\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"wrn_tensor.png\")\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"deneme.png\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbbLi83NyVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_test(model,X_adv, X_test, y_test, epsilon):\n",
        "  loss, acc = model.evaluate(X_adv,y_test)\n",
        "  print(\"epsilon: {} and test evaluation : {}, {}\".format(epsilon,loss, acc))\n",
        "  SNR = 20*np.log10(np.linalg.norm(X_test)/np.linalg.norm(X_test-X_adv))\n",
        "  print(\"SNR: {}\".format(SNR))\n",
        "  return loss, acc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxNDNAa6MWu7",
        "colab_type": "text"
      },
      "source": [
        "**Train a Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghSgp3NvhhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "BS = 128\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBnqXaiNwHGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wrn_16_2.summary()\n",
        "wrn_16_2.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")\n",
        "\n",
        "hist = wrn_16_2.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "wrn_16_2.save(\"wrn_model.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvd1NVKvpH0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "c2b430cc-a9a2-489c-dbff-d07307c74737"
      },
      "source": [
        "show_graph(hist)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCdsaWdoWLSX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "52d75d7c-303f-4581-f995-869bed0663d5"
      },
      "source": [
        "res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "loss, acc = wrn_16_2.evaluate(X_test, y_test)\n",
        "adv1 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.003)\n",
        "loss1, acc1 = wrn_16_2.evaluate(adv1,y_test)\n",
        "adv2 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.005)\n",
        "loss2, acc2 = wrn_16_2.evaluate(adv2, y_test)\n",
        "adv3 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.01)\n",
        "loss3, acc3 = wrn_16_2.evaluate(adv3,y_test)\n",
        "adv4 = get_adversarial_examples(wrn_16_2, X_test, y_test, 0.02)\n",
        "loss4, acc4 = wrn_16_2.evaluate(adv4,y_test)\n",
        "    \n",
        "row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "           'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "res_df = res_df.append(row , ignore_index=True)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 12ms/step - loss: 0.9245 - acc: 0.7190\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0335 - acc: 0.6719\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1107 - acc: 0.6370\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3175 - acc: 0.5602\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.7531 - acc: 0.4206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN2_LSBXAlEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dcc45747-b1d3-4e69-f5b2-8b6b61d1c18b"
      },
      "source": [
        "print_test(wrn_16_2, get_adversarial_examples(wrn_16_2, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[0])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 [==============================] - 0s 4ms/step - loss: 1.7531 - acc: 0.4206\n",
            "epsilon: 0.003 and test evaluation : 1.753075122833252, 0.42059338092803955\n",
            "SNR: 33.75005006790161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.753075122833252, 0.42059338092803955)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw8RqG_AWftH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "e6c0369b-d8f8-4cad-ab49-a81007989cce"
      },
      "source": [
        "res_df"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.924536</td>\n",
              "      <td>0.719023</td>\n",
              "      <td>1.033453</td>\n",
              "      <td>0.671902</td>\n",
              "      <td>1.110741</td>\n",
              "      <td>0.636998</td>\n",
              "      <td>1.317532</td>\n",
              "      <td>0.560209</td>\n",
              "      <td>1.753075</td>\n",
              "      <td>0.420593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean     loss1  ...      acc3     loss4      acc4\n",
              "0    0.924536   0.719023  1.033453  ...  0.560209  1.753075  0.420593\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDHJIheU8bm",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydHkZ8pauMe",
        "colab_type": "text"
      },
      "source": [
        "**Non_Adversarial Training Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkjbXCV6KFcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb050acc-a73d-4b26-b38d-ef42733835a6"
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "logits_model = tf.keras.Model(wrn_16_2.input, wrn_16_2.layers[-1].output)\n",
        "train_object = Non_adversarial()\n",
        "result_df = train_object.train_iterate(X_train, y_train, X_val, y_val, X_test, y_test, EPOCHS, BS,sgd, epsilon_list)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.6009 - acc: 0.2945 - val_loss: 1.5739 - val_acc: 0.3767 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.5252 - acc: 0.3642 - val_loss: 1.5484 - val_acc: 0.3456 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4794 - acc: 0.3739 - val_loss: 1.4652 - val_acc: 0.3864 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4643 - acc: 0.3886 - val_loss: 1.4979 - val_acc: 0.3845 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4518 - acc: 0.3924 - val_loss: 1.5835 - val_acc: 0.4078 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4406 - acc: 0.4097 - val_loss: 1.4557 - val_acc: 0.4117 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4274 - acc: 0.4336 - val_loss: 1.4590 - val_acc: 0.4039 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.3888 - acc: 0.4647 - val_loss: 1.6501 - val_acc: 0.3689 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3666 - acc: 0.4922 - val_loss: 1.4028 - val_acc: 0.4932 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.3436 - acc: 0.5144 - val_loss: 1.3439 - val_acc: 0.5204 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.2897 - acc: 0.5411 - val_loss: 1.2288 - val_acc: 0.5786 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.2776 - acc: 0.5499 - val_loss: 1.3192 - val_acc: 0.5320 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2298 - acc: 0.5763 - val_loss: 1.2083 - val_acc: 0.5825 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.2120 - acc: 0.5814 - val_loss: 1.2434 - val_acc: 0.5534 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1881 - acc: 0.5897 - val_loss: 1.1731 - val_acc: 0.6117 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1558 - acc: 0.6105 - val_loss: 1.2312 - val_acc: 0.6000 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0968 - acc: 0.6421 - val_loss: 1.1955 - val_acc: 0.6272 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1127 - acc: 0.6307 - val_loss: 1.1741 - val_acc: 0.6291 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0651 - acc: 0.6560 - val_loss: 1.0387 - val_acc: 0.6660 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0503 - acc: 0.6653 - val_loss: 1.2425 - val_acc: 0.6369 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0399 - acc: 0.6711 - val_loss: 1.0638 - val_acc: 0.6602 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9950 - acc: 0.6886 - val_loss: 4.2054 - val_acc: 0.3883 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.0174 - acc: 0.6873 - val_loss: 1.2896 - val_acc: 0.6447 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9732 - acc: 0.6991 - val_loss: 1.0861 - val_acc: 0.6583 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9771 - acc: 0.6980 - val_loss: 1.2144 - val_acc: 0.6252 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9468 - acc: 0.7166 - val_loss: 1.0618 - val_acc: 0.6874 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9293 - acc: 0.7188 - val_loss: 0.9650 - val_acc: 0.7107 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9104 - acc: 0.7268 - val_loss: 1.1219 - val_acc: 0.6718 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.9142 - acc: 0.7208 - val_loss: 0.9750 - val_acc: 0.7262 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8918 - acc: 0.7310 - val_loss: 1.1813 - val_acc: 0.6718 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8942 - acc: 0.7295 - val_loss: 1.0344 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8580 - acc: 0.7548 - val_loss: 0.9945 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8329 - acc: 0.7605 - val_loss: 0.9880 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8226 - acc: 0.7739 - val_loss: 0.9898 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8232 - acc: 0.7634 - val_loss: 0.9527 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8138 - acc: 0.7710 - val_loss: 1.0595 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8201 - acc: 0.7668 - val_loss: 0.9837 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8075 - acc: 0.7770 - val_loss: 0.9545 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8112 - acc: 0.7712 - val_loss: 1.0084 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8026 - acc: 0.7767 - val_loss: 0.9845 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8023 - acc: 0.7741 - val_loss: 0.9649 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7992 - acc: 0.7796 - val_loss: 0.9948 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8045 - acc: 0.7745 - val_loss: 0.9956 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7968 - acc: 0.7765 - val_loss: 1.0194 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7929 - acc: 0.7761 - val_loss: 0.9560 - val_acc: 0.7359 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7986 - acc: 0.7736 - val_loss: 0.9813 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8045 - acc: 0.7741 - val_loss: 0.9800 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7975 - acc: 0.7763 - val_loss: 1.0014 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7891 - acc: 0.7783 - val_loss: 0.9533 - val_acc: 0.7320 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7929 - acc: 0.7825 - val_loss: 0.9403 - val_acc: 0.7437 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7900 - acc: 0.7794 - val_loss: 0.9745 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7915 - acc: 0.7743 - val_loss: 1.0364 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7847 - acc: 0.7874 - val_loss: 0.9466 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7809 - acc: 0.7805 - val_loss: 0.9748 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7816 - acc: 0.7863 - val_loss: 0.9399 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7832 - acc: 0.7798 - val_loss: 0.9677 - val_acc: 0.7398 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7884 - acc: 0.7838 - val_loss: 0.9638 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7783 - acc: 0.7883 - val_loss: 0.9728 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7824 - acc: 0.7796 - val_loss: 0.9670 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7722 - acc: 0.7898 - val_loss: 0.9148 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7889 - acc: 0.7818 - val_loss: 0.9247 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7794 - acc: 0.7818 - val_loss: 0.9494 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7887 - acc: 0.7759 - val_loss: 0.9410 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7786 - acc: 0.7892 - val_loss: 0.9301 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7710 - acc: 0.7850 - val_loss: 0.9831 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7861 - acc: 0.7810 - val_loss: 1.0478 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7795 - acc: 0.7847 - val_loss: 0.9540 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7733 - acc: 0.7867 - val_loss: 0.9891 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7812 - acc: 0.7830 - val_loss: 0.9811 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7855 - acc: 0.7770 - val_loss: 0.9628 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7747 - acc: 0.7898 - val_loss: 0.9330 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7920 - acc: 0.7801 - val_loss: 0.9887 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7826 - acc: 0.7821 - val_loss: 0.9463 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7916 - acc: 0.7785 - val_loss: 1.0594 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7769 - acc: 0.7854 - val_loss: 1.0253 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7781 - acc: 0.7812 - val_loss: 0.9602 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7790 - acc: 0.7796 - val_loss: 0.9422 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7769 - acc: 0.7850 - val_loss: 0.9330 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7824 - acc: 0.7794 - val_loss: 0.9255 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7759 - acc: 0.7876 - val_loss: 0.9699 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7859 - acc: 0.7807 - val_loss: 0.9447 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7883 - acc: 0.7781 - val_loss: 0.9316 - val_acc: 0.7534 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7874 - acc: 0.7785 - val_loss: 0.9840 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7735 - acc: 0.7827 - val_loss: 0.9513 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7736 - acc: 0.7881 - val_loss: 0.9526 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7850 - acc: 0.7834 - val_loss: 0.9521 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7815 - acc: 0.7803 - val_loss: 0.9817 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7738 - acc: 0.7843 - val_loss: 0.9762 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7817 - acc: 0.7765 - val_loss: 0.9763 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7723 - acc: 0.7874 - val_loss: 0.9842 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7859 - acc: 0.7770 - val_loss: 0.9514 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7908 - acc: 0.7747 - val_loss: 1.0078 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7838 - acc: 0.7756 - val_loss: 0.9419 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7873 - acc: 0.7752 - val_loss: 0.9814 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7742 - acc: 0.7856 - val_loss: 0.9946 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7858 - acc: 0.7909 - val_loss: 0.9587 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7893 - acc: 0.7796 - val_loss: 0.9575 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7896 - acc: 0.7810 - val_loss: 0.9537 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7781 - acc: 0.7794 - val_loss: 0.9791 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7839 - acc: 0.7783 - val_loss: 0.9277 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7808 - acc: 0.7925 - val_loss: 0.9685 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7807 - acc: 0.7872 - val_loss: 0.9666 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7750 - acc: 0.7823 - val_loss: 0.9390 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7783 - acc: 0.7834 - val_loss: 0.9410 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7806 - acc: 0.7895 - val_loss: 0.9626 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7735 - acc: 0.7867 - val_loss: 0.9663 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7792 - acc: 0.7832 - val_loss: 0.9763 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7819 - acc: 0.7772 - val_loss: 0.9582 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7859 - acc: 0.7823 - val_loss: 0.9885 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7817 - acc: 0.7816 - val_loss: 0.9554 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7713 - acc: 0.7872 - val_loss: 0.9307 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7801 - acc: 0.7781 - val_loss: 0.9529 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7687 - acc: 0.7952 - val_loss: 0.9250 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7857 - acc: 0.7805 - val_loss: 0.9407 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7807 - acc: 0.7841 - val_loss: 0.9446 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7796 - acc: 0.7801 - val_loss: 0.9365 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7910 - acc: 0.7792 - val_loss: 0.9733 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7738 - acc: 0.7856 - val_loss: 0.9468 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7804 - acc: 0.7816 - val_loss: 1.0046 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7758 - acc: 0.7794 - val_loss: 0.9496 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7852 - acc: 0.7798 - val_loss: 0.9448 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7813 - acc: 0.7803 - val_loss: 0.9438 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7802 - acc: 0.7787 - val_loss: 0.9291 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7800 - acc: 0.7825 - val_loss: 0.9494 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7806 - acc: 0.7885 - val_loss: 0.9290 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7870 - acc: 0.7876 - val_loss: 0.9892 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7807 - acc: 0.7856 - val_loss: 0.9457 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7746 - acc: 0.7865 - val_loss: 0.9369 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7753 - acc: 0.7925 - val_loss: 0.9530 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7803 - acc: 0.7838 - val_loss: 0.9924 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7915 - acc: 0.7801 - val_loss: 0.9539 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7808 - acc: 0.7798 - val_loss: 0.9884 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7877 - acc: 0.7776 - val_loss: 0.9388 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7809 - acc: 0.7803 - val_loss: 0.9652 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7765 - acc: 0.7847 - val_loss: 0.9218 - val_acc: 0.7495 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7811 - acc: 0.7776 - val_loss: 0.9395 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7851 - acc: 0.7816 - val_loss: 1.0129 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7861 - acc: 0.7776 - val_loss: 0.9902 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7774 - acc: 0.7881 - val_loss: 0.9608 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7816 - acc: 0.7836 - val_loss: 0.9819 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7862 - acc: 0.7818 - val_loss: 0.9300 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7850 - acc: 0.7792 - val_loss: 0.9651 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7898 - acc: 0.7812 - val_loss: 0.9351 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7880 - acc: 0.7794 - val_loss: 0.9831 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7808 - acc: 0.7834 - val_loss: 0.9652 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7929 - acc: 0.7792 - val_loss: 0.9566 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7755 - acc: 0.7841 - val_loss: 0.9727 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7775 - acc: 0.7850 - val_loss: 0.9521 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7902 - acc: 0.7734 - val_loss: 0.9440 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7824 - acc: 0.7750 - val_loss: 0.9851 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7880 - acc: 0.7785 - val_loss: 0.9444 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7941 - acc: 0.7774 - val_loss: 0.9483 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7747 - acc: 0.7856 - val_loss: 0.9958 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7923 - acc: 0.7759 - val_loss: 0.9540 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7775 - acc: 0.7885 - val_loss: 0.9522 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.7757 - acc: 0.7854 - val_loss: 0.9441 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7789 - acc: 0.7852 - val_loss: 0.9637 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7830 - acc: 0.7792 - val_loss: 0.9507 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7832 - acc: 0.7818 - val_loss: 0.9305 - val_acc: 0.7573 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7856 - acc: 0.7807 - val_loss: 0.9435 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7853 - acc: 0.7798 - val_loss: 0.9366 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7798 - acc: 0.7858 - val_loss: 0.9813 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7888 - acc: 0.7803 - val_loss: 0.9570 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7827 - acc: 0.7801 - val_loss: 0.9796 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7892 - acc: 0.7798 - val_loss: 0.9686 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7826 - acc: 0.7823 - val_loss: 0.9469 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7793 - acc: 0.7838 - val_loss: 0.9780 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7743 - acc: 0.7845 - val_loss: 0.9763 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7792 - acc: 0.7834 - val_loss: 0.9316 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7839 - acc: 0.7867 - val_loss: 0.9675 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7790 - acc: 0.7845 - val_loss: 0.9451 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7786 - acc: 0.7858 - val_loss: 1.0048 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7823 - acc: 0.7874 - val_loss: 0.9361 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7769 - acc: 0.7834 - val_loss: 0.9345 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7826 - acc: 0.7883 - val_loss: 0.9854 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7830 - acc: 0.7856 - val_loss: 0.9776 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7843 - acc: 0.7787 - val_loss: 0.9889 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7804 - acc: 0.7838 - val_loss: 1.0411 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7789 - acc: 0.7841 - val_loss: 0.9639 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7753 - acc: 0.7832 - val_loss: 0.9432 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7727 - acc: 0.7894 - val_loss: 1.0350 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7761 - acc: 0.7810 - val_loss: 0.9748 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7756 - acc: 0.7818 - val_loss: 0.9476 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7751 - acc: 0.7810 - val_loss: 0.9492 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7883 - acc: 0.7810 - val_loss: 0.9777 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7795 - acc: 0.7843 - val_loss: 0.9685 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7824 - acc: 0.7783 - val_loss: 0.9283 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7775 - acc: 0.7801 - val_loss: 0.9668 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7844 - acc: 0.7763 - val_loss: 0.9364 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7742 - acc: 0.7852 - val_loss: 0.9703 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7858 - acc: 0.7772 - val_loss: 0.9678 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7761 - acc: 0.7892 - val_loss: 0.9261 - val_acc: 0.7515 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7771 - acc: 0.7858 - val_loss: 0.9546 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7846 - acc: 0.7818 - val_loss: 0.9639 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7774 - acc: 0.7874 - val_loss: 0.9516 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7787 - acc: 0.7798 - val_loss: 0.9621 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7879 - acc: 0.7785 - val_loss: 0.9532 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.7764 - acc: 0.7838 - val_loss: 0.9652 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.7832 - acc: 0.7863 - val_loss: 0.9334 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.7802 - acc: 0.7836 - val_loss: 0.9725 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9353 - acc: 0.7138\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0595 - acc: 0.6841\n",
            "epsilon: 0.003 and test evaluation : 1.0594652891159058, 0.6841186881065369\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1482 - acc: 0.6545\n",
            "epsilon: 0.005 and test evaluation : 1.1481841802597046, 0.6544502377510071\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.3858 - acc: 0.5899\n",
            "epsilon: 0.01 and test evaluation : 1.3857669830322266, 0.5898778438568115\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.8987 - acc: 0.4433\n",
            "epsilon: 0.02 and test evaluation : 1.8986780643463135, 0.443280965089798\n",
            "SNR: 33.75005006790161\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5873 - acc: 0.3174 - val_loss: 1.5439 - val_acc: 0.3379 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.5008 - acc: 0.3724 - val_loss: 1.5134 - val_acc: 0.3495 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4848 - acc: 0.3724 - val_loss: 1.4518 - val_acc: 0.3981 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4581 - acc: 0.3946 - val_loss: 1.4749 - val_acc: 0.3437 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4486 - acc: 0.3990 - val_loss: 1.4427 - val_acc: 0.4175 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4294 - acc: 0.4283 - val_loss: 1.4375 - val_acc: 0.4544 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4145 - acc: 0.4525 - val_loss: 1.4163 - val_acc: 0.4466 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3973 - acc: 0.4716 - val_loss: 1.5047 - val_acc: 0.3981 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3697 - acc: 0.4905 - val_loss: 1.3322 - val_acc: 0.5243 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3670 - acc: 0.4938 - val_loss: 1.3237 - val_acc: 0.5534 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3266 - acc: 0.5242 - val_loss: 1.4844 - val_acc: 0.4078 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3024 - acc: 0.5324 - val_loss: 1.2831 - val_acc: 0.5456 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2748 - acc: 0.5495 - val_loss: 1.2396 - val_acc: 0.5864 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2395 - acc: 0.5723 - val_loss: 1.4431 - val_acc: 0.5184 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.2291 - acc: 0.5808 - val_loss: 1.1739 - val_acc: 0.5825 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1950 - acc: 0.6039 - val_loss: 1.3406 - val_acc: 0.5223 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.1613 - acc: 0.6176 - val_loss: 1.1234 - val_acc: 0.6311 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.1280 - acc: 0.6229 - val_loss: 1.0956 - val_acc: 0.6175 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.1119 - acc: 0.6300 - val_loss: 1.1483 - val_acc: 0.5981 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0617 - acc: 0.6534 - val_loss: 1.1162 - val_acc: 0.6311 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0470 - acc: 0.6549 - val_loss: 1.1865 - val_acc: 0.6058 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0388 - acc: 0.6702 - val_loss: 1.0837 - val_acc: 0.6563 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.0256 - acc: 0.6729 - val_loss: 1.0907 - val_acc: 0.6388 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0050 - acc: 0.6822 - val_loss: 1.1647 - val_acc: 0.6330 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9884 - acc: 0.6942 - val_loss: 1.0565 - val_acc: 0.6718 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9820 - acc: 0.6922 - val_loss: 1.0603 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9529 - acc: 0.7048 - val_loss: 1.0223 - val_acc: 0.7029 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.9481 - acc: 0.7099 - val_loss: 1.2638 - val_acc: 0.6214 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.9413 - acc: 0.7162 - val_loss: 1.2625 - val_acc: 0.6388 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9326 - acc: 0.7164 - val_loss: 1.1415 - val_acc: 0.6466 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8944 - acc: 0.7310 - val_loss: 1.0085 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8762 - acc: 0.7359 - val_loss: 1.0198 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8762 - acc: 0.7430 - val_loss: 1.0340 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8582 - acc: 0.7541 - val_loss: 0.9930 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8595 - acc: 0.7472 - val_loss: 0.9900 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8558 - acc: 0.7466 - val_loss: 0.9910 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8549 - acc: 0.7541 - val_loss: 1.0009 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8527 - acc: 0.7481 - val_loss: 0.9899 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8489 - acc: 0.7548 - val_loss: 1.0341 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8533 - acc: 0.7537 - val_loss: 1.0044 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8539 - acc: 0.7534 - val_loss: 0.9843 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8488 - acc: 0.7479 - val_loss: 0.9709 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8526 - acc: 0.7552 - val_loss: 1.0294 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8451 - acc: 0.7503 - val_loss: 0.9899 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8467 - acc: 0.7526 - val_loss: 0.9944 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8496 - acc: 0.7554 - val_loss: 0.9734 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8437 - acc: 0.7590 - val_loss: 1.0032 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8427 - acc: 0.7563 - val_loss: 1.0048 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8355 - acc: 0.7563 - val_loss: 1.0151 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8360 - acc: 0.7579 - val_loss: 0.9788 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8307 - acc: 0.7588 - val_loss: 1.0314 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8422 - acc: 0.7568 - val_loss: 0.9903 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7623 - val_loss: 1.0181 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8341 - acc: 0.7559 - val_loss: 0.9731 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8398 - acc: 0.7552 - val_loss: 0.9880 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8349 - acc: 0.7570 - val_loss: 1.0056 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8323 - acc: 0.7574 - val_loss: 0.9800 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8246 - acc: 0.7605 - val_loss: 0.9846 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8313 - acc: 0.7572 - val_loss: 0.9702 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8308 - acc: 0.7652 - val_loss: 0.9946 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8215 - acc: 0.7621 - val_loss: 0.9923 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8258 - acc: 0.7617 - val_loss: 0.9724 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8275 - acc: 0.7636 - val_loss: 0.9720 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8362 - acc: 0.7561 - val_loss: 1.0479 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8282 - acc: 0.7630 - val_loss: 1.0037 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8272 - acc: 0.7608 - val_loss: 1.0287 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8323 - acc: 0.7617 - val_loss: 1.0378 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8353 - acc: 0.7534 - val_loss: 0.9768 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8273 - acc: 0.7605 - val_loss: 0.9703 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8375 - acc: 0.7561 - val_loss: 1.0055 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8289 - acc: 0.7641 - val_loss: 1.0273 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8275 - acc: 0.7610 - val_loss: 0.9882 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8333 - acc: 0.7548 - val_loss: 0.9869 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8255 - acc: 0.7605 - val_loss: 0.9931 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8239 - acc: 0.7685 - val_loss: 0.9814 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8233 - acc: 0.7608 - val_loss: 1.0083 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8160 - acc: 0.7681 - val_loss: 0.9850 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8231 - acc: 0.7668 - val_loss: 0.9783 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8273 - acc: 0.7634 - val_loss: 0.9833 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8322 - acc: 0.7568 - val_loss: 0.9681 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8393 - acc: 0.7517 - val_loss: 1.0220 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8283 - acc: 0.7583 - val_loss: 0.9967 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8386 - acc: 0.7612 - val_loss: 0.9742 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8301 - acc: 0.7628 - val_loss: 0.9883 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8209 - acc: 0.7643 - val_loss: 0.9983 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8210 - acc: 0.7674 - val_loss: 0.9858 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8305 - acc: 0.7614 - val_loss: 0.9912 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8339 - acc: 0.7570 - val_loss: 1.0045 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8353 - acc: 0.7532 - val_loss: 1.0256 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8305 - acc: 0.7574 - val_loss: 1.0047 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8332 - acc: 0.7614 - val_loss: 1.0575 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8248 - acc: 0.7639 - val_loss: 0.9894 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8307 - acc: 0.7630 - val_loss: 0.9956 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8310 - acc: 0.7605 - val_loss: 0.9992 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8315 - acc: 0.7563 - val_loss: 1.0148 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8274 - acc: 0.7599 - val_loss: 0.9908 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8273 - acc: 0.7681 - val_loss: 1.0067 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8240 - acc: 0.7632 - val_loss: 1.0368 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8284 - acc: 0.7610 - val_loss: 1.0397 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8276 - acc: 0.7583 - val_loss: 1.0188 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8268 - acc: 0.7599 - val_loss: 0.9837 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8396 - acc: 0.7559 - val_loss: 0.9939 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8177 - acc: 0.7690 - val_loss: 1.0006 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8276 - acc: 0.7608 - val_loss: 0.9742 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8249 - acc: 0.7654 - val_loss: 0.9929 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8231 - acc: 0.7621 - val_loss: 1.0300 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8367 - acc: 0.7554 - val_loss: 0.9718 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8268 - acc: 0.7690 - val_loss: 0.9976 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8265 - acc: 0.7622 - val_loss: 0.9934 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8368 - acc: 0.7548 - val_loss: 0.9587 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8400 - acc: 0.7612 - val_loss: 1.0042 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8333 - acc: 0.7608 - val_loss: 0.9904 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.8392 - acc: 0.7545 - val_loss: 1.0063 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8333 - acc: 0.7594 - val_loss: 0.9623 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8260 - acc: 0.7696 - val_loss: 0.9727 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8358 - acc: 0.7583 - val_loss: 0.9993 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8302 - acc: 0.7552 - val_loss: 0.9781 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8298 - acc: 0.7570 - val_loss: 0.9629 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8298 - acc: 0.7663 - val_loss: 0.9675 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8347 - acc: 0.7599 - val_loss: 0.9987 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8334 - acc: 0.7617 - val_loss: 0.9962 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8233 - acc: 0.7623 - val_loss: 1.0196 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8251 - acc: 0.7654 - val_loss: 0.9662 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8312 - acc: 0.7613 - val_loss: 0.9923 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8226 - acc: 0.7688 - val_loss: 0.9903 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8215 - acc: 0.7570 - val_loss: 1.0038 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8332 - acc: 0.7545 - val_loss: 0.9776 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8284 - acc: 0.7601 - val_loss: 1.0024 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8203 - acc: 0.7628 - val_loss: 0.9770 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8342 - acc: 0.7539 - val_loss: 0.9764 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8301 - acc: 0.7619 - val_loss: 0.9876 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8334 - acc: 0.7572 - val_loss: 0.9642 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8175 - acc: 0.7603 - val_loss: 0.9976 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8236 - acc: 0.7601 - val_loss: 1.0296 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8396 - acc: 0.7572 - val_loss: 0.9808 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8282 - acc: 0.7581 - val_loss: 0.9844 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8363 - acc: 0.7532 - val_loss: 0.9825 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8374 - acc: 0.7565 - val_loss: 1.0119 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8269 - acc: 0.7621 - val_loss: 0.9687 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8333 - acc: 0.7572 - val_loss: 0.9696 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8334 - acc: 0.7588 - val_loss: 0.9922 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8322 - acc: 0.7581 - val_loss: 1.0153 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8188 - acc: 0.7659 - val_loss: 0.9923 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8275 - acc: 0.7645 - val_loss: 1.0435 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8367 - acc: 0.7574 - val_loss: 0.9931 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8250 - acc: 0.7601 - val_loss: 1.0057 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8271 - acc: 0.7590 - val_loss: 0.9833 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8249 - acc: 0.7676 - val_loss: 1.0093 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8258 - acc: 0.7628 - val_loss: 0.9588 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8280 - acc: 0.7663 - val_loss: 0.9755 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8240 - acc: 0.7630 - val_loss: 0.9916 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8333 - acc: 0.7592 - val_loss: 0.9740 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8248 - acc: 0.7654 - val_loss: 0.9783 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8351 - acc: 0.7574 - val_loss: 0.9787 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8273 - acc: 0.7668 - val_loss: 1.0242 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8352 - acc: 0.7526 - val_loss: 1.0415 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8290 - acc: 0.7572 - val_loss: 1.0262 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8163 - acc: 0.7690 - val_loss: 0.9952 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8318 - acc: 0.7523 - val_loss: 0.9835 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8345 - acc: 0.7672 - val_loss: 1.0179 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8300 - acc: 0.7552 - val_loss: 1.0170 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8276 - acc: 0.7611 - val_loss: 1.0301 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8308 - acc: 0.7641 - val_loss: 1.0518 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8306 - acc: 0.7652 - val_loss: 0.9737 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8274 - acc: 0.7634 - val_loss: 1.0292 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8278 - acc: 0.7601 - val_loss: 0.9713 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8342 - acc: 0.7568 - val_loss: 0.9868 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8322 - acc: 0.7579 - val_loss: 1.0222 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8321 - acc: 0.7592 - val_loss: 1.0104 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8255 - acc: 0.7668 - val_loss: 0.9831 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8320 - acc: 0.7552 - val_loss: 0.9801 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8302 - acc: 0.7568 - val_loss: 0.9732 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8280 - acc: 0.7597 - val_loss: 0.9866 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8249 - acc: 0.7641 - val_loss: 0.9944 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8385 - acc: 0.7570 - val_loss: 0.9779 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8253 - acc: 0.7610 - val_loss: 1.0240 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8383 - acc: 0.7610 - val_loss: 0.9782 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8305 - acc: 0.7634 - val_loss: 1.0056 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8184 - acc: 0.7683 - val_loss: 0.9871 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8263 - acc: 0.7619 - val_loss: 1.0048 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8281 - acc: 0.7594 - val_loss: 0.9976 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8272 - acc: 0.7608 - val_loss: 1.0090 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8306 - acc: 0.7652 - val_loss: 1.0045 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7603 - val_loss: 1.0341 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8259 - acc: 0.7645 - val_loss: 0.9604 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8284 - acc: 0.7603 - val_loss: 0.9894 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8218 - acc: 0.7625 - val_loss: 0.9709 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8260 - acc: 0.7577 - val_loss: 0.9838 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8293 - acc: 0.7603 - val_loss: 0.9983 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8332 - acc: 0.7603 - val_loss: 0.9739 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8244 - acc: 0.7599 - val_loss: 0.9835 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8222 - acc: 0.7641 - val_loss: 1.0004 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8271 - acc: 0.7605 - val_loss: 1.0031 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8218 - acc: 0.7659 - val_loss: 0.9819 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7583 - val_loss: 0.9617 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8322 - acc: 0.7579 - val_loss: 0.9787 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8344 - acc: 0.7557 - val_loss: 1.0599 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8242 - acc: 0.7610 - val_loss: 0.9763 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8233 - acc: 0.7599 - val_loss: 1.0454 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8311 - acc: 0.7610 - val_loss: 0.9884 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9977 - acc: 0.6998\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1290 - acc: 0.6475\n",
            "epsilon: 0.003 and test evaluation : 1.1289870738983154, 0.6474694609642029\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.2236 - acc: 0.6003\n",
            "epsilon: 0.005 and test evaluation : 1.2236303091049194, 0.6003490686416626\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.4766 - acc: 0.5393\n",
            "epsilon: 0.01 and test evaluation : 1.4765980243682861, 0.5392670035362244\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0071 - acc: 0.4084\n",
            "epsilon: 0.02 and test evaluation : 2.0070960521698, 0.4083769619464874\n",
            "SNR: 33.75005006790161\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5861 - acc: 0.3185 - val_loss: 1.5674 - val_acc: 0.3340 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.5093 - acc: 0.3597 - val_loss: 1.4907 - val_acc: 0.3612 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4788 - acc: 0.3715 - val_loss: 1.5032 - val_acc: 0.3883 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4673 - acc: 0.3848 - val_loss: 1.4509 - val_acc: 0.4194 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4592 - acc: 0.3950 - val_loss: 1.4411 - val_acc: 0.4019 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4437 - acc: 0.4032 - val_loss: 1.4356 - val_acc: 0.4311 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4414 - acc: 0.4206 - val_loss: 1.4495 - val_acc: 0.3961 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4280 - acc: 0.4387 - val_loss: 1.4083 - val_acc: 0.4757 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.4063 - acc: 0.4581 - val_loss: 1.5010 - val_acc: 0.3883 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 1.3889 - acc: 0.4751 - val_loss: 1.4459 - val_acc: 0.4330 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3540 - acc: 0.5000 - val_loss: 1.3750 - val_acc: 0.4699 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3248 - acc: 0.5293 - val_loss: 1.6942 - val_acc: 0.4350 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.2928 - acc: 0.5419 - val_loss: 1.3909 - val_acc: 0.5068 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2540 - acc: 0.5677 - val_loss: 1.2665 - val_acc: 0.5650 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2268 - acc: 0.5834 - val_loss: 1.2006 - val_acc: 0.5961 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.2109 - acc: 0.5830 - val_loss: 1.2796 - val_acc: 0.5573 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1734 - acc: 0.5988 - val_loss: 1.1256 - val_acc: 0.6330 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1515 - acc: 0.6127 - val_loss: 1.5317 - val_acc: 0.5437 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1414 - acc: 0.6156 - val_loss: 1.1540 - val_acc: 0.6272 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.0961 - acc: 0.6411 - val_loss: 1.2151 - val_acc: 0.6039 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0866 - acc: 0.6480 - val_loss: 1.1005 - val_acc: 0.6485 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0467 - acc: 0.6653 - val_loss: 1.1038 - val_acc: 0.6408 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.0384 - acc: 0.6696 - val_loss: 1.2181 - val_acc: 0.6117 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0244 - acc: 0.6767 - val_loss: 1.0197 - val_acc: 0.6854 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9960 - acc: 0.6824 - val_loss: 1.2258 - val_acc: 0.5845 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9805 - acc: 0.6977 - val_loss: 1.0649 - val_acc: 0.6699 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9779 - acc: 0.6951 - val_loss: 1.1120 - val_acc: 0.6427 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.9511 - acc: 0.7117 - val_loss: 1.5748 - val_acc: 0.5243 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9564 - acc: 0.7071 - val_loss: 1.1404 - val_acc: 0.6524 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9230 - acc: 0.7168 - val_loss: 1.0690 - val_acc: 0.6893 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9285 - acc: 0.7230 - val_loss: 1.0520 - val_acc: 0.6777 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8890 - acc: 0.7364 - val_loss: 1.0182 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8722 - acc: 0.7448 - val_loss: 1.0003 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8721 - acc: 0.7521 - val_loss: 1.0333 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8738 - acc: 0.7517 - val_loss: 0.9813 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8439 - acc: 0.7585 - val_loss: 1.0132 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8529 - acc: 0.7581 - val_loss: 1.0191 - val_acc: 0.6874 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8573 - acc: 0.7519 - val_loss: 1.0463 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8445 - acc: 0.7603 - val_loss: 1.0559 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8441 - acc: 0.7574 - val_loss: 1.0134 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8431 - acc: 0.7595 - val_loss: 1.0566 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8385 - acc: 0.7630 - val_loss: 1.0215 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8452 - acc: 0.7512 - val_loss: 1.0509 - val_acc: 0.6874 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8427 - acc: 0.7563 - val_loss: 1.1210 - val_acc: 0.6738 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8424 - acc: 0.7605 - val_loss: 1.0002 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8404 - acc: 0.7623 - val_loss: 0.9950 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8474 - acc: 0.7519 - val_loss: 1.0185 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8322 - acc: 0.7639 - val_loss: 1.0532 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8343 - acc: 0.7632 - val_loss: 1.0390 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8306 - acc: 0.7690 - val_loss: 1.0292 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8407 - acc: 0.7581 - val_loss: 0.9839 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8374 - acc: 0.7641 - val_loss: 1.0294 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8343 - acc: 0.7608 - val_loss: 1.0141 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8293 - acc: 0.7625 - val_loss: 0.9980 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7639 - val_loss: 1.0705 - val_acc: 0.6816 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8322 - acc: 0.7645 - val_loss: 1.0383 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8321 - acc: 0.7601 - val_loss: 0.9936 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8222 - acc: 0.7710 - val_loss: 1.0319 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8238 - acc: 0.7597 - val_loss: 1.0019 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8329 - acc: 0.7663 - val_loss: 1.0058 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8240 - acc: 0.7676 - val_loss: 1.0055 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8234 - acc: 0.7696 - val_loss: 0.9950 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8187 - acc: 0.7696 - val_loss: 1.0455 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8203 - acc: 0.7676 - val_loss: 1.0421 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8263 - acc: 0.7623 - val_loss: 1.0963 - val_acc: 0.6718 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7612 - val_loss: 1.1118 - val_acc: 0.6699 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8296 - acc: 0.7643 - val_loss: 0.9788 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7705 - val_loss: 0.9830 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7619 - val_loss: 1.0554 - val_acc: 0.6816 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8163 - acc: 0.7670 - val_loss: 1.0455 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8324 - acc: 0.7650 - val_loss: 1.0759 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8282 - acc: 0.7676 - val_loss: 0.9964 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8160 - acc: 0.7699 - val_loss: 1.0062 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8166 - acc: 0.7701 - val_loss: 1.0066 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8368 - acc: 0.7621 - val_loss: 1.0286 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8255 - acc: 0.7632 - val_loss: 0.9950 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8296 - acc: 0.7708 - val_loss: 0.9852 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8171 - acc: 0.7716 - val_loss: 1.0688 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8270 - acc: 0.7634 - val_loss: 1.0780 - val_acc: 0.6816 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8280 - acc: 0.7625 - val_loss: 1.0583 - val_acc: 0.6796 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8332 - acc: 0.7610 - val_loss: 1.0258 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8242 - acc: 0.7688 - val_loss: 1.0497 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8146 - acc: 0.7710 - val_loss: 1.1370 - val_acc: 0.6621 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8230 - acc: 0.7712 - val_loss: 0.9884 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8243 - acc: 0.7685 - val_loss: 1.0358 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8288 - acc: 0.7685 - val_loss: 1.0308 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7583 - val_loss: 1.0512 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8238 - acc: 0.7670 - val_loss: 1.0765 - val_acc: 0.6816 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8278 - acc: 0.7641 - val_loss: 1.0493 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8272 - acc: 0.7623 - val_loss: 1.0255 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8300 - acc: 0.7569 - val_loss: 1.0296 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8294 - acc: 0.7597 - val_loss: 1.0735 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8210 - acc: 0.7654 - val_loss: 1.0345 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8227 - acc: 0.7674 - val_loss: 1.0448 - val_acc: 0.6835 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8274 - acc: 0.7650 - val_loss: 1.0048 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8248 - acc: 0.7701 - val_loss: 1.0267 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8255 - acc: 0.7645 - val_loss: 1.0776 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8259 - acc: 0.7659 - val_loss: 1.0681 - val_acc: 0.6835 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8298 - acc: 0.7634 - val_loss: 1.0351 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8198 - acc: 0.7685 - val_loss: 0.9851 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8242 - acc: 0.7648 - val_loss: 1.0398 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8295 - acc: 0.7650 - val_loss: 1.0543 - val_acc: 0.6777 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8382 - acc: 0.7597 - val_loss: 0.9912 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8361 - acc: 0.7597 - val_loss: 1.0060 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8194 - acc: 0.7645 - val_loss: 1.0140 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8298 - acc: 0.7632 - val_loss: 1.0442 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8346 - acc: 0.7621 - val_loss: 1.0130 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8253 - acc: 0.7663 - val_loss: 1.0196 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8260 - acc: 0.7623 - val_loss: 1.0181 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8292 - acc: 0.7676 - val_loss: 1.0148 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8266 - acc: 0.7674 - val_loss: 1.0096 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8278 - acc: 0.7588 - val_loss: 1.0026 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8269 - acc: 0.7630 - val_loss: 1.0625 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8221 - acc: 0.7712 - val_loss: 1.0106 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8263 - acc: 0.7668 - val_loss: 1.0086 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8289 - acc: 0.7636 - val_loss: 1.0122 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8331 - acc: 0.7643 - val_loss: 1.0335 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8345 - acc: 0.7674 - val_loss: 0.9981 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8171 - acc: 0.7714 - val_loss: 0.9964 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8393 - acc: 0.7617 - val_loss: 1.0179 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8319 - acc: 0.7597 - val_loss: 1.0116 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8308 - acc: 0.7605 - val_loss: 1.0846 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8263 - acc: 0.7652 - val_loss: 1.0118 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8262 - acc: 0.7603 - val_loss: 1.0180 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8177 - acc: 0.7716 - val_loss: 1.0478 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8233 - acc: 0.7681 - val_loss: 1.0130 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8266 - acc: 0.7681 - val_loss: 1.0379 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8265 - acc: 0.7636 - val_loss: 0.9955 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8185 - acc: 0.7623 - val_loss: 1.0245 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8396 - acc: 0.7594 - val_loss: 1.0199 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8197 - acc: 0.7676 - val_loss: 1.0711 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8266 - acc: 0.7621 - val_loss: 1.0445 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8366 - acc: 0.7625 - val_loss: 1.0316 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8219 - acc: 0.7636 - val_loss: 1.0733 - val_acc: 0.6738 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8225 - acc: 0.7628 - val_loss: 1.1088 - val_acc: 0.6816 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8313 - acc: 0.7696 - val_loss: 1.0150 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8249 - acc: 0.7670 - val_loss: 1.0297 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8283 - acc: 0.7665 - val_loss: 1.0286 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8398 - acc: 0.7559 - val_loss: 1.0247 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8267 - acc: 0.7665 - val_loss: 1.0017 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8222 - acc: 0.7696 - val_loss: 1.0319 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8215 - acc: 0.7654 - val_loss: 0.9862 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8329 - acc: 0.7577 - val_loss: 1.0532 - val_acc: 0.6835 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8211 - acc: 0.7665 - val_loss: 1.0262 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8272 - acc: 0.7601 - val_loss: 1.0463 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8303 - acc: 0.7634 - val_loss: 0.9978 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8331 - acc: 0.7588 - val_loss: 1.0226 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8263 - acc: 0.7676 - val_loss: 1.0645 - val_acc: 0.6796 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8272 - acc: 0.7630 - val_loss: 1.0026 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8205 - acc: 0.7696 - val_loss: 1.0594 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8306 - acc: 0.7656 - val_loss: 1.0330 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8266 - acc: 0.7652 - val_loss: 1.0144 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8226 - acc: 0.7650 - val_loss: 1.0017 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8215 - acc: 0.7719 - val_loss: 1.0617 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8280 - acc: 0.7663 - val_loss: 1.0380 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8281 - acc: 0.7643 - val_loss: 0.9971 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8185 - acc: 0.7743 - val_loss: 1.0149 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8240 - acc: 0.7632 - val_loss: 1.0402 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8288 - acc: 0.7634 - val_loss: 1.0177 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8286 - acc: 0.7685 - val_loss: 1.0655 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8288 - acc: 0.7663 - val_loss: 1.0391 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8283 - acc: 0.7672 - val_loss: 1.0512 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8339 - acc: 0.7585 - val_loss: 1.0575 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8225 - acc: 0.7690 - val_loss: 1.0589 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8295 - acc: 0.7625 - val_loss: 1.0408 - val_acc: 0.6816 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8181 - acc: 0.7688 - val_loss: 0.9998 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8216 - acc: 0.7652 - val_loss: 1.0141 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8244 - acc: 0.7681 - val_loss: 0.9939 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8309 - acc: 0.7636 - val_loss: 1.0193 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8087 - acc: 0.7754 - val_loss: 1.0767 - val_acc: 0.6777 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8214 - acc: 0.7681 - val_loss: 1.0315 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8287 - acc: 0.7601 - val_loss: 1.1099 - val_acc: 0.6718 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8279 - acc: 0.7599 - val_loss: 1.0279 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8319 - acc: 0.7652 - val_loss: 1.0416 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8257 - acc: 0.7608 - val_loss: 1.0443 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8273 - acc: 0.7623 - val_loss: 1.0332 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8209 - acc: 0.7723 - val_loss: 1.0208 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8200 - acc: 0.7685 - val_loss: 1.0090 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8209 - acc: 0.7661 - val_loss: 1.0185 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8244 - acc: 0.7730 - val_loss: 1.0333 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8297 - acc: 0.7636 - val_loss: 1.0294 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8269 - acc: 0.7617 - val_loss: 1.0250 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8267 - acc: 0.7696 - val_loss: 1.0416 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8264 - acc: 0.7605 - val_loss: 1.0595 - val_acc: 0.6757 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7639 - val_loss: 1.0143 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8314 - acc: 0.7561 - val_loss: 1.0266 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8198 - acc: 0.7687 - val_loss: 1.0388 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8285 - acc: 0.7665 - val_loss: 1.0114 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8277 - acc: 0.7622 - val_loss: 0.9998 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8185 - acc: 0.7723 - val_loss: 0.9817 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8258 - acc: 0.7750 - val_loss: 1.0084 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8313 - acc: 0.7559 - val_loss: 1.0336 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8336 - acc: 0.7632 - val_loss: 1.0360 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8291 - acc: 0.7581 - val_loss: 1.0325 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8300 - acc: 0.7659 - val_loss: 1.0583 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8229 - acc: 0.7690 - val_loss: 1.0424 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8253 - acc: 0.7690 - val_loss: 1.0322 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8278 - acc: 0.7672 - val_loss: 0.9918 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8242 - acc: 0.7652 - val_loss: 1.0336 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8323 - acc: 0.7579 - val_loss: 1.0972 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0425 - acc: 0.6911\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1850 - acc: 0.6475\n",
            "epsilon: 0.003 and test evaluation : 1.185017704963684, 0.6474694609642029\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.2855 - acc: 0.6091\n",
            "epsilon: 0.005 and test evaluation : 1.2855393886566162, 0.6090750694274902\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.5507 - acc: 0.5236\n",
            "epsilon: 0.01 and test evaluation : 1.5506657361984253, 0.5235602259635925\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 2.0815 - acc: 0.3805\n",
            "epsilon: 0.02 and test evaluation : 2.0815045833587646, 0.38045376539230347\n",
            "SNR: 33.75005006790161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkS-pL3EkeO",
        "colab_type": "text"
      },
      "source": [
        "# **Show Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3iEL9P9Tapn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_df[\"acc_clean_mean\"]= np.sum(result_df['acc_clean'])/3.0\n",
        "result_df[\"acc_0.003_mean\"]= np.sum(result_df['acc1'])/3.0\n",
        "result_df[\"acc_0.005_mean\"]= np.sum(result_df['acc2'])/3.0\n",
        "result_df[\"acc_0.02_mean\"]= np.sum(result_df['acc3'])/3.0\n",
        "result_df[\"acc_0.01_mean\"]= np.sum(result_df['acc4'])/3.0"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SovKQmEBBkuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "fe8a06f1-355e-49d3-e3d3-9d05fa711651"
      },
      "source": [
        "result_df.head(1)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.935317</td>\n",
              "      <td>0.713787</td>\n",
              "      <td>1.059465</td>\n",
              "      <td>0.684119</td>\n",
              "      <td>1.148184</td>\n",
              "      <td>0.65445</td>\n",
              "      <td>1.385767</td>\n",
              "      <td>0.589878</td>\n",
              "      <td>1.898678</td>\n",
              "      <td>0.443281</td>\n",
              "      <td>0.701571</td>\n",
              "      <td>0.659686</td>\n",
              "      <td>0.621291</td>\n",
              "      <td>0.550902</td>\n",
              "      <td>0.410704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    0.935317   0.713787  ...       0.550902       0.410704\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWpwrKuwmsc",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxVNgZvPRHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "class AdversarialTraining(object):\n",
        "    \"\"\"Adversarial Training  \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def train(self, pretrained_model, X_train, Y_train, X_val, y_val, X_test, y_test, epochs, BS, epsilon_list, sgd):\n",
        "        init = (32, 32,1)\n",
        "        res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "        x_train, y_train = self.data_augmentation(X_train, Y_train, BS, pretrained_model, epsilon_list)\n",
        "        x_val, y_val = self.data_augmentation(X_val, y_val, BS, pretrained_model, epsilon_list)\n",
        "        for j in range(3):\n",
        "          model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "          model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "          hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                          callbacks = [lr_scheduler],\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          validation_steps=x_val.shape[0] // BS,)\n",
        "          loss, acc = model.evaluate(X_test, y_test)\n",
        "          loss1, acc1 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "          loss2, acc2 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "          loss3, acc3 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "          loss4, acc4 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "          row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                  'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "          res_df = res_df.append(row , ignore_index=True)\n",
        "          \n",
        "        return res_df\n",
        "    def mini_batch_train(self, model, X_train,y_train, x_val, y_val, BS, pretrained_model, epsilon):\n",
        "\n",
        "\n",
        "        hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=1,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   validation_steps=x_val.shape[0] // BS, shuffle = True)\n",
        "        \n",
        "        ### TODO ###\n",
        "        ## Save hist on file.###\n",
        "\n",
        "\n",
        "    def data_augmentation(self, X_train, Y_train, batch_size, pretrained_model, epsilon_list):\n",
        "      ### divide data 16,16,16,16 for 4 different epsilons and 64 is true image. ### \n",
        "        #start_index = self.data_iteration(X_train, batch_size)\n",
        "        first_half_end = int(len(X_train)/2)\n",
        "        second_half_end = int(len(X_train))\n",
        "        x_clean = X_train[0:first_half_end,:,:,:]\n",
        "        x_adv = self.get_adversarial(X_train[first_half_end:second_half_end,:,:,:], Y_train[first_half_end:second_half_end], epsilon_list)\n",
        "        x_mix = self.merge_data(x_clean, x_adv)\n",
        "        y_mix = Y_train[0:second_half_end]\n",
        "        ### TODO###\n",
        "        # Mixture data for 4 epsilon values\n",
        "\n",
        "        return x_mix, y_mix\n",
        "\n",
        "    def data_iteration(self, X_train, batch_size):\n",
        "        N = X_train.shape[0]\n",
        "        start = np.random.randint(0, N-batch_size)\n",
        "        return start\n",
        "\n",
        "    def merge_data(self, x_clean, x_adv):\n",
        "        x_mix = []\n",
        "        for i in range(len(x_clean)):\n",
        "          x_mix.append(x_clean[i])\n",
        "        for j in range(len(x_adv)):\n",
        "          x_mix.append(x_adv[j])\n",
        "        x_mix = np.array(x_mix)\n",
        "\n",
        "        return x_mix\n",
        "\n",
        "\n",
        "    def get_adversarial(self, X_true, y_true, epsilon_list):\n",
        "\n",
        "        return self.adversarial_example(X_true, y_true, epsilon_list)\n",
        "\n",
        "    def adversarial_example(self, X_true, Y_true, epsilon_list):\n",
        "        size = len(X_true)\n",
        "        X_adv = []\n",
        "        interval = int(size/4)\n",
        "        index_list = [0,interval, interval*2, interval*3, size]\n",
        "        index = 0\n",
        "        for epsilon in epsilon_list:\n",
        "          if index == 4:\n",
        "            break\n",
        "          x_true = X_true[index_list[index]:index_list[index+1],:,:,:]\n",
        "          y_true = Y_true[index_list[index]:index_list[index+1]]\n",
        "\n",
        "          index = index + 1\n",
        "\n",
        "          for i in range(len(x_true)):\n",
        "            random_index = i\n",
        "            original_image = x_true[random_index]\n",
        "            original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "            original_label = y_true[random_index]\n",
        "            original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "            adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "            X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "          \n",
        "        X_adv = np.array(X_adv)\n",
        "        return X_adv\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5vG0s9PAmw",
        "colab_type": "text"
      },
      "source": [
        "Adversarial Training Second Wide ResNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR3373MWPvSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "adversarial_training =  AdversarialTraining()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2LxFwajOiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_model = tf.keras.Model(wrn_16_2.input, wrn_16_2.layers[-1].output)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i82EfjWHP2mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fa8684c-1a7a-4921-ab71-108290835061"
      },
      "source": [
        "result_adv_df = adversarial_training.train(logits_model, X_train, y_train,X_val, y_val, X_test, y_test, EPOCHS, BS, epsilon_list, sgd)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.5868 - acc: 0.3187 - val_loss: 1.5256 - val_acc: 0.3709 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.5064 - acc: 0.3628 - val_loss: 1.8871 - val_acc: 0.3301 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4680 - acc: 0.3764 - val_loss: 1.4993 - val_acc: 0.3903 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4647 - acc: 0.3835 - val_loss: 1.4512 - val_acc: 0.3845 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4436 - acc: 0.3979 - val_loss: 1.5496 - val_acc: 0.3806 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4333 - acc: 0.4217 - val_loss: 1.4796 - val_acc: 0.3650 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.4377 - acc: 0.4245 - val_loss: 1.4090 - val_acc: 0.4466 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4163 - acc: 0.4543 - val_loss: 1.3632 - val_acc: 0.5243 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3893 - acc: 0.4805 - val_loss: 1.3535 - val_acc: 0.5087 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.3473 - acc: 0.5160 - val_loss: 1.3041 - val_acc: 0.5573 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3459 - acc: 0.5060 - val_loss: 1.2557 - val_acc: 0.5650 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2964 - acc: 0.5342 - val_loss: 1.2516 - val_acc: 0.5728 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.2596 - acc: 0.5542 - val_loss: 1.2115 - val_acc: 0.5903 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2461 - acc: 0.5688 - val_loss: 1.2853 - val_acc: 0.5398 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.2279 - acc: 0.5704 - val_loss: 1.2406 - val_acc: 0.5903 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1941 - acc: 0.5959 - val_loss: 1.1929 - val_acc: 0.5922 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.1684 - acc: 0.6061 - val_loss: 1.2684 - val_acc: 0.5825 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1349 - acc: 0.6187 - val_loss: 1.2280 - val_acc: 0.5631 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.1278 - acc: 0.6176 - val_loss: 1.1396 - val_acc: 0.6097 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0860 - acc: 0.6462 - val_loss: 1.0886 - val_acc: 0.6680 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0734 - acc: 0.6514 - val_loss: 1.0844 - val_acc: 0.6408 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0281 - acc: 0.6764 - val_loss: 1.1690 - val_acc: 0.6350 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0565 - acc: 0.6562 - val_loss: 1.0834 - val_acc: 0.6544 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0130 - acc: 0.6833 - val_loss: 1.2916 - val_acc: 0.6019 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.0190 - acc: 0.6760 - val_loss: 1.0975 - val_acc: 0.6485 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9958 - acc: 0.6893 - val_loss: 1.0857 - val_acc: 0.6524 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9684 - acc: 0.7066 - val_loss: 1.1481 - val_acc: 0.6350 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.9607 - acc: 0.7093 - val_loss: 1.0995 - val_acc: 0.6583 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9406 - acc: 0.7142 - val_loss: 1.0517 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.9326 - acc: 0.7133 - val_loss: 1.1113 - val_acc: 0.6408 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0113 - acc: 0.6800 - val_loss: 1.0947 - val_acc: 0.6660 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9456 - acc: 0.7093 - val_loss: 1.0598 - val_acc: 0.6757 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9058 - acc: 0.7324 - val_loss: 1.0266 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8847 - acc: 0.7410 - val_loss: 1.0233 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8821 - acc: 0.7383 - val_loss: 1.0327 - val_acc: 0.6835 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8753 - acc: 0.7468 - val_loss: 1.0350 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8610 - acc: 0.7481 - val_loss: 1.0362 - val_acc: 0.6816 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8639 - acc: 0.7455 - val_loss: 1.0140 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8619 - acc: 0.7448 - val_loss: 1.0314 - val_acc: 0.6796 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8594 - acc: 0.7459 - val_loss: 0.9965 - val_acc: 0.6874 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8548 - acc: 0.7497 - val_loss: 0.9964 - val_acc: 0.6835 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8514 - acc: 0.7490 - val_loss: 0.9889 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8466 - acc: 0.7543 - val_loss: 0.9882 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8510 - acc: 0.7561 - val_loss: 1.0203 - val_acc: 0.6835 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8520 - acc: 0.7581 - val_loss: 0.9863 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8407 - acc: 0.7552 - val_loss: 0.9918 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8513 - acc: 0.7499 - val_loss: 0.9789 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8490 - acc: 0.7512 - val_loss: 0.9829 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8403 - acc: 0.7534 - val_loss: 0.9878 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8352 - acc: 0.7614 - val_loss: 1.0007 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8417 - acc: 0.7521 - val_loss: 0.9761 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8420 - acc: 0.7594 - val_loss: 1.0023 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8446 - acc: 0.7530 - val_loss: 0.9809 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8389 - acc: 0.7579 - val_loss: 0.9807 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8374 - acc: 0.7612 - val_loss: 1.0055 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8354 - acc: 0.7557 - val_loss: 0.9770 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8364 - acc: 0.7608 - val_loss: 0.9923 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8397 - acc: 0.7563 - val_loss: 0.9894 - val_acc: 0.6874 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8327 - acc: 0.7559 - val_loss: 0.9759 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8331 - acc: 0.7583 - val_loss: 0.9866 - val_acc: 0.6932 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8290 - acc: 0.7632 - val_loss: 0.9801 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8352 - acc: 0.7554 - val_loss: 1.0020 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8327 - acc: 0.7599 - val_loss: 1.0053 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7588 - val_loss: 0.9985 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8348 - acc: 0.7572 - val_loss: 1.0058 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7612 - val_loss: 0.9779 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8381 - acc: 0.7572 - val_loss: 0.9992 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8382 - acc: 0.7552 - val_loss: 1.0095 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8466 - acc: 0.7534 - val_loss: 0.9987 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7648 - val_loss: 0.9877 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8369 - acc: 0.7619 - val_loss: 0.9850 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8386 - acc: 0.7534 - val_loss: 0.9787 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8292 - acc: 0.7632 - val_loss: 1.0050 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8305 - acc: 0.7581 - val_loss: 1.0089 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8311 - acc: 0.7585 - val_loss: 0.9943 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8409 - acc: 0.7539 - val_loss: 1.0155 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8331 - acc: 0.7603 - val_loss: 0.9931 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8360 - acc: 0.7579 - val_loss: 1.0142 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8269 - acc: 0.7614 - val_loss: 0.9849 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8293 - acc: 0.7579 - val_loss: 0.9809 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8376 - acc: 0.7579 - val_loss: 0.9920 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8386 - acc: 0.7592 - val_loss: 0.9939 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8228 - acc: 0.7641 - val_loss: 0.9797 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7568 - val_loss: 1.0065 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8332 - acc: 0.7610 - val_loss: 0.9845 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7532 - val_loss: 1.0064 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8259 - acc: 0.7619 - val_loss: 0.9981 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8321 - acc: 0.7539 - val_loss: 0.9885 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8391 - acc: 0.7559 - val_loss: 0.9947 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8374 - acc: 0.7526 - val_loss: 1.0128 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8261 - acc: 0.7590 - val_loss: 1.0130 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8293 - acc: 0.7577 - val_loss: 1.0380 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8336 - acc: 0.7612 - val_loss: 0.9890 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8238 - acc: 0.7674 - val_loss: 1.0255 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8394 - acc: 0.7574 - val_loss: 0.9934 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7588 - val_loss: 0.9852 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8394 - acc: 0.7534 - val_loss: 1.0067 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8357 - acc: 0.7563 - val_loss: 0.9897 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8342 - acc: 0.7643 - val_loss: 0.9903 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8287 - acc: 0.7634 - val_loss: 0.9767 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8331 - acc: 0.7568 - val_loss: 0.9988 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8297 - acc: 0.7641 - val_loss: 0.9903 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8264 - acc: 0.7608 - val_loss: 0.9872 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8292 - acc: 0.7614 - val_loss: 0.9926 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8330 - acc: 0.7603 - val_loss: 1.0062 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8290 - acc: 0.7630 - val_loss: 0.9876 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8302 - acc: 0.7599 - val_loss: 0.9907 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8327 - acc: 0.7552 - val_loss: 0.9859 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8345 - acc: 0.7641 - val_loss: 0.9848 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8429 - acc: 0.7588 - val_loss: 0.9910 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8299 - acc: 0.7592 - val_loss: 0.9966 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8399 - acc: 0.7581 - val_loss: 0.9964 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8277 - acc: 0.7656 - val_loss: 0.9948 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8396 - acc: 0.7645 - val_loss: 1.0102 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8382 - acc: 0.7625 - val_loss: 1.0218 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8281 - acc: 0.7621 - val_loss: 0.9866 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8299 - acc: 0.7617 - val_loss: 1.0034 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8207 - acc: 0.7601 - val_loss: 1.0371 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8315 - acc: 0.7568 - val_loss: 0.9892 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8243 - acc: 0.7639 - val_loss: 0.9933 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8301 - acc: 0.7603 - val_loss: 0.9905 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8352 - acc: 0.7590 - val_loss: 0.9985 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8330 - acc: 0.7603 - val_loss: 0.9950 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8341 - acc: 0.7601 - val_loss: 0.9845 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8325 - acc: 0.7588 - val_loss: 1.0483 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8354 - acc: 0.7539 - val_loss: 0.9892 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8256 - acc: 0.7670 - val_loss: 0.9916 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8164 - acc: 0.7681 - val_loss: 1.0236 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8310 - acc: 0.7579 - val_loss: 1.0173 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8259 - acc: 0.7628 - val_loss: 0.9956 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8258 - acc: 0.7590 - val_loss: 0.9954 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8300 - acc: 0.7665 - val_loss: 0.9975 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8302 - acc: 0.7610 - val_loss: 0.9995 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8412 - acc: 0.7526 - val_loss: 0.9950 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8272 - acc: 0.7608 - val_loss: 0.9849 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8359 - acc: 0.7592 - val_loss: 0.9815 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8250 - acc: 0.7641 - val_loss: 1.0294 - val_acc: 0.6835 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8381 - acc: 0.7588 - val_loss: 0.9870 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8351 - acc: 0.7569 - val_loss: 0.9984 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8337 - acc: 0.7570 - val_loss: 0.9814 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8248 - acc: 0.7672 - val_loss: 0.9812 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8347 - acc: 0.7610 - val_loss: 1.0111 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8327 - acc: 0.7612 - val_loss: 0.9815 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8291 - acc: 0.7597 - val_loss: 0.9977 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8288 - acc: 0.7574 - val_loss: 1.0227 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8376 - acc: 0.7545 - val_loss: 0.9844 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8368 - acc: 0.7619 - val_loss: 0.9744 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8336 - acc: 0.7568 - val_loss: 0.9812 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8351 - acc: 0.7639 - val_loss: 1.0151 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8343 - acc: 0.7579 - val_loss: 0.9848 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8361 - acc: 0.7639 - val_loss: 0.9854 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8359 - acc: 0.7552 - val_loss: 1.0026 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8318 - acc: 0.7570 - val_loss: 0.9955 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8322 - acc: 0.7570 - val_loss: 0.9858 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8370 - acc: 0.7617 - val_loss: 0.9893 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8263 - acc: 0.7601 - val_loss: 1.0525 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8268 - acc: 0.7610 - val_loss: 1.0043 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8330 - acc: 0.7583 - val_loss: 1.0076 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8282 - acc: 0.7563 - val_loss: 0.9825 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8256 - acc: 0.7563 - val_loss: 0.9815 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8343 - acc: 0.7617 - val_loss: 0.9962 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8326 - acc: 0.7605 - val_loss: 0.9980 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8338 - acc: 0.7597 - val_loss: 0.9782 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8259 - acc: 0.7668 - val_loss: 0.9854 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8270 - acc: 0.7645 - val_loss: 0.9924 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8277 - acc: 0.7610 - val_loss: 0.9810 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8326 - acc: 0.7554 - val_loss: 0.9926 - val_acc: 0.6971 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8338 - acc: 0.7554 - val_loss: 0.9802 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8317 - acc: 0.7563 - val_loss: 0.9804 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8308 - acc: 0.7583 - val_loss: 0.9845 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8358 - acc: 0.7565 - val_loss: 0.9894 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8175 - acc: 0.7650 - val_loss: 0.9828 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 44ms/step - loss: 0.8293 - acc: 0.7590 - val_loss: 0.9840 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8244 - acc: 0.7617 - val_loss: 0.9817 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8292 - acc: 0.7597 - val_loss: 0.9887 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8232 - acc: 0.7645 - val_loss: 0.9878 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8309 - acc: 0.7628 - val_loss: 0.9823 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8356 - acc: 0.7585 - val_loss: 0.9871 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8263 - acc: 0.7639 - val_loss: 1.0237 - val_acc: 0.6874 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8258 - acc: 0.7574 - val_loss: 0.9984 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8309 - acc: 0.7541 - val_loss: 0.9858 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8334 - acc: 0.7608 - val_loss: 0.9958 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8380 - acc: 0.7572 - val_loss: 0.9840 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8352 - acc: 0.7577 - val_loss: 0.9768 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8349 - acc: 0.7597 - val_loss: 1.0126 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8245 - acc: 0.7594 - val_loss: 0.9875 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8334 - acc: 0.7583 - val_loss: 0.9870 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8292 - acc: 0.7621 - val_loss: 0.9979 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8284 - acc: 0.7612 - val_loss: 1.0240 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8364 - acc: 0.7597 - val_loss: 0.9850 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8237 - acc: 0.7692 - val_loss: 0.9836 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8283 - acc: 0.7608 - val_loss: 0.9835 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8365 - acc: 0.7545 - val_loss: 0.9989 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8275 - acc: 0.7645 - val_loss: 1.0113 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8273 - acc: 0.7636 - val_loss: 1.0541 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8337 - acc: 0.7579 - val_loss: 0.9887 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8369 - acc: 0.7585 - val_loss: 0.9864 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8379 - acc: 0.7570 - val_loss: 0.9779 - val_acc: 0.6893 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8344 - acc: 0.7668 - val_loss: 0.9876 - val_acc: 0.6951 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8276 - acc: 0.7636 - val_loss: 0.9888 - val_acc: 0.6854 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8800 - acc: 0.7522\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9029 - acc: 0.7435\n",
            "epsilon: 0.003 and test evaluation : 0.9028597474098206, 0.7434554696083069\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9187 - acc: 0.7365\n",
            "epsilon: 0.005 and test evaluation : 0.9187217354774475, 0.7364746928215027\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9603 - acc: 0.7243\n",
            "epsilon: 0.01 and test evaluation : 0.9602619409561157, 0.724258303642273\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0495 - acc: 0.6736\n",
            "epsilon: 0.02 and test evaluation : 1.0494650602340698, 0.6736474633216858\n",
            "SNR: 33.75005006790161\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5818 - acc: 0.3316 - val_loss: 1.5159 - val_acc: 0.3767 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4982 - acc: 0.3688 - val_loss: 1.5039 - val_acc: 0.3612 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4797 - acc: 0.3802 - val_loss: 1.4620 - val_acc: 0.4039 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4605 - acc: 0.3939 - val_loss: 1.4563 - val_acc: 0.3864 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4515 - acc: 0.4095 - val_loss: 1.4300 - val_acc: 0.4252 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4344 - acc: 0.4265 - val_loss: 1.4312 - val_acc: 0.4175 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4249 - acc: 0.4239 - val_loss: 1.3991 - val_acc: 0.4680 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4110 - acc: 0.4512 - val_loss: 1.3768 - val_acc: 0.4874 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3852 - acc: 0.4758 - val_loss: 1.4023 - val_acc: 0.4388 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3459 - acc: 0.5102 - val_loss: 1.3957 - val_acc: 0.4913 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3041 - acc: 0.5368 - val_loss: 1.3828 - val_acc: 0.4816 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2672 - acc: 0.5550 - val_loss: 1.2467 - val_acc: 0.5592 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2429 - acc: 0.5706 - val_loss: 1.2558 - val_acc: 0.5650 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.1944 - acc: 0.5981 - val_loss: 1.2848 - val_acc: 0.5301 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1873 - acc: 0.5988 - val_loss: 1.2175 - val_acc: 0.5670 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1556 - acc: 0.6187 - val_loss: 1.3705 - val_acc: 0.5515 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1410 - acc: 0.6238 - val_loss: 1.1171 - val_acc: 0.6330 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.1154 - acc: 0.6332 - val_loss: 1.1156 - val_acc: 0.6408 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0824 - acc: 0.6494 - val_loss: 1.1371 - val_acc: 0.6117 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0707 - acc: 0.6589 - val_loss: 1.1163 - val_acc: 0.6427 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0484 - acc: 0.6678 - val_loss: 1.0797 - val_acc: 0.6583 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0352 - acc: 0.6755 - val_loss: 1.4962 - val_acc: 0.5359 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0268 - acc: 0.6767 - val_loss: 1.2309 - val_acc: 0.6291 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 1.0135 - acc: 0.6760 - val_loss: 1.1178 - val_acc: 0.6388 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9820 - acc: 0.6864 - val_loss: 1.0833 - val_acc: 0.6835 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9914 - acc: 0.6946 - val_loss: 1.0612 - val_acc: 0.6524 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9711 - acc: 0.6929 - val_loss: 1.0597 - val_acc: 0.7029 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9642 - acc: 0.7020 - val_loss: 1.0877 - val_acc: 0.6777 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9552 - acc: 0.7075 - val_loss: 1.0383 - val_acc: 0.6932 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.9221 - acc: 0.7164 - val_loss: 1.1329 - val_acc: 0.6524 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 1.0560 - acc: 0.6667 - val_loss: 1.0777 - val_acc: 0.6699 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9779 - acc: 0.7006 - val_loss: 1.0308 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9292 - acc: 0.7197 - val_loss: 0.9761 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9051 - acc: 0.7317 - val_loss: 0.9835 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8947 - acc: 0.7315 - val_loss: 0.9630 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8872 - acc: 0.7350 - val_loss: 0.9633 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8826 - acc: 0.7379 - val_loss: 0.9578 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8699 - acc: 0.7432 - val_loss: 0.9580 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8715 - acc: 0.7352 - val_loss: 0.9547 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8530 - acc: 0.7441 - val_loss: 0.9437 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8547 - acc: 0.7450 - val_loss: 0.9529 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8501 - acc: 0.7519 - val_loss: 0.9523 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8478 - acc: 0.7510 - val_loss: 0.9508 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8450 - acc: 0.7543 - val_loss: 0.9568 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8457 - acc: 0.7461 - val_loss: 0.9572 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8472 - acc: 0.7506 - val_loss: 0.9717 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8464 - acc: 0.7526 - val_loss: 0.9505 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8355 - acc: 0.7577 - val_loss: 0.9515 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8297 - acc: 0.7594 - val_loss: 0.9589 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8392 - acc: 0.7519 - val_loss: 0.9621 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8386 - acc: 0.7530 - val_loss: 0.9528 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8271 - acc: 0.7645 - val_loss: 0.9577 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8357 - acc: 0.7526 - val_loss: 0.9514 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8309 - acc: 0.7541 - val_loss: 0.9651 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8356 - acc: 0.7602 - val_loss: 0.9719 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8239 - acc: 0.7608 - val_loss: 0.9624 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8222 - acc: 0.7663 - val_loss: 0.9573 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8335 - acc: 0.7561 - val_loss: 0.9724 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8308 - acc: 0.7599 - val_loss: 0.9591 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8276 - acc: 0.7625 - val_loss: 0.9540 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8340 - acc: 0.7557 - val_loss: 0.9604 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8294 - acc: 0.7554 - val_loss: 0.9579 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8237 - acc: 0.7639 - val_loss: 0.9738 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8299 - acc: 0.7552 - val_loss: 0.9660 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8178 - acc: 0.7617 - val_loss: 0.9563 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8217 - acc: 0.7612 - val_loss: 0.9591 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8278 - acc: 0.7570 - val_loss: 0.9597 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8241 - acc: 0.7694 - val_loss: 0.9486 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8232 - acc: 0.7610 - val_loss: 0.9586 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8194 - acc: 0.7706 - val_loss: 0.9714 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8267 - acc: 0.7609 - val_loss: 0.9578 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8203 - acc: 0.7645 - val_loss: 0.9594 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8216 - acc: 0.7603 - val_loss: 0.9532 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8177 - acc: 0.7650 - val_loss: 0.9629 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8284 - acc: 0.7610 - val_loss: 0.9583 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8195 - acc: 0.7641 - val_loss: 0.9561 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8194 - acc: 0.7605 - val_loss: 0.9554 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8293 - acc: 0.7583 - val_loss: 0.9650 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8193 - acc: 0.7610 - val_loss: 0.9598 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8211 - acc: 0.7617 - val_loss: 0.9598 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8365 - acc: 0.7588 - val_loss: 0.9555 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8245 - acc: 0.7588 - val_loss: 0.9672 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8298 - acc: 0.7561 - val_loss: 0.9701 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8270 - acc: 0.7625 - val_loss: 0.9751 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8311 - acc: 0.7579 - val_loss: 0.9574 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8310 - acc: 0.7572 - val_loss: 0.9631 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8222 - acc: 0.7605 - val_loss: 0.9800 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8229 - acc: 0.7661 - val_loss: 0.9587 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8216 - acc: 0.7565 - val_loss: 0.9498 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8279 - acc: 0.7590 - val_loss: 0.9824 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8302 - acc: 0.7628 - val_loss: 0.9642 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8229 - acc: 0.7621 - val_loss: 0.9728 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8183 - acc: 0.7610 - val_loss: 0.9558 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8303 - acc: 0.7612 - val_loss: 0.9619 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8265 - acc: 0.7672 - val_loss: 0.9589 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8225 - acc: 0.7597 - val_loss: 0.9510 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8209 - acc: 0.7645 - val_loss: 0.9603 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7721 - val_loss: 0.9577 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8334 - acc: 0.7601 - val_loss: 0.9721 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8218 - acc: 0.7625 - val_loss: 0.9708 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8100 - acc: 0.7699 - val_loss: 0.9552 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8189 - acc: 0.7639 - val_loss: 0.9700 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8187 - acc: 0.7590 - val_loss: 0.9686 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8302 - acc: 0.7639 - val_loss: 0.9570 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8262 - acc: 0.7619 - val_loss: 0.9514 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8246 - acc: 0.7570 - val_loss: 0.9572 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8282 - acc: 0.7659 - val_loss: 0.9501 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8227 - acc: 0.7614 - val_loss: 0.9510 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8308 - acc: 0.7545 - val_loss: 0.9694 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8222 - acc: 0.7579 - val_loss: 0.9631 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8212 - acc: 0.7674 - val_loss: 0.9539 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8291 - acc: 0.7594 - val_loss: 0.9651 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8274 - acc: 0.7623 - val_loss: 0.9520 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8258 - acc: 0.7552 - val_loss: 0.9589 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8305 - acc: 0.7552 - val_loss: 0.9618 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8138 - acc: 0.7701 - val_loss: 0.9721 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8239 - acc: 0.7625 - val_loss: 0.9560 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8219 - acc: 0.7582 - val_loss: 0.9594 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8335 - acc: 0.7590 - val_loss: 0.9603 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8174 - acc: 0.7594 - val_loss: 0.9583 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8204 - acc: 0.7654 - val_loss: 0.9681 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8267 - acc: 0.7614 - val_loss: 0.9724 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8341 - acc: 0.7592 - val_loss: 0.9707 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8187 - acc: 0.7617 - val_loss: 0.9535 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8166 - acc: 0.7670 - val_loss: 0.9688 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8259 - acc: 0.7603 - val_loss: 0.9577 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8143 - acc: 0.7632 - val_loss: 0.9557 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8199 - acc: 0.7681 - val_loss: 0.9534 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8194 - acc: 0.7652 - val_loss: 0.9755 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8283 - acc: 0.7588 - val_loss: 0.9606 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8283 - acc: 0.7583 - val_loss: 0.9646 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8149 - acc: 0.7696 - val_loss: 0.9615 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8320 - acc: 0.7557 - val_loss: 0.9634 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8261 - acc: 0.7594 - val_loss: 0.9704 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8252 - acc: 0.7630 - val_loss: 0.9845 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8252 - acc: 0.7608 - val_loss: 0.9526 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8102 - acc: 0.7747 - val_loss: 0.9522 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8242 - acc: 0.7641 - val_loss: 0.9644 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8202 - acc: 0.7641 - val_loss: 0.9610 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7579 - val_loss: 0.9585 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8152 - acc: 0.7625 - val_loss: 0.9677 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8158 - acc: 0.7688 - val_loss: 0.9602 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8152 - acc: 0.7617 - val_loss: 0.9612 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8110 - acc: 0.7714 - val_loss: 0.9562 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8210 - acc: 0.7641 - val_loss: 0.9646 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8361 - acc: 0.7526 - val_loss: 0.9841 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8292 - acc: 0.7588 - val_loss: 0.9490 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8288 - acc: 0.7601 - val_loss: 0.9582 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8209 - acc: 0.7585 - val_loss: 0.9582 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8144 - acc: 0.7630 - val_loss: 0.9673 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8316 - acc: 0.7552 - val_loss: 0.9532 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8259 - acc: 0.7565 - val_loss: 0.9682 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8161 - acc: 0.7612 - val_loss: 0.9752 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8272 - acc: 0.7628 - val_loss: 0.9619 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8251 - acc: 0.7639 - val_loss: 0.9592 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8244 - acc: 0.7650 - val_loss: 0.9574 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8245 - acc: 0.7652 - val_loss: 0.9547 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8280 - acc: 0.7585 - val_loss: 0.9559 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8266 - acc: 0.7650 - val_loss: 0.9497 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8233 - acc: 0.7568 - val_loss: 0.9747 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7669 - val_loss: 0.9660 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8283 - acc: 0.7523 - val_loss: 0.9531 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8172 - acc: 0.7652 - val_loss: 0.9564 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8150 - acc: 0.7694 - val_loss: 0.9633 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8156 - acc: 0.7674 - val_loss: 0.9537 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8216 - acc: 0.7645 - val_loss: 0.9690 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8237 - acc: 0.7601 - val_loss: 0.9577 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8228 - acc: 0.7619 - val_loss: 0.9608 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8200 - acc: 0.7656 - val_loss: 0.9620 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8248 - acc: 0.7639 - val_loss: 0.9666 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8212 - acc: 0.7643 - val_loss: 0.9512 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8246 - acc: 0.7663 - val_loss: 0.9685 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8223 - acc: 0.7634 - val_loss: 0.9555 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8321 - acc: 0.7628 - val_loss: 0.9592 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8316 - acc: 0.7574 - val_loss: 0.9637 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8251 - acc: 0.7605 - val_loss: 0.9644 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8216 - acc: 0.7594 - val_loss: 0.9689 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8226 - acc: 0.7592 - val_loss: 0.9580 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7574 - val_loss: 0.9662 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8246 - acc: 0.7579 - val_loss: 0.9706 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8212 - acc: 0.7565 - val_loss: 0.9616 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7621 - val_loss: 0.9582 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8199 - acc: 0.7681 - val_loss: 0.9537 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8136 - acc: 0.7670 - val_loss: 0.9648 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8150 - acc: 0.7628 - val_loss: 0.9551 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8257 - acc: 0.7661 - val_loss: 0.9583 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8264 - acc: 0.7648 - val_loss: 0.9587 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8243 - acc: 0.7594 - val_loss: 0.9568 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8238 - acc: 0.7634 - val_loss: 0.9677 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8219 - acc: 0.7634 - val_loss: 0.9638 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8219 - acc: 0.7628 - val_loss: 0.9530 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8206 - acc: 0.7606 - val_loss: 0.9793 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8211 - acc: 0.7641 - val_loss: 0.9548 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8232 - acc: 0.7608 - val_loss: 0.9570 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8262 - acc: 0.7592 - val_loss: 0.9547 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8243 - acc: 0.7652 - val_loss: 0.9680 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8162 - acc: 0.7592 - val_loss: 0.9689 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8201 - acc: 0.7654 - val_loss: 1.0020 - val_acc: 0.6777 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8198 - acc: 0.7681 - val_loss: 0.9569 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8203 - acc: 0.7601 - val_loss: 0.9620 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8642 - acc: 0.7574\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8846 - acc: 0.7539\n",
            "epsilon: 0.003 and test evaluation : 0.8845880031585693, 0.753926694393158\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.8988 - acc: 0.7539\n",
            "epsilon: 0.005 and test evaluation : 0.8987670540809631, 0.753926694393158\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9362 - acc: 0.7382\n",
            "epsilon: 0.01 and test evaluation : 0.9362226128578186, 0.7382199168205261\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0202 - acc: 0.7068\n",
            "epsilon: 0.02 and test evaluation : 1.0201642513275146, 0.7068063020706177\n",
            "SNR: 33.75005006790161\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.5893 - acc: 0.3116 - val_loss: 1.6188 - val_acc: 0.2544 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.5057 - acc: 0.3673 - val_loss: 1.5183 - val_acc: 0.3359 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4808 - acc: 0.3680 - val_loss: 1.4833 - val_acc: 0.3515 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4581 - acc: 0.3864 - val_loss: 1.4552 - val_acc: 0.3825 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4408 - acc: 0.4086 - val_loss: 1.4514 - val_acc: 0.3981 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.4353 - acc: 0.4075 - val_loss: 1.3982 - val_acc: 0.4427 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4423 - acc: 0.4026 - val_loss: 1.3875 - val_acc: 0.4466 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.4164 - acc: 0.4343 - val_loss: 1.3672 - val_acc: 0.4835 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3864 - acc: 0.4680 - val_loss: 1.4932 - val_acc: 0.3825 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3591 - acc: 0.4976 - val_loss: 1.4861 - val_acc: 0.4291 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.3596 - acc: 0.5018 - val_loss: 1.3169 - val_acc: 0.5165 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3149 - acc: 0.5242 - val_loss: 1.2581 - val_acc: 0.5573 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.3164 - acc: 0.5293 - val_loss: 1.7992 - val_acc: 0.4447 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2822 - acc: 0.5530 - val_loss: 1.3063 - val_acc: 0.5301 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2324 - acc: 0.5701 - val_loss: 1.2029 - val_acc: 0.6000 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.2211 - acc: 0.5861 - val_loss: 1.2560 - val_acc: 0.5670 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1885 - acc: 0.5905 - val_loss: 1.2227 - val_acc: 0.5864 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1698 - acc: 0.6132 - val_loss: 1.1647 - val_acc: 0.6019 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1516 - acc: 0.6059 - val_loss: 1.0922 - val_acc: 0.6447 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.1166 - acc: 0.6272 - val_loss: 1.0961 - val_acc: 0.6272 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.0815 - acc: 0.6391 - val_loss: 1.0713 - val_acc: 0.6233 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0930 - acc: 0.6376 - val_loss: 1.4674 - val_acc: 0.5961 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0663 - acc: 0.6536 - val_loss: 1.1012 - val_acc: 0.6252 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 1.0416 - acc: 0.6700 - val_loss: 1.2291 - val_acc: 0.5922 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 1.0185 - acc: 0.6696 - val_loss: 1.2011 - val_acc: 0.5942 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9965 - acc: 0.6895 - val_loss: 1.2629 - val_acc: 0.6233 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.9882 - acc: 0.6882 - val_loss: 1.0846 - val_acc: 0.6854 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9639 - acc: 0.7031 - val_loss: 1.0305 - val_acc: 0.6874 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.9785 - acc: 0.6911 - val_loss: 1.1074 - val_acc: 0.6602 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9440 - acc: 0.7117 - val_loss: 1.1037 - val_acc: 0.6699 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 1.0102 - acc: 0.6715 - val_loss: 0.9535 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.9362 - acc: 0.7122 - val_loss: 0.9561 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.9043 - acc: 0.7308 - val_loss: 0.9310 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8980 - acc: 0.7328 - val_loss: 0.9586 - val_acc: 0.6990 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8865 - acc: 0.7381 - val_loss: 0.9238 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8712 - acc: 0.7479 - val_loss: 0.9348 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8878 - acc: 0.7392 - val_loss: 0.9493 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8812 - acc: 0.7383 - val_loss: 0.9356 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8644 - acc: 0.7435 - val_loss: 0.9634 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8581 - acc: 0.7430 - val_loss: 0.9304 - val_acc: 0.7184 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8681 - acc: 0.7443 - val_loss: 0.9598 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8563 - acc: 0.7488 - val_loss: 0.9437 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8529 - acc: 0.7501 - val_loss: 0.9325 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8588 - acc: 0.7492 - val_loss: 0.9360 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8566 - acc: 0.7503 - val_loss: 0.9406 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8541 - acc: 0.7514 - val_loss: 0.9470 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8482 - acc: 0.7539 - val_loss: 0.9509 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8529 - acc: 0.7483 - val_loss: 0.9528 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8495 - acc: 0.7503 - val_loss: 0.9542 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8418 - acc: 0.7523 - val_loss: 0.9325 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8482 - acc: 0.7503 - val_loss: 0.9426 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8473 - acc: 0.7561 - val_loss: 0.9579 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8532 - acc: 0.7539 - val_loss: 0.9675 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8499 - acc: 0.7470 - val_loss: 0.9278 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8477 - acc: 0.7565 - val_loss: 0.9348 - val_acc: 0.7379 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8404 - acc: 0.7563 - val_loss: 0.9461 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8408 - acc: 0.7534 - val_loss: 0.9392 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8350 - acc: 0.7601 - val_loss: 0.9496 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8461 - acc: 0.7508 - val_loss: 0.9502 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8420 - acc: 0.7572 - val_loss: 0.9575 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8508 - acc: 0.7470 - val_loss: 0.9683 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8407 - acc: 0.7577 - val_loss: 0.9678 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8412 - acc: 0.7519 - val_loss: 0.9737 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8399 - acc: 0.7550 - val_loss: 0.9763 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8385 - acc: 0.7550 - val_loss: 0.9498 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8427 - acc: 0.7572 - val_loss: 0.9342 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8409 - acc: 0.7528 - val_loss: 0.9877 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8395 - acc: 0.7583 - val_loss: 0.9520 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8349 - acc: 0.7537 - val_loss: 0.9546 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8326 - acc: 0.7519 - val_loss: 0.9633 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8403 - acc: 0.7570 - val_loss: 0.9310 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8325 - acc: 0.7594 - val_loss: 0.9328 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8401 - acc: 0.7513 - val_loss: 0.9659 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8338 - acc: 0.7539 - val_loss: 0.9441 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8363 - acc: 0.7577 - val_loss: 0.9692 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8427 - acc: 0.7521 - val_loss: 1.0037 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8488 - acc: 0.7568 - val_loss: 0.9669 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8416 - acc: 0.7572 - val_loss: 0.9728 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8453 - acc: 0.7481 - val_loss: 0.9459 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8429 - acc: 0.7501 - val_loss: 0.9758 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8407 - acc: 0.7579 - val_loss: 0.9562 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8399 - acc: 0.7508 - val_loss: 0.9515 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8437 - acc: 0.7534 - val_loss: 0.9693 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8463 - acc: 0.7526 - val_loss: 0.9471 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8393 - acc: 0.7537 - val_loss: 0.9561 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8367 - acc: 0.7581 - val_loss: 0.9528 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8445 - acc: 0.7605 - val_loss: 0.9746 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8424 - acc: 0.7437 - val_loss: 0.9378 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8408 - acc: 0.7561 - val_loss: 0.9431 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8424 - acc: 0.7503 - val_loss: 0.9652 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8423 - acc: 0.7568 - val_loss: 0.9585 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8371 - acc: 0.7541 - val_loss: 0.9436 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8365 - acc: 0.7557 - val_loss: 0.9525 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8389 - acc: 0.7483 - val_loss: 0.9546 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8404 - acc: 0.7569 - val_loss: 0.9812 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8400 - acc: 0.7561 - val_loss: 0.9474 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8456 - acc: 0.7508 - val_loss: 0.9501 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8435 - acc: 0.7519 - val_loss: 0.9299 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8330 - acc: 0.7565 - val_loss: 0.9445 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8417 - acc: 0.7574 - val_loss: 0.9554 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8481 - acc: 0.7497 - val_loss: 0.9558 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8412 - acc: 0.7539 - val_loss: 0.9575 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8430 - acc: 0.7492 - val_loss: 0.9769 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8428 - acc: 0.7554 - val_loss: 0.9624 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8380 - acc: 0.7534 - val_loss: 0.9773 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8426 - acc: 0.7526 - val_loss: 0.9356 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8376 - acc: 0.7572 - val_loss: 0.9816 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8393 - acc: 0.7541 - val_loss: 0.9511 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8393 - acc: 0.7537 - val_loss: 0.9617 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8478 - acc: 0.7514 - val_loss: 0.9321 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8358 - acc: 0.7581 - val_loss: 0.9312 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8395 - acc: 0.7621 - val_loss: 0.9867 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8442 - acc: 0.7512 - val_loss: 0.9644 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8433 - acc: 0.7517 - val_loss: 0.9335 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8345 - acc: 0.7570 - val_loss: 0.9552 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8445 - acc: 0.7561 - val_loss: 0.9801 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8364 - acc: 0.7521 - val_loss: 0.9563 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8401 - acc: 0.7545 - val_loss: 0.9693 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8398 - acc: 0.7565 - val_loss: 0.9752 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8406 - acc: 0.7526 - val_loss: 0.9522 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 43ms/step - loss: 0.8439 - acc: 0.7541 - val_loss: 0.9523 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8445 - acc: 0.7474 - val_loss: 0.9508 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8358 - acc: 0.7514 - val_loss: 0.9536 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8411 - acc: 0.7501 - val_loss: 0.9505 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8487 - acc: 0.7530 - val_loss: 0.9553 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8436 - acc: 0.7543 - val_loss: 0.9523 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8386 - acc: 0.7559 - val_loss: 0.9409 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8374 - acc: 0.7541 - val_loss: 0.9387 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8335 - acc: 0.7563 - val_loss: 0.9604 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8513 - acc: 0.7521 - val_loss: 0.9500 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8470 - acc: 0.7612 - val_loss: 0.9473 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8380 - acc: 0.7557 - val_loss: 0.9290 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8289 - acc: 0.7590 - val_loss: 0.9758 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8416 - acc: 0.7552 - val_loss: 0.9544 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8479 - acc: 0.7603 - val_loss: 0.9431 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8381 - acc: 0.7565 - val_loss: 0.9657 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8451 - acc: 0.7519 - val_loss: 0.9647 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8312 - acc: 0.7570 - val_loss: 0.9364 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8332 - acc: 0.7612 - val_loss: 0.9565 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8348 - acc: 0.7579 - val_loss: 0.9922 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8470 - acc: 0.7450 - val_loss: 0.9674 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8407 - acc: 0.7537 - val_loss: 0.9340 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8418 - acc: 0.7561 - val_loss: 0.9528 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8411 - acc: 0.7548 - val_loss: 0.9452 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8360 - acc: 0.7614 - val_loss: 0.9818 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8458 - acc: 0.7508 - val_loss: 0.9494 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8463 - acc: 0.7491 - val_loss: 0.9461 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8432 - acc: 0.7568 - val_loss: 0.9545 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8383 - acc: 0.7605 - val_loss: 0.9509 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8305 - acc: 0.7619 - val_loss: 0.9482 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8400 - acc: 0.7577 - val_loss: 0.9644 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8375 - acc: 0.7550 - val_loss: 0.9647 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8409 - acc: 0.7599 - val_loss: 0.9698 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8425 - acc: 0.7568 - val_loss: 0.9401 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8373 - acc: 0.7625 - val_loss: 0.9632 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8359 - acc: 0.7510 - val_loss: 0.9446 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8364 - acc: 0.7563 - val_loss: 0.9532 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8434 - acc: 0.7548 - val_loss: 0.9473 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8380 - acc: 0.7577 - val_loss: 0.9428 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8404 - acc: 0.7545 - val_loss: 0.9468 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8467 - acc: 0.7452 - val_loss: 0.9775 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8292 - acc: 0.7543 - val_loss: 0.9978 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8294 - acc: 0.7592 - val_loss: 0.9518 - val_acc: 0.7107 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8330 - acc: 0.7550 - val_loss: 0.9391 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8302 - acc: 0.7588 - val_loss: 0.9593 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8456 - acc: 0.7557 - val_loss: 0.9354 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8459 - acc: 0.7526 - val_loss: 0.9348 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8428 - acc: 0.7579 - val_loss: 0.9592 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8353 - acc: 0.7552 - val_loss: 0.9499 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8364 - acc: 0.7559 - val_loss: 0.9527 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8284 - acc: 0.7595 - val_loss: 0.9489 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8397 - acc: 0.7581 - val_loss: 0.9734 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8385 - acc: 0.7541 - val_loss: 0.9409 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8415 - acc: 0.7512 - val_loss: 0.9536 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8336 - acc: 0.7621 - val_loss: 0.9484 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8381 - acc: 0.7528 - val_loss: 0.9584 - val_acc: 0.7146 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8458 - acc: 0.7559 - val_loss: 0.9566 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8373 - acc: 0.7617 - val_loss: 0.9512 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8370 - acc: 0.7572 - val_loss: 0.9599 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8355 - acc: 0.7541 - val_loss: 0.9434 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8427 - acc: 0.7523 - val_loss: 0.9333 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8421 - acc: 0.7554 - val_loss: 0.9500 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8381 - acc: 0.7592 - val_loss: 1.0025 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8295 - acc: 0.7610 - val_loss: 0.9472 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8370 - acc: 0.7588 - val_loss: 0.9682 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8428 - acc: 0.7537 - val_loss: 0.9713 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8473 - acc: 0.7503 - val_loss: 0.9455 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8331 - acc: 0.7550 - val_loss: 0.9432 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8483 - acc: 0.7448 - val_loss: 0.9498 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8475 - acc: 0.7452 - val_loss: 0.9434 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8449 - acc: 0.7463 - val_loss: 0.9844 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8302 - acc: 0.7619 - val_loss: 0.9631 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8360 - acc: 0.7590 - val_loss: 0.9548 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8322 - acc: 0.7605 - val_loss: 0.9765 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8342 - acc: 0.7512 - val_loss: 0.9358 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 1s 41ms/step - loss: 0.8458 - acc: 0.7526 - val_loss: 0.9434 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8343 - acc: 0.7552 - val_loss: 0.9562 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8355 - acc: 0.7554 - val_loss: 0.9475 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 1s 42ms/step - loss: 0.8396 - acc: 0.7648 - val_loss: 0.9418 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 42ms/step - loss: 0.8389 - acc: 0.7557 - val_loss: 0.9996 - val_acc: 0.6990 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.9708 - acc: 0.7103\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9917 - acc: 0.7051\n",
            "epsilon: 0.003 and test evaluation : 0.9916713237762451, 0.7050610780715942\n",
            "SNR: 50.228538513183594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0060 - acc: 0.6998\n",
            "epsilon: 0.005 and test evaluation : 1.006009817123413, 0.6998254656791687\n",
            "SNR: 45.79124450683594\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.0430 - acc: 0.6771\n",
            "epsilon: 0.01 and test evaluation : 1.0430419445037842, 0.6771378517150879\n",
            "SNR: 39.77065086364746\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 1.1229 - acc: 0.6422\n",
            "epsilon: 0.02 and test evaluation : 1.1229488849639893, 0.6422338485717773\n",
            "SNR: 33.75005006790161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBzRKS8IoCR",
        "colab_type": "text"
      },
      "source": [
        "# **Show Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KYCUfIFW6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_adv_df[\"acc_clean_mean\"]= np.sum(result_adv_df['acc_clean'])/3.0\n",
        "result_adv_df[\"acc_0.003_mean\"]= np.sum(result_adv_df['acc1'])/3.0\n",
        "result_adv_df[\"acc_0.005_mean\"]= np.sum(result_adv_df['acc2'])/3.0\n",
        "result_adv_df[\"acc_0.02_mean\"]= np.sum(result_adv_df['acc3'])/3.0\n",
        "result_adv_df[\"acc_0.01_mean\"]= np.sum(result_adv_df['acc4'])/3.0"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_mZX3HVU1G1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "881fb991-684f-4e64-bb95-af9591fdb745"
      },
      "source": [
        "result_adv_df.head(3)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.879967</td>\n",
              "      <td>0.752182</td>\n",
              "      <td>0.902860</td>\n",
              "      <td>0.743455</td>\n",
              "      <td>0.918722</td>\n",
              "      <td>0.736475</td>\n",
              "      <td>0.960262</td>\n",
              "      <td>0.724258</td>\n",
              "      <td>1.049465</td>\n",
              "      <td>0.673647</td>\n",
              "      <td>0.739965</td>\n",
              "      <td>0.734148</td>\n",
              "      <td>0.730076</td>\n",
              "      <td>0.713205</td>\n",
              "      <td>0.674229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.864215</td>\n",
              "      <td>0.757417</td>\n",
              "      <td>0.884588</td>\n",
              "      <td>0.753927</td>\n",
              "      <td>0.898767</td>\n",
              "      <td>0.753927</td>\n",
              "      <td>0.936223</td>\n",
              "      <td>0.738220</td>\n",
              "      <td>1.020164</td>\n",
              "      <td>0.706806</td>\n",
              "      <td>0.739965</td>\n",
              "      <td>0.734148</td>\n",
              "      <td>0.730076</td>\n",
              "      <td>0.713205</td>\n",
              "      <td>0.674229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.970818</td>\n",
              "      <td>0.710297</td>\n",
              "      <td>0.991671</td>\n",
              "      <td>0.705061</td>\n",
              "      <td>1.006010</td>\n",
              "      <td>0.699825</td>\n",
              "      <td>1.043042</td>\n",
              "      <td>0.677138</td>\n",
              "      <td>1.122949</td>\n",
              "      <td>0.642234</td>\n",
              "      <td>0.739965</td>\n",
              "      <td>0.734148</td>\n",
              "      <td>0.730076</td>\n",
              "      <td>0.713205</td>\n",
              "      <td>0.674229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    0.879967   0.752182  ...       0.713205       0.674229\n",
              "1    0.864215   0.757417  ...       0.713205       0.674229\n",
              "2    0.970818   0.710297  ...       0.713205       0.674229\n",
              "\n",
              "[3 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    }
  ]
}