{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Tensorflow_keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/AE_Parseval_Network/blob/master/src/notebooks/ResNet_Tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cczYDRrfFlDx",
        "colab_type": "text"
      },
      "source": [
        "# Wide ResNet 16_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWvd9YADGtMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32398829-a813-427b-8671-c7128ceb98e5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
        "import tensorflow\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tensorflow Version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqbIFJTwXLH",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRdSMgRjG8ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7a46737e-0d9e-43e8-bed5-e330541d4c7e"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0001\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "  \n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv2:channel:  {}\".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv3 channel_axis:{} \".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "if __name__ == \"__main__\":\n",
        "  init = (32, 32,1)\n",
        "  wrn_16_2 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNo5x-Ft9Fe",
        "colab_type": "text"
      },
      "source": [
        "# Data Prepare and Processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqH742XcPQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNBI_SkvuzgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euxwMe2jIoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41e6adf7-4be3-42b6-8512-b51d363b3ac9"
      },
      "source": [
        "import cv2\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNBsNVDNu6Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "474988f7-7450-4bd3-fadd-5423fe2e9792"
      },
      "source": [
        "X = new_data_X.astype('float32')\n",
        "X.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQdrnTKuM8c",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqf-dZOrvC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X[0].shape\n",
        "\n",
        "# transform data set\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEHVf2Bu9xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "y_df = pd.DataFrame(Y_data, columns=['Label'])\n",
        "y_df['Encoded'] = labelencoder.fit_transform(y_df['Label'])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdkpb2Jkqu6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_cat = to_categorical(y_df['Encoded'])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb5M1kDQnX5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, y_test = train_test_split(X, y_cat, test_size = 0.1)\n",
        "x_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kif3Li9NuSnV",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88yOqhbSwjPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_sch(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.1\n",
        "    elif epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_sch)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpiWMEgRpWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVs_QNHoEKji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import  KFold\n",
        "\n",
        "class Non_adversarial(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train_iterate(self, X_train, Y_train, X_test, y_test, epochs, BS,sgd, epsilon_list):\n",
        "          init = (32, 32,1)\n",
        "          res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                  'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                    'acc3','loss4', 'acc4'])\n",
        "          kf = KFold(n_splits=3, random_state=42, shuffle=False)\n",
        "          \n",
        "          for j, (train, val) in enumerate(kf.split(X_train)):\n",
        "            x_train, y_train,  x_val, y_val = X_train[train], Y_train[train], X_train[val], Y_train[val]\n",
        "            model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "\n",
        "            model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "            hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                            callbacks = [lr_scheduler],\n",
        "                            validation_data=(x_val, y_val),\n",
        "                            validation_steps=x_val.shape[0] // BS,)\n",
        "            loss, acc = model.evaluate(X_test, y_test)\n",
        "            loss1, acc1 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "            loss2, acc2 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "            loss3, acc3 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "            loss4, acc4 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "            row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                    'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "            res_df = res_df.append(row , ignore_index=True)\n",
        "            \n",
        "          return res_df"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFgKVWiHKYsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)\n",
        "print(\"Cleverhans Version: \" + cleverhans.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6HjbpvKslU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "def get_adversarial_examples(pretrained_model, X_true, y_true, epsilon):\n",
        "  #The attack requires the model to ouput the logits\n",
        "   \n",
        "  logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
        "  X_adv = []\n",
        "  for i in range(len(X_true)):\n",
        "    random_index = i\n",
        "    original_image = X_true[random_index]\n",
        "    original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "    original_label = y_true[random_index]\n",
        "    original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "    adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "    X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "  X_adv = np.array(X_adv)\n",
        "  return X_adv\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_quzDGwKGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_graph(hist):\n",
        "  history = hist\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"wrn_tensor.png\")\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"deneme.png\")"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbbLi83NyVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_test(model,X_adv, X_test, y_test, epsilon):\n",
        "  loss, acc = model.evaluate(X_adv,y_test)\n",
        "  print(\"epsilon: {} and test evaluation : {}, {}\".format(epsilon,loss, acc))\n",
        "  SNR = 20*np.log10(np.linalg.norm(X_test)/np.linalg.norm(X_test-X_adv))\n",
        "  print(\"SNR: {}\".format(SNR))\n",
        "  return loss, acc"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxNDNAa6MWu7",
        "colab_type": "text"
      },
      "source": [
        "**Train a Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghSgp3NvhhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 70\n",
        "BS = 128\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBnqXaiNwHGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "83088b83-9006-41b9-d2f9-d5265fcb1429"
      },
      "source": [
        "#wrn_16_2.summary()\n",
        "wrn_16_2.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")\n",
        "\n",
        "hist = wrn_16_2.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "wrn_16_2.save(\"wrn_model.h5\")\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n",
            "Epoch 1/70\n",
            "36/36 [==============================] - 2s 60ms/step - loss: 1.4775 - acc: 0.3759 - val_loss: 1.4742 - val_acc: 0.3495 - lr: 0.1000\n",
            "Epoch 2/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.4553 - acc: 0.3837 - val_loss: 1.4557 - val_acc: 0.3650 - lr: 0.1000\n",
            "Epoch 3/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4437 - acc: 0.3890 - val_loss: 1.4695 - val_acc: 0.3922 - lr: 0.1000\n",
            "Epoch 4/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4332 - acc: 0.4008 - val_loss: 1.4591 - val_acc: 0.3942 - lr: 0.1000\n",
            "Epoch 5/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4251 - acc: 0.4037 - val_loss: 1.4508 - val_acc: 0.3767 - lr: 0.1000\n",
            "Epoch 6/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4249 - acc: 0.4095 - val_loss: 1.4544 - val_acc: 0.4058 - lr: 0.1000\n",
            "Epoch 7/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4043 - acc: 0.4356 - val_loss: 1.4425 - val_acc: 0.4155 - lr: 0.1000\n",
            "Epoch 8/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4060 - acc: 0.4341 - val_loss: 1.4133 - val_acc: 0.4583 - lr: 0.1000\n",
            "Epoch 9/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3813 - acc: 0.4554 - val_loss: 1.3953 - val_acc: 0.4680 - lr: 0.1000\n",
            "Epoch 10/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3756 - acc: 0.4649 - val_loss: 1.4961 - val_acc: 0.3981 - lr: 0.1000\n",
            "Epoch 11/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3527 - acc: 0.4938 - val_loss: 1.3953 - val_acc: 0.5029 - lr: 0.1000\n",
            "Epoch 12/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.3198 - acc: 0.5144 - val_loss: 1.8671 - val_acc: 0.3631 - lr: 0.1000\n",
            "Epoch 13/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.2882 - acc: 0.5393 - val_loss: 1.3017 - val_acc: 0.5282 - lr: 0.1000\n",
            "Epoch 14/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.2431 - acc: 0.5626 - val_loss: 1.6652 - val_acc: 0.4097 - lr: 0.1000\n",
            "Epoch 15/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.2152 - acc: 0.5881 - val_loss: 1.4050 - val_acc: 0.4854 - lr: 0.1000\n",
            "Epoch 16/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1915 - acc: 0.5897 - val_loss: 1.1330 - val_acc: 0.6311 - lr: 0.1000\n",
            "Epoch 17/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.1712 - acc: 0.6001 - val_loss: 1.1770 - val_acc: 0.5922 - lr: 0.1000\n",
            "Epoch 18/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.1487 - acc: 0.6052 - val_loss: 1.1566 - val_acc: 0.6252 - lr: 0.1000\n",
            "Epoch 19/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.1154 - acc: 0.6347 - val_loss: 1.1809 - val_acc: 0.6097 - lr: 0.1000\n",
            "Epoch 20/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0975 - acc: 0.6438 - val_loss: 1.3578 - val_acc: 0.5417 - lr: 0.1000\n",
            "Epoch 21/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0839 - acc: 0.6458 - val_loss: 1.0576 - val_acc: 0.6660 - lr: 0.1000\n",
            "Epoch 22/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.0696 - acc: 0.6520 - val_loss: 1.0466 - val_acc: 0.6718 - lr: 0.1000\n",
            "Epoch 23/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0441 - acc: 0.6636 - val_loss: 1.0988 - val_acc: 0.6233 - lr: 0.1000\n",
            "Epoch 24/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.0179 - acc: 0.6747 - val_loss: 0.9518 - val_acc: 0.7184 - lr: 0.1000\n",
            "Epoch 25/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0088 - acc: 0.6842 - val_loss: 1.0024 - val_acc: 0.6951 - lr: 0.1000\n",
            "Epoch 26/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0110 - acc: 0.6738 - val_loss: 1.2635 - val_acc: 0.5845 - lr: 0.1000\n",
            "Epoch 27/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9700 - acc: 0.7006 - val_loss: 1.0301 - val_acc: 0.6738 - lr: 0.1000\n",
            "Epoch 28/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9740 - acc: 0.7015 - val_loss: 0.9793 - val_acc: 0.7184 - lr: 0.1000\n",
            "Epoch 29/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.9697 - acc: 0.7004 - val_loss: 0.9117 - val_acc: 0.7515 - lr: 0.1000\n",
            "Epoch 30/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9430 - acc: 0.7135 - val_loss: 1.0588 - val_acc: 0.6583 - lr: 0.1000\n",
            "Epoch 31/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.9526 - acc: 0.6999 - val_loss: 0.9726 - val_acc: 0.6874 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9128 - acc: 0.7213 - val_loss: 0.9400 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9014 - acc: 0.7330 - val_loss: 0.9424 - val_acc: 0.7068 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8950 - acc: 0.7361 - val_loss: 0.9381 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8819 - acc: 0.7355 - val_loss: 0.9343 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8759 - acc: 0.7430 - val_loss: 0.9117 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8749 - acc: 0.7428 - val_loss: 0.9072 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8663 - acc: 0.7441 - val_loss: 0.9014 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8643 - acc: 0.7501 - val_loss: 0.9136 - val_acc: 0.7049 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8671 - acc: 0.7457 - val_loss: 0.9096 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8662 - acc: 0.7430 - val_loss: 0.9194 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8613 - acc: 0.7492 - val_loss: 0.8965 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8590 - acc: 0.7503 - val_loss: 0.8960 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8640 - acc: 0.7466 - val_loss: 0.9063 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8515 - acc: 0.7523 - val_loss: 0.9028 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8526 - acc: 0.7550 - val_loss: 0.8880 - val_acc: 0.7282 - lr: 0.0010\n",
            "Epoch 47/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8718 - acc: 0.7417 - val_loss: 0.9022 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 48/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8709 - acc: 0.7468 - val_loss: 0.9052 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 49/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8590 - acc: 0.7503 - val_loss: 0.8900 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 50/70\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 0.8535 - acc: 0.7486 - val_loss: 0.8919 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 51/70\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 0.8493 - acc: 0.7532 - val_loss: 0.8949 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 52/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8528 - acc: 0.7557 - val_loss: 0.9037 - val_acc: 0.7029 - lr: 0.0010\n",
            "Epoch 53/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8547 - acc: 0.7510 - val_loss: 0.8964 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 54/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8453 - acc: 0.7597 - val_loss: 0.8955 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 55/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8504 - acc: 0.7468 - val_loss: 0.8893 - val_acc: 0.7476 - lr: 0.0010\n",
            "Epoch 56/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8470 - acc: 0.7543 - val_loss: 0.8951 - val_acc: 0.7165 - lr: 0.0010\n",
            "Epoch 57/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8492 - acc: 0.7532 - val_loss: 0.8863 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 58/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8514 - acc: 0.7494 - val_loss: 0.8771 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 59/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8439 - acc: 0.7534 - val_loss: 0.8785 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 60/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8452 - acc: 0.7583 - val_loss: 0.8958 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 61/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8436 - acc: 0.7532 - val_loss: 0.8985 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8337 - acc: 0.7619 - val_loss: 0.8796 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8494 - acc: 0.7528 - val_loss: 0.8942 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8472 - acc: 0.7581 - val_loss: 0.8938 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8385 - acc: 0.7603 - val_loss: 0.8869 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8373 - acc: 0.7623 - val_loss: 0.9023 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8539 - acc: 0.7483 - val_loss: 0.8863 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8529 - acc: 0.7463 - val_loss: 0.8759 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8469 - acc: 0.7565 - val_loss: 0.8905 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8453 - acc: 0.7554 - val_loss: 0.8782 - val_acc: 0.7262 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvd1NVKvpH0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "af3c3c2f-458c-47cb-c833-281165a79dc9"
      },
      "source": [
        "show_graph(hist)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3zV1f348dc7e+8wAwRkyB6G5URFBffEvaqltUutWrWtte23U3+OarVO6qxVUetCxQkiCLJk75WwMiB7557fH+fe5Ca5SW7g3twk9/18PPK4N595EsLnfc95nyHGGJRSSgWvkEAXQCmlVGBpIFBKqSCngUAppYKcBgKllApyGgiUUirIaSBQSqkgp4FAKS+JyAsi8icvj90lItOP9jpKdQQNBEopFeQ0ECilVJDTQKC6FWeTzF0iskZEykTkeRHpKSIfiUiJiHwmIslux58vIutFpFBEvhKR4W77xovISud5rwNRTe51roisdp67WETGHGGZfygi20TkkIi8JyJ9nNtFRB4RkVwRKRaRtSIyyrnvbBHZ4CzbXhG584h+YUqhgUB1T5cAZwBDgfOAj4BfA+nYv/lfAIjIUOA14DbnvnnA+yISISIRwP+Al4EU4E3ndXGeOx6YA/wISAWeBt4Tkcj2FFRETgP+CswCegO7gf86d58JnOz8ORKdxxQ49z0P/MgYEw+MAr5oz32VcqeBQHVHjxtjDhpj9gJfA0uNMauMMZXAO8B453GXAx8aYz41xtQA/w+IBo4HpgDhwKPGmBpjzFzgO7d7zAaeNsYsNcbUGWNeBKqc57XH1cAcY8xKY0wVcC8wVUQygRogHjgWEGPMRmPMfud5NcAIEUkwxhw2xqxs532VqqeBQHVHB93eV3j4Ps75vg/2EzgAxhgHkA30de7baxrPyrjb7f0A4A5ns1ChiBQC/ZzntUfTMpRiP/X3NcZ8AfwTeALIFZFnRCTBeeglwNnAbhFZICJT23lfpeppIFDBbB/2gQ7YNnnsw3wvsB/o69zm0t/tfTbwZ2NMkttXjDHmtaMsQyy2qWkvgDHmMWPMccAIbBPRXc7t3xljLgB6YJuw3mjnfZWqp4FABbM3gHNE5HQRCQfuwDbvLAaWALXAL0QkXEQuBia5nfss8GMRmexM6saKyDkiEt/OMrwG3Cgi45z5hb9gm7J2ichE5/XDgTKgEnA4cxhXi0iis0mrGHAcxe9BBTkNBCpoGWM2A9cAjwP52MTyecaYamNMNXAxcANwCJtPeNvt3OXAD7FNN4eBbc5j21uGz4D7gLewtZBjgCucuxOwAecwtvmoAHjQue9aYJeIFAM/xuYalDoiogvTKKVUcNMagVJKBTkNBEopFeQ0ECilVJDTQKCUUkEuLNAFaK+0tDSTmZkZ6GIopVSXsmLFinxjTLqnfV0uEGRmZrJ8+fJAF0MppboUEdnd0j5tGlJKqSCngUAppYKcBgKllApyXS5H4ElNTQ05OTlUVlYGuih+FxUVRUZGBuHh4YEuilKqm+gWgSAnJ4f4+HgyMzNpPFlk92KMoaCggJycHAYOHBjo4iiluolu0TRUWVlJampqtw4CACJCampqUNR8lFIdp1sEAqDbBwGXYPk5lVIdp1s0DQWNqlIICQ10KZRS3Uy3qREEUmFhIU8++WS7zzv77LMpLCz0/oSiHCje3/ZxSinVDhoIfKClQFBbW9vqefPmzSMpKcn7G5k6+6WUUj6kTUM+cM8997B9+3bGjRtHeHg4UVFRJCcns2nTJrZs2cKFF15IdnY2lZWV3HrrrcyePRtomC6jtLSUmTNncuKJJ7J48WL69u3Lu+++S3R0dOMbOeogRFckVEr5VrcLBH94fz0b9hX79Joj+iRw/3kjW9z/t7/9jXXr1rF69Wq++uorzjnnHNatW1ffxXPOnDmkpKRQUVHBxIkTueSSS0hNTW10ja1bt/Laa6/x7LPPMmvWLN566y2uueaaxjcyDvullFI+1O0CQWcwadKkRv38H3vsMd555x0AsrOz2bp1a7NAMHDgQMaNGwfAcccdx65duxpf1DgA43zVFj2llO90u0DQ2if3jhIbG1v//quvvuKzzz5jyZIlxMTEMG3aNI/jACIjI+vfh4aGUlFR0fgAh7MmoIFAKeVj+kTxgfj4eEpKSjzuKyoqIjk5mZiYGDZt2sS33357ZDdxNQkZc4SlVEopz7pdjSAQUlNTOeGEExg1ahTR0dH07Nmzft+MGTN46qmnGD58OMOGDWPKlClHdhNXbyFTB+g8Q0op3xHTxT5hZmVlmaYL02zcuJHhw4cHqEQdpLoM8rcAsLEwkuEjRgS4QEqprkREVhhjsjzt06ahrsLhPn6gawVvpVTn5rdAICJzRCRXRNa1sD9RRN4Xke9FZL2I3OivsnQL7t1Gu1gtTinVufmzRvACMKOV/T8FNhhjxgLTgIdEJMKP5enaGgUCHUuglPIdvwUCY8xC4FBrhwDxYqfTjHMe2/qcDMHMaNOQUso/Apkj+CcwHNgHrAVuNcbzR10RmS0iy0VkeV5eXkeWsfNwaNOQUso/AhkIzgJWA32AccA/RSTB04HGmGeMMVnGmKz09PSOLGPn4V4j0ECglPKhQAaCG4G3jbUN2AkcG8DyHLEjnYYa4NFHH6W8vLztAxtVljQQKKV8J5CBYA9wOoCI9ASGATsCWJ4j1uGBQJPFSikf8tvIYhF5DdsbKE1EcoD7cQ6JNcY8Bfwf8IKIrAUEuNsYk++v8viT+zTUZ5xxBj169OCNN96gqqqKiy66iD/84Q+UlZUxa9YscnJyqKur47777uPgwYPs27ePU089lbS0NL788suWb+KoAwlpmHxOKaV8xG+BwBhzZRv79wFn+vzGH90DB9b69pq9RsPMv7W4230a6vnz5zN37lyWLVuGMYbzzz+fhQsXkpeXR58+ffjwww8BOwdRYmIiDz/8MF9++SVpaWmtl8E4ICQM6qo1R6CU8ikdWexj8+fPZ/78+YwfP54JEyawadMmtm7dyujRo/n000+5++67+frrr0lMTGzfhY0DQpxzDGkgUEr5UPebdK6VT+4dwRjDvffey49+9KNm+1auXMm8efP47W9/y+mnn87vfvc77y/sqINQ12RzmiNQSvmO1gh8wH0a6rPOOos5c+ZQWloKwN69e8nNzWXfvn3ExMRwzTXXcNddd7Fy5cpm57bKOEBCnXkCrREopXyn+9UIAsB9GuqZM2dy1VVXMXXqVADi4uJ45ZVX2LZtG3fddRchISGEh4fzr3/9C4DZs2czY8YM+vTp03qy2NRBSIgGAqWUz+k01F3F/u8hJg0qC9mYc5jhx50Y6BIppboQnYa6qzPOtYpFawRKKd/TQNAVuAaQadOQUsoPuk0g6GpNXO3iCgQSgkGazESqlFJHp1sEgqioKAoKCrpvMHA++A0hFJRWE1W+L8AFUkp1J92i11BGRgY5OTl02ymq66qhJBdiHUSV7CFj0/Mw7epAl0op1U10i0AQHh7OwIEDA10M/9n1DcydBde9Bzs+hfKDgS6RUqob6RZNQ91etR2cRkQchEdDjRezlSqllJc0EHQFrkAQGQfhMRoIlFI+pYGgK6hy1QhibSCorWy8dKVSSh0FDQRdQXWZfY2Ig4gY+15rBUopH9FA0BU0yhG4AkFF4MqjlOpWNBB0BdWlEBYFoWFugaAssGVSSnUbGgi6gqpSmx8A22sItEaglPIZDQRdQXWZbRaChoCgOQKllI9oIOgKqksbAoGrRlDdQiB4/VrY8knHlEsp1S1oIOgKqkvtGAJoPVlcUwEb34Ntn3dc2ZRSXZ4Ggq6gUY6glWRxZZF9LcvtmHIppboFDQRdgXuOoLVkcX0gyO+YcimlugUNBF2Be47AVTOobqVGUKo1AqWU9zQQdAWNcgSt1AgqCu2rNg0ppdpBA0FX4DFH4KHXkKtGUHEY6mo6pmxKqS5PA0FnV1sNjpqGpqGQUAiNbCEQFDa81zyBUspLGgg6O/d5hlwiYlpPFoM2DymlvKaBoLNzX4vAJTzG84CyRoGgmy7bqZTyOb8FAhGZIyK5IrKulWOmichqEVkvIgv8VZYuzX0tApeWVilzbxoq1UCglPKOP2sELwAzWtopIknAk8D5xpiRwGV+LEvXVb8WQXzDtpZWKassgoS+9r02DSmlvOS3QGCMWQgcauWQq4C3jTF7nMfrk8uT6hL72rRpqMVA0AfConUsgVLKa4HMEQwFkkXkKxFZISLXtXSgiMwWkeUisjwvL8iaPOprBG5NQxGt5AiikiA2XXsNKaW8FshAEAYcB5wDnAXcJyJDPR1ojHnGGJNljMlKT0/vyDIGXpWHXkPhLfQaqiiEqESIS9emIaWU18ICeO8coMAYUwaUichCYCywJYBl6nw8dR8Nj2l50rnoJKjuAUU5HVM+pVSXF8gawbvAiSISJiIxwGRgYwDL0zl57D4a3bxGYIyzachVIwiyJjSl1BHzW41ARF4DpgFpIpID3A+EAxhjnjLGbBSRj4E1gAN4zhjTYlfToFVVChJi1yx2iYhtniOoLgNTZwMB2EDgcECIDhVRSrXOb4HAGHOlF8c8CDzorzJ0C9VltuuoSMM21zgCYxq2u8YQRCXaKShMnZ1zKDa148uslOpS9ONiZ1dd0rjHENhAYOoaTyznGlUclWSbhkATxkopr2gg6OyqyxrnBwDCXQvYuyWM6wNBou0+CponUEp5RQNBZ+c+BbWLpzUJGgWCHva9DipTSnkhkN1HlTfcl6l0qV+lzC1h7B4IopPte60RKKW8oDWCzq66pHkgqK8RuAUC1+pk0ck2TyChGgiUUl7RQNDZecwReFilzFUjiEywXUZj07VpSCnlFQ0EnZ3HHEELgSAiDkKdrX06qEwp5SUNBJ2dxxyBMxA0zRG4BpOB1giUUl7TQNCZORy2i2izHIGrRuDea6jQ5gZcYnvoDKRKKa9oIOjMXOMEmuUIohvvh+Y1AtcMpMb4t4xKqS5PA0Fn5mmZSmilRtCkaai2EqpK/FtGpVSXp4GgM/O0TCW0nCxuFAicg8o0YayUaoMGgs7MtUxl0xpBWKSdkbRpsjjaLUcQp9NMKKW8o4Ggs9i/BjZ/3HhbdQs5ApHGq5Q5HFBZ7LlGoD2HlFJt0EDQWXx2P7x1MzjqGra1lCOAxquUVRUDpnmOALRGoJRqkwaCzsBRBznLbVNQrtsibfXLVMY3P8d9lTL3eYZcYtPsqwYCpVQbgioQ5JdWBboInuVudH6qB7K/bdhe3UqNICK2oenIfS0Cl9BwiE7RpiGlVJuCJhB8vHonTzxwL4u2dMIHY/ZS+xoWDXuWNmxvKUcATWoEbquTuYtN18VplFJtCppAcELFl9wf8jxVr1zBgrXbPR+UvxXWv9N8PWB/y14KcT1hyPSGoAANOYLwlnIEznJ6ahoCiNPRxUqptgVNIIifciPl0//KtJBVZMw9l4WLv2nYWVUC8++DJ6fAmzfAIyPhiz93XLPKnm+h3yToNwUKd0PJAbu9utTWEkI9LBvhTSDQ+YaUUl4ImkCACDEn/oTKq94hLaSU8Z9cwpKPXoW1c+GfE2HxYzD2Crh6LvSfAgsfhEdGwbs/g5wV/puqoeSAffj3m2LvCw21gmoPM4+6eEoWu48jAGfTkCaLlVKtC7oVymKHTqP8lq858OwlTF36E1gKdT1HEzrrJfupHGDIGZC/Db59Alb/B1a9DCnHwJjLYcxlkJQJxTmQv8U2J5XlwZgrIH1o+wvkeuj3mwy9xkBYlM0TjLjA81oELhGxDU1YlUWANO9dFJduk9A1lRAe1f6yKaWCQtAFAoCYHpn0uX0Bnz17B1/kxjLv4Bn8YEsyN6TXkBAVbg9KGwznPgKn3w8b34c1r8NXf4Wv/mIf1rWVblcUWPQIjL0STrkbkgd4X5jsZfZ6vcdCWAT0mdDQc6iqtPnMoy7h0Q1NQxWFEOVckMad+zQTSf28L5NSKqgEZSAAiIqJY/qtT9NrbxF5n2/l4U+38PyinfzghIFcN3UAybER9sDoJJhwrf0q2gvr5kLJQUgbAmlDIX2YPW7RI7DsWVjzBky4Dqbd2zDNQ2v2fGsf/mHO+/WfDIsft80+1a0FgiY5gqb5AXAbVJargUAp1aKgDQQuo/om8ux1WazbW8Sjn23lkc+28NSC7Vw+sR83nTiQfikxDQcn9oUTbvV8obP+DFN/anMLK1+EPUvgpk9bbtoB+7Df/709z6XfZHA8AntX2kAQleT53PAYWytxOJyBwMNxca5pJjRPoJRqWfAki9swqm8iz12fxSe3nczZo3vz6tLdnPLgl/zsPyv5Zls+dQ4vksUJfWxz0tVvQt4meO9nrSeZ960CR01Dkhggw5mnyP62jRyB2wykbdYINBAopVoW9DWCpob1iuehWWO566xh/Pubnfxn6R4+WLOfHvGRnDe2D+eP7cOYjEREpOWLHHManP47+Oz3ttnnhF94Pm6PMxfgevgDxKZC6hCbO2g1R+C2JkFlIaQMan6Me9OQUkq1QANBC3olRnHv2cO5/YyhfL4xl3dX7+XlJbt5ftFOIsNCCA0RQkQQgYSocP500ShOHdaj4QIn3GY/8X92v00EDzql+U2yl9mHfmxq4+39J8OmD22zT2vJYrATz7XUNBQRY8/XpiGlVCs0ELQhKjyUc8b05pwxvSkqr+Hj9fvZnleGMQaHAYcxLNlewM0vLuevF49mVpYzKSsCFzwBeZth7o0we0HjhK0xtuvosWc3v2m/ybDqFfu+xXEE7jWCouZjCFx0LIFSqg1+CwQiMgc4F8g1xoxq5biJwBLgCmPMXH+VxxcSY8K5fGL/ZttLq2q55ZUV/GruGg4WVfKz0wbbpqPIeLj8VXj2VPjvVXDN2w09ifK3QsUh+9Bvqp9bzqClHIErEFQWO5PKHnIE4JxmQpuGlFIt82ey+AVgRmsHiEgo8Hdgvh/L4XdxkWE8f/1ELhrfl4c+3cJ9766jrKqWfYUVrK9OZ8PxD+PI2wxPnwzZ39mT6geSTWl+wbQhEJ1s37fUNORKFpc6p6NoKRDEpmvTkFKqVX6rERhjFopIZhuH/Rx4C5jor3J0lIiwEB66bCw9E6J4asF2Xvl2j9veGEbI/bwR+SRx/54JM/4K+1fbh33q4OYXE7E1hS0ft50sLt5vX1sLBHuWHPHPpZTq/gKWIxCRvsBFwKl0g0AAEBIi3DPzWMZmJLKzoIzkmAiSY8JJiongya/SOXHr75g/4FV6zLsTQsJg8PTmo4Fd6gNBGzmCkn32taXxBrHpUH7ILn4TEnp0P6BSqlsKZLL4UeBuY4yj1a6YgIjMBmYD9O/fvI2+s5k5unezbWMyErnu+VpOzJ7N/AnjyVz7Dxh0assXGXQKfA4k9PW839VrqMSLpiGMDQbejHRWSgWdQA4oywL+KyK7gEuBJ0XkQk8HGmOeMcZkGWOy0tO75sMsJiKM52+YyOAeicxcPZU1sxbDpB+2fELf4+DWNdCvhcqSq6ZQ7KoRtBQInF1Ty3VdAqWUZwELBMaYgcaYTGNMJjAX+Ikx5n+BKk9HSIwO58UfTKJXYhTXvJHNi99ms+VgCaal0cetTV5XXyPwIkcA2oVUKdUir5qGRORW4N9ACfAcMB64xxjTYm8fEXkNmAakiUgOcD8QDmCMeeroit11pcdH8srNk7lhzjLuf289ACmxEUzKTOGCcX08Nit51DRZ3No4AtBAoJRqkbc5gh8YY/4hImcBycC1wMu00u3TGHOlt4Uwxtzg7bHdQd+kaObffjI5hytYsqOApTsO8e2OAj5ef4DfnTuCH5w4sO2LhIRCaKQdWRwS1hAYmopJs69lBb77AZRS3Yq3gcCVzT0beNkYs17ayvCqVokI/VJi6JcSw6ysftTUOfj5f1bxxw824DCGm0/yMHdQU+HRUFdlm4Va+ueISQFEawRKqRZ5myNYISLzsYHgExGJBxz+K1bwCQ8N4fGrxjNzVC/+9OFGnvt6R9snuRLGLeUHwNYcYlJ9HwiM8d/ynUqpDuVtILgJuAeYaIwpx7b13+i3UgWp8NAQHrtyPOeM7s2fPtzIswvbCAauhHFLYwhc/DHf0HfPwT/GajBQqhvwtmloKrDaGFMmItcAE4B/+K9YwSs8NIR/XDEOBP48byM9EiK5YFxLYwmceYHWagQAsWlQ7uMcQfYyKNxt10NoadCbUqpL8LZG8C+gXETGAncA24GX/FaqIBcWGsI/Lh/HhP5J/PZ/69hbWOH5wPYEAl/XCAp321dfBxilVIfzNhDUGtvZ/QLgn8aYJ4B4/xVLhYWG8Mjl43A4DHe8sRqHpxXS6puG2goEfmgaOrzLvmogUKrL8zYQlIjIvdhuox+KSAjOMQHKfwakxnL/eSP5dschnlvkIV/gapJpaQyBS0yaXbOgtto3Basuh9KD9n35Id9cUykVMN4GgsuBKux4ggNABvCg30ql6l2WlcGZI3ry4Ceb2bCvuPFOr2sEzrEEvvr0Xug2s6oGAqW6PK8CgfPh/yqQKCLnApXGGM0RdAAR4W+XjCEpJoLbXl9FZU1dw06vcwQ+Hl3syg+ANg0p1Q14FQhEZBawDLgMmAUsFZFL/Vkw1SAlNoIHLh3DloOlPPDx5oYd9YGgre6jrhqBjyaeO+wWCCq0RqBUV+dt99HfYMcQ5AKISDrwGXayONUBTh3Wg+unDmDONzs5aUgapx7bo2GVMm/GEQCU+SoQ7IKwaAiL1BqBUt2AtzmCEFcQcCpox7nKR+49ezjH9ornjje/J7e4sv05Al82DSUP8M/4BKVUh/P2Yf6xiHwiIjeIyA3Ah8A8/xVLeRIVHso/rxpPRXUdt7+xGkeYlzmCqCQ7MZ3PagS7IWmAnbpCk8VKdXneJovvAp4Bxji/njHG3O3PginPBveI5/fnj+CbbQV8vC/Krmkc37P1k0RsF1Jf1AiMsU1DyZkQnaKBQKluwOulKo0xb2EXmlcBNiurHwu35vPzldD75u8Y31aNAJyDynxQI6g4DNUltmmougz2f3/011RKBVSrNQIRKRGRYg9fJSJS3Nq5yn9EhL9ePJreidH8/M1NlFfXtn1SbJpveg25RhQnZ9oprisO6cRzSnVxrQYCY0y8MSbBw1e8MSahowqpmkuICuehy8aSc7iCl5bsbvsEX8035AoESQNsIKittBPPKaW6LO3504VNHpTKyUPTeXrBdkqr2qgV+KppyDWYLNmZLAbtOaRUF6eBoIv75RlDOVxew78X7Wz9wNg0qC6FmhZmMvXW4V02AETGuwUCTRgr1ZVpIOjixvVLYvrwHjz79Q6KKmpaPrB+7eKjrBW4uo6C7TUEWiNQqovTQNAN3H7GUIora3m+teUtfTXfkGswGXRMjaC2Ch4dDRvf9989lApyGgi6gZF9Epk5qhdzvtnF4bIWppr2xTQTjjoozLY9hqAhEPhzvqGS/Xa2070r/HcPpYKcBoJu4vYzhlJWXcvTLa1zHOv69H4UgaB4Hzhq3JqGkgDxb9NQqbMGU3LQf/dQKshpIOgmhvaM57wxfXhx8S7yS6uaH+CLpiH3HkMAIaE2GPgzEJQ5p7gqPeC/eygV5DQQdCO3TR9CVW0dv39vPabpIK+IOAiLOrpA4D6YzMXf8w25yluigUApf9FA0I0MSo/jzrOG8cGa/Tz51fbGO+vnGzqKT++Hd4OEQGK/hm3RKR3UNKSBQCl/0UDQzdxyyjFcOK4PD36ymfnrmzw8j3Z0ceFuSOgLoW7LVXdUjaDikO1BpJTyOQ0E3YxracuxGYnc9vpqNu53mxIqNv3om4bcm4XABgJ/9hoqc1sGo1QTxkr5gwaCbigqPJRnrssiPiqMm19cToEreRybfnTNOO6DyVxiku01/TXxXKlb4NKeQ0r5hQaCbqpnQhTPXJtFfmkVP3p5BRXVdbYLaVnekT20aypsz53kpoEg1b8Tz5XlQfJA+157DinlF34LBCIyR0RyRWRdC/uvFpE1IrJWRBaLyFh/lSVYje2XxMOzxrFyz2F++NJyaqKcD+3q0vZfrHCPffXUNAT+yxOU5UKvUfa9JoyV8gt/1gheAGa0sn8ncIoxZjTwf9gV0JSPnTOmNw9cOpZvtufz79VlduORjC4+7BxD0LRpyJ/zDdXV2IVweowACdVAoJSf+C0QGGMWAi1+TDTGLDbGHHZ++y2Q4a+yBLtLj8vgrxeNZrHzOVpTnNv6CZ7UjyHw0DQE/gkEroAV1xPiemjTkFJ+0llyBDcBH7W0U0Rmi8hyEVmel+eDxVWC0BWT+nPZKRMAeObjpdTUOdp3gcLddkBaXJP1kevnGzrc/Jyj5eoxFNfD3leTxUr5RcADgYicig0Ed7d0jDHmGWNMljEmKz09veMK182cM3k0ALuz93Dzi8spa2sxG3eHd9lmIZHG22P82DTk6uoamw7xvbRpSCk/CWggEJExwHPABcYYndTe32LtmgRXj4xh0bZ8Ln9mCbklld6de3h382YhgCg/TjxX2iQQaNOQUn4RsEAgIv2Bt4FrjTFbAlWOoBIeDRFxjE2p4bnrstieW8bFTy4m5/svYefXLZ+3cyHkbYK0oc33hYY5J57zQ68hV9NQbDrE9bI5g7pWFt9RSh0Rf3YffQ1YAgwTkRwRuUlEfiwiP3Ye8jsgFXhSRFaLyHJ/lUW5cU4zceqxPXj9R1MIqy4m/p2rMS+eB98+1fz47GXwnysgdTCc+EvP1/TXfENleTYvERkP8T0BA6VHkOhWSrUqzF8XNsZc2cb+m4Gb/XV/1YKYtPreOGMyknh3wioSvytjqRnB5I/vhuIcmP5HCAmB/WvglUvtQ/i6/zWsadDsmqn+axqK7WHzEvG9ndsOQGJf399LqSAW8GSx6mCx6Q3dMkvzSFz9LFXDLuQPSX/hFceZsPhxePtmOLAWXr7Ifhq/7l3bRt8Sf803VJZbn9eo762kPYeU8jm/1QhUJxWbBvtW2feLHobaSiLPuI9XovtzzbOh7C1I5+51r8K6t23QuP49SOrf+jVjUuDAGt+XtSzPznYKDYGoZL/v76NUkNMaQbCJTbfLVRZmw3fPwbirIG0wKbER/Gf2FJhZHHEAAB8cSURBVL7peRW31/6MwuTRtjko9Zi2rxnjpxxBaV7DymqxPQDRGUiV8gMNBMEmNg0ctfDJr+33pzQM30iKieCVmyezq8/ZZB28lxWVfby7ZnSKcw4jH04853DYgOUKBKFh9r2OJVDK5zQQBBvXg3Xje5B1EyT1a7Q7ISqcF38wiZ4JUdzxxmrvBp35Y5qJykIbsOJ6NGyL76k1AqX8QANBsHElX8Nj4CTP3UETosJ5eNZYdh8q58/zNrZ9TX8EglK3MQQucb00R6CUH2ggCDaubphTbmn8abuJyYNSmX3SIP6zdA9fbGrjU7hrmglf9hxyn17CJb6X9hpSyg80EASb9GPhyv/Cyb9q89BfnjmUY3vF86u5azlUVt3ygf5Yk8B9wjmX+F52u6POd/dRSmkgCDoiMGwmhEe1eWhkWCiPXD6O4ooafv32WkxLK5v5pWnIQ40gricYx9Gtu6yUakYDgWrV8N4J3HHmUD5ef4Dnvt7pORhEJdlXn9YI8kBCGha+gYZmLe05pJRPaSBQbbr5pEFMH96TP8/byJ1vrrHrH7sLDbPBwJc1grJcOx1GiNufqGtQmfYcUsqnNBCoNoWGCE9fexy3nj6Et1flcPG/FrO7oKzxQb6eb6gsv3kyu36aCe05pJQvaSBQXgkNEW4/YyhzbpjIvsIKzn18EfPXuzXRxKS03WuoqgSWPOHdVNKluY3zA6DzDSnlJxoIVLucOqwHH/z8RDJTY5n98gpueWUFOYfLvasRLHnSjmjetajtG5V5CARhEfY+ukCNUj6lgUC1W7+UGObeMpW7zhrGV5vzmP7wAjYUhWPKWgkEtVV2biOA/K1t38RT0xA4B5VpjUApX9JAoI5IZFgoPz11MJ/fcQqnD+/Jor0Oqkry+XJTCwvHrJ3rHBsgkL+59YtXlUJNefMaAdhpJjRHoJRPaSBQR6VPUjRPXDWBs7JGEEU1t7ywiFteWcH+ooqGg4yxuYEeI6HvBMhvY2XSMg/TS7jE99ZeQ0r5mAYC5RMD+tnJ6+49JZ0vNuUy/aEFzFm0k9o6B+xcALnr7bQWacPabhpyLZzjsWnIOfGcw+Hjn0Cp4KWBQPmGc76h68cl8Ontp5CVmcIfP9jANc8vpW7xE/bT/ejLIG2IbdqpLG75WvUTzqU13xffy85K6o/1D5QKUhoIlG+4TTPRPzWGF26cyAOXjiFv1zpCt83HcdwP7LQWaUPtcQWt1ArqJ5xroUYA2nNIKR/SQKB8wxUIdi8BRx0iwqysfjx5zFKqTBh/yT3eTk+RPswe11rzkKeZR13qp5loZ55g9xKYMwOqy9o+Vqkgo4FA+UZyJvQZDwsfgH9OhJUvQckBhu3/gK09Z/Lc6jIe/nSLPS4krPWEcWkuRCXacQNNxR/h6OKtn8CeJbDrm/adp1QQ0MXrlW+ERcLNn8OmD+Drh+C9n0NYFNRWMvLie7hysYPHv9hGWlwk16cMgrxWupCW5XluFgI7jgDa3zTkqoHs+BKGntm+c5Xq5jQQKN8JCYURF8Dw82H7F/DNo5DYD+k1iv+7wEF+aTW/f389Zw3sT6+2moZaWjQnPMpOcNfepiFXDWTHV+07T6kgoE1DyvdEYPDpcP37cOGTAISFhvD4leOZ0D+Zd3NicRza0fKcQ6W5nnsMucT3al+NoK4GDu2AyATI3aDTWCvVhAYC1WGiwkN57rosDkdnEuKoYc+ODZ4PbK1pCGzPofY8zA/vsl1Ox11tv9+xwPtzlQoCGghUh0qOjeCG888C4Ik3PyK3pLLxAbXVUFnouceQS8ogyN0IFYXe3dTVLDTqErvQzY4vj6DkSnVfGghUh+t1zCgA0it3c9MLyymrqm3YWe4aVdxKIDjuBqguhZUvendDVyBIHwqDTrF5gpaW3VQqCGkgUB0vKhHienHVoErW7yvizEcW8tjnWzlQVOk2qriVpqE+42DgyfDtU7YG0Zb8rba3UVQiDDrVdj1trdeSUkHGb4FAROaISK6IrGthv4jIYyKyTUTWiMgEf5VFdUJpQ+hTm80LN05iUHosD3+6heP/9jmPvmv7+b+7rZo/fbCBX7y2imufX8ryXU0WvTn+F1CyD9a/3fa98jbbqS0ABk2zr9p7SKl6/qwRvADMaGX/TGCI82s28C8/lkV1NmlDIX8LJw9J4+WbJrPgrmn8+JRjKMzfB8BDiwt5ZeluVmcX8n12Ib9+Z62dwM5l8HToMQK+eaz1Zh5jbI3ANaI5eYDNMWggUKqe3wKBMWYh0NrahRcALxnrWyBJRHr7qzyqk0kbCpVF9dNJDEiN5VczjuW+qZEAfHDvRWz84wwW/upUHrh0DFsOlvL68uyG80Vg6s/srKbbv2j5PqW5UFXUMMcR2FrBrkXeLZmpVBAIZI6gL+D2P5sc57ZmRGS2iCwXkeV5eXkdUjjlZ+nOB7P7VBM1lYSuehkGTychIRkRAeCskb2YlJnCw/O3UFLp9vAefalt+1/8eMv3cV3f1TQENhBUl8DeFT75UZTq6rpEstgY84wxJssYk5We3kpvEtV1pHkIBGtet4vSHP+LRoeKCL89dzgFZdU8+dX2hh1hkTDlx7Y76IG1nu9THwjcagQDTwYEtms3UqUgsIFgL9DP7fsM5zYVDOL7QHgs5Dkf1A6H/WTfa4zzQd3YmIwkLh7fl+cX7ST7UHnDjuNuhIg4WPxPz/fJ32rvE9+nYVt0sp0gT/MESgGBDQTvAdc5ew9NAYqMMboYbbAICYG0wQ2f2Ld+YtcoOOFW2/7vwZ1nDSNE4IFP3Lp+RifBhOtg3Vwo8vA5In+LvU9Ikz/1Y06FnO9aXyBHqSDhz+6jrwFLgGEikiMiN4nIj0Xkx85D5gE7gG3As8BP/FUW1UmlDW2YFfSbxyCxH4y4sMXD+yRFM/ukQbz//T6+23WIzQdKeO/7fTxTNR0ctSx460nmrd3PrvwyHA5nT6L8rY2bhVwGTQNTB7t1Wmql/Db7qDHmyjb2G+Cn/rq/6gLShsLaN2HnQtizGM76K4S2/if5o1OO4bXvsrnsqSX128JChBMih5C660Ou33I8ALERodw0uRe/LNoDadc2v1C/yXaA2dcPw+Az2ryvUt2Z/vWrwHF9Uv/wTvtQnnBdm6fERobx2BXj+XprHkN7xjOsVzyD0mOJXLYD5v+GT67ty+ryFL7YlMvni77hl5F4rhGERcLZD8HbN9v1E6bd7dufTakupEv0GlLdVH3Poc2QdRNExnl12tRjUvnVjGO5cHxfhvdOIDIsFEZeBMCw/E+5fGJ//nX1cVzc3y5LuagwxfOFxlwGoy+DBX+H7O+O+sdRqqvSQKACJ2UQSAiERsDkHx3dtRL7Qv+p9VNOhIQI1w+ppo4QfvLxYVZnN56pdHdBGY9/vpUdk34PCX3g7R9CVenRlUGpLkoDgQqc8CjomwVZP7CLzRytkRfbhWdyNwEQdngbJPUnMT6em1/8jt0FZXy+8SA3/HsZ0/7fVzz06RZ+8b+dOC58Ggp3w8f3HH0ZlOqCNEegAuum+b671ogL4OO7ba2gx68hfyuh6cP49xmTuORfizntoQXUOQw9EyK59fQhJESF88cPNvBm/mguP/F2myvoPdYGpZIDULwPygsgOdOOb+g1GuJ7+q68SnUSGghUYLUwZuCIxPeEASfAurfhlLuhYBsMmsbgHnE8f30Wc77ZyXlj+jB9RE/CQ0MwxjBv7X4e/GQzM2+/k4TtX8C8O93KFmrHKZQXNGyLTbeT3aUfa6fJSD/Wfh/TQh5CqS5AA4HqXkZdAh/cBpvnQW1lfUI6KzOFrMzGD2sR4f7zRnL+E4v454Ld/Pr692HPUrsoTnxviEmzA9EqDsPB9XYaiwNr7epoq1+1i+OAzXFc8pytkSjVBWkgUN3L8PPhwztsTyDw3HXUzeiMRGYd149/f7OTKyb2Y9CQ6c0Pik6GzBPtl4sxULwX8jbBggfgzRvhkmdtIFKqi9FksepeYlPtqGHXJHSudQhacedZw4gKC+VPH270/j4ikJhh10W45i07QO2tm+H714+o2F45vAuqSvx3fRW0NBCo7mfUxfY1JtWrtvv0+Eh+cfoQvtiUy5ebc9t/v8h4uGauzU+88yNY9Wr7r1FX4/kh76iDTR/Ci+fDP8bCc9MblvNUykfEdLFFvLOysszy5csDXQzVmVUchgeHQEYW/OBjr06prnVw1qMLKa2q5ZdnDOWSCRlEhLXzc1J1Ofz3Kjur6cgLoc8E2wup91ibdPakthpWvQQLHoTSA3a+pfRjocdwO6vq6legcA8k9LWD5pbPgaT+cP37ENfKus5KNSEiK4wxWR73aSBQ3dI3j9mBYqMv9fqUdXuL+M3/1vF9diF9EqO45dTBzMrKsCOXvVVTabuwbv0MinMatqcMsjWGzJNsriG+F6ydC1/9xTb59J8Kg0+303LnbrSjreuq7TmTZsOx59r5kHYtglcvswHj+vdb785atBeWPQOHdkDqYLs4T9pQ+xqV2PrPYQyUHrRlO7zb5kOGnAm9Rnn/u/D0uzm03Qa6kHb8TpVPaCBQykvGGBZuzecfn21h5Z5CeiZEMnNUb04Zms6UQalER7TjAVaWD/u/t18539mZTiuL7L6oRPu+12g4/X6ba3DvSltXC5WFEJvW/Lr1wSDDGQyaDMbb/71dn2H92/aBnpxpB8w5au1+CYHJt8Bpv4GI2MbnVhbDl3+BlS9CTXnjfSFhcNIdcNKdEBbh3e/AGFueVa/A2jfszxyTBseeA8PPs2tPhEV6dy1/qiqBohzbPdjT7/xIlRyAeXfZgHr+Y3YdjADRQKBUOxljWLy9gOe+3sHi7QVU1TqICAthUmYKF4zrw8UTMggNaecYCEcdHFxnH+T7v4ehM+y0203XSvDGrm9sMAiLsMt1hobbbqx1VTZRHhFvJ/Gb8mPblFRXYz/Z52+BLR/bB31ifzj3ERgy3T6w178DH99rawJjLrdNa8mZ9isiDj67364i12MkXPhE84eaw+FWi9hlayKbP4KDayE00vngP8nONrvlE9v9NjIBkgdAeAyER9vXuB42QAycZpP/LrXVdnnRXYts8Bgz68hHpBfthU9/Z2tfRTl2XWuwv7dLnoVhMz2ft2OBXTcjxPn7Dg2HqCQYMLVxUDXGzqw77y7bjTkq0Y5HmXYPnHB7QGa71UCg1FGorKlj2c5DLNySx5ebc9meV8aI3gn87rwRTBmU2vYF/CX7O9v0U1thH/R11TbYDJ4Ox13fevPP7iXw/i9sYBh9mX1Ibf/CjqA+91HIOM7zeZs/gg9utwnrYTOhpgLK8uz5ZXm2DPXE5kfGX2Ob6KKTG3bVVMLOBfZ6pQdt7aOmwuZZCvc0PJh7jYH+U+y6EtlLnbUUAYwd8Dd4ur3+0Bne11K2fgpvz7ZlHXiKnacqMcOuYrfknzZIn/ZbW/tx1dJKDsBHv4IN73q+ZmikDXJDZ9gAuuABO5YlYxJc+KTtuDDvTlj3lt120VOQekzLZdwy3y7WNO5q6DvBu5+rDRoIlPIRYwzvr9nP3z/axN7CCmaM7MW9Zx/LgNTYtk/ubGoq7bQaix6BsCj78Jt4c9ufVisK4dP77Cf76JSG5pTYNFv7SMq0tYikfkfW7FNXC/tX2zWld3wFOctsjiPzJPuwHXCC7RCw6hX4/jUo2W8ftOOuggk32BXpPF63Br74E3zzKPQcDZe90PzYmgp492d2xbuRF8P5j9ta0Gd/sJ/sp91tH86OWhtI6mpt/mTrp7amdci5pnZYFJx2H0y5pXE+ZO1c+PCXtiwTb4LJP7ZByKXkoHOalHeoD3gjLrTXaunn8pIGAqV8rLKmjue+3sGTX22nsqaOUX0TmXpMKscfk8bEzGRiIrrQWM3Du+y6znHpgS5J+9XV2prMyhdt7cLU2YBx3A02KV5ZbPMSVcWw4kXI/taucz3jr7YpyhNjbLD47A+2Say6xDZVnfto65/iAfK32VxQ5oktH1u01wbS9f+zNY6RF8GUn9iayGf32wB98l12MsZlT9t8T20lTLgWTrkHEnof0a9KA4FSfnKwuJLXlu1h8bYCVmUfpqbOEB4qpMVFEh0RSkxEKDHhYfRKjOKCcX04eWg64aGde/iOMYa80irCQkKICg8hKiyUkPbmQwKh5ICd+mPFizY53lREPJz3qPc9ybbMt726Js2GsVf6dl4ssDmbpU/DypdssAEbxM59tPGn/9JcWPggLP+3DQ5nP3BEt9NAoFQHKK+u5btdh/l2RwF5JVVUVNdRXl1LeXUdW3NLOVRWTVpcBBeO68vFEzIY2jOOMA9BoabOwYGiSnJLqogMCyE6IpTocBtUEqPDEV8/kNxUVNdx59zv+XDN/kbbo8JDmD68J7dNH8rgHt4tIBQwDkdDD62oBJuQjkqEuJ4QERPo0jVXWQRr3rCDH0de3HLAObTT1lCOsOamgUCpAKuudfDV5lzeWpnDF5tyqakziEByTASpsRGkxkXgcMDewgr2F1XgaOG/ZWJ0OGMyEhnfL4mx/ZIY2SeR9PjIZj2YcosrWbmnkNXZhcREhHL98ZkkRoe3WsYDRZX88KXlrNtXxOyTB9EnMZqKmjoqquvIL63inVV7qayp46LxGdx6+hD6p3bCh6pqkQYCpTqRQ2XVfLbhIHsLK8gvraKgtJr80ipCRMhIjiYjOZq+ydH0SIiiptZR/zAuq65jW24Jq7OL2HyguD5YhAikxkXSIz6SlNgIduSVsbewAoDwUKGmzpAUE87PTh3MtVMHeBwgtzq7kNkvLaesqpZ/XDGe6SOaD1QrKK3iqQXbeWnJbuochnPG9OaEY9KYODCFzNQYv9ZU1NHTQKBUN1NeXcu6vcVsPlBMbkkVucVV5JZUUlBWTb+UGMb3S2J8/2RG9klgW24pf/94E19vzScjOZqfnzaYtLhISqtqKauq42BxJf9asJ0e8ZE8f/1EhvWKb/XeB4srefLLbby/Zj+Hymx30bS4SCYNTGZkn0RG9E5geO8EeiZEIiIUldewI7+UHXll7DlUTkFZFYfKqikoraawvIZRfRO5ekp/xvdL8hhMHA7jdY6ips5BfmkVeSVVZCTHkBLrZZfSDrTlYAmPf7GN2jpHfR4pNiKM/qkxnHBMGgP8FFQ1ECil+HprHn+dt4kN+4ub7ZsyKIUnrppAapz33T2NMWzPK2XZzsMs21nAij2HyT5UUb8/OSacEBEKyhrGFohAUnQ4KbERpMZGEhcVxtIdBZRV1zG8dwJXT+7PxMwU1u0tYlX2YVbtKWTTgRKSY8IZmBZLZmosA9NjiQoLtQGwpJK8Evvgzy2pqg9MAKEhwuSBKcwc3ZuzRvakR3xUo7JX1TqIDAvx+qF7oKiS//twAzmHK0iPiyQ93n71T4nhjOE9SYxpvenNGMMrS/fwpw82EBUeSo/4SMqdeaSy6jqqax0A9E2K5oTBqUzMTCE9PpLE6HCSYiJIjA4nISrMY17JGxoIlFKA/XT9fU4hISLERoYRGxlKTEQYCVFhPvkUWlxZw6b9JWw6UMzG/cUYA4PSYxmYFseg9Fj6Jcc0m8yvtKqWd1fv5ZVv97DRLUjFRYYxtl8io/okUlhew878MnYWlJFXUgXYZq/0uEjSE6LoEW+bxtLjI+kRH0VKbATr9hYxb91+duSVIQLHpMdRVVtHSWUtpZW11DoMoSHifNCGkxQdzrBeCVwzpT8j+zQMxjPG8L/Ve7n/3fXU1BmOG5BMfmmVbdYrq8YYiAgL4cwRPbksqx8nDk5rlrM5VFbN3W+t4dMNBzl5aDoPXTaW9PjIRvfYkV/G4m35LNqWz5LtBRRX1jb7/f7wpIH85pwRR/Rvo4FAKdXpGWNYnV3IttxSxmQkMbhHnMdpPEoqa2zeIzq8zSYjYwxbc0uZt3Y/G/YVExcZRlxUGHGRYcRGhlFRXUdhRTWHy2soLK9m5e5CKmrqmDQwhRuPz+S4Acn87t31fLz+AFkDkvl/l40lM61h8GBtnYMN+4t5a0UO736/j8LyGnomRDK0ZzyxEQ33+midbUa7e8ax/OCEgW2Wu85h2F1QRmFFDUXlNRRWVFNUXsOIPolMGnhky6JqIFBKKS8Uldfw5opsXlyyi+xDFYhAeEgId541lJtOHNTq/FJVtXV8sTGX977fx4HiSsqqbM2jpKqWjOQYHrx0DKP6tjHrqx9pIFBKqXaocxi+2JTLN9vyuWpyf4b2bD2B3hW0Fgi60Dh4pZTqGKEhwhkjenKGh2603VHnHuuulFLK7/waCERkhohsFpFtInKPh/39ReRLEVklImtE5Gx/lkcppVRzfgsEIhIKPAHMBEYAV4pI035PvwXeMMaMB64AnvRXeZRSSnnmzxrBJGCbMWaHMaYa+C9wQZNjDJDgfJ8I7PNjeZRSSnngz0DQF8h2+z7Huc3d74FrRCQHmAf83NOFRGS2iCwXkeV5eXn+KKtSSgWtQCeLrwReMMZkAGcDL4tIszIZY54xxmQZY7LS07vg4hlKKdWJ+TMQ7AX6uX2f4dzm7ibgDQBjzBIgCkjzY5mUUko14c9A8B0wREQGikgENhn8XpNj9gCnA4jIcGwg0LYfpZTqQH4dWezsDvooEArMMcb8WUT+CCw3xrzn7EX0LBCHTRz/yhgzv41r5gEe1qHzShqQf4TnBkpXK7OW17+0vP7Vncs7wBjjsW29y00xcTREZHlLQ6w7q65WZi2vf2l5/StYyxvoZLFSSqkA00CglFJBLtgCwTOBLsAR6Gpl1vL6l5bXv4KyvEGVI1BKKdVcsNUIlFJKNaGBQCmlglzQBIK2psQONBGZIyK5IrLObVuKiHwqIludr8mBLKM7EennnEJ8g4isF5Fbnds7ZZlFJEpElonI987y/sG5faCILHX+XbzuHPzYaYhIqHOa9g+c33fa8orILhFZKyKrRWS5c1un/HtwEZEkEZkrIptEZKOITO2sZRaRYc7freurWERu80V5gyIQeDkldqC9AMxosu0e4HNjzBDgc+f3nUUtcIcxZgQwBfip83faWctcBZxmjBkLjANmiMgU4O/AI8aYwcBh7LQnncmtwEa37zt7eU81xoxz69veWf8eXP4BfGyMORYYi/1dd8oyG2M2O3+344DjgHLgHXxRXmNMt/8CpgKfuH1/L3BvoMvloZyZwDq37zcDvZ3vewObA13GVsr+LnBGVygzEAOsBCZjR2WGefo7CfQXdn6uz4HTgA8A6eTl3QWkNdnWaf8esFPf78TZaaYrlNmtjGcC3/iqvEFRI8C7KbE7o57GmP3O9weATrmAqohkAuOBpXTiMjubWVYDucCnwHag0BhT6zyks/1dPAr8CnA4v0+lc5fXAPNFZIWIzHZu67R/D8BA7Nxm/3Y2vz0nIrF07jK7XAG85nx/1OUNlkDQ5Rkb7jtdX18RiQPeAm4zxhS77+tsZTbG1Blbrc7ALpx0bICL1CIRORfINcasCHRZ2uFEY8wEbBPsT0XkZPedne3vAQgDJgD/MnaVxDKaNKt0wjLjzAudD7zZdN+RljdYAoE3U2J3RgdFpDeA8zU3wOVpRETCsUHgVWPM287NnbrMAMaYQuBLbNNKkoiEOXd1pr+LE4DzRWQXdnW/07Dt2Z21vBhj9jpfc7Ft15Po3H8POUCOMWap8/u52MDQmcsMNtCuNMYcdH5/1OUNlkDgzZTYndF7wPXO99dj2+E7BRER4HlgozHmYbddnbLMIpIuIknO99HYfMZGbEC41HlYpymvMeZeY0yGMSYT+/f6hTHmajppeUUkVkTiXe+xbdjr6KR/DwDGmANAtogMc246HdhAJy6z05U0NAuBL8ob6KRHByZXzga2YNuFfxPo8ngo32vAfqAG+0nlJmyb8OfAVuAzICXQ5XQr74nYKugaYLXz6+zOWmZgDLDKWd51wO+c2wcBy4Bt2Kp2ZKDL6qHs04APOnN5neX63vm13vV/rLP+PbiVexyw3Pl38T8guTOXGYgFCoBEt21HXV6dYkIppYJcsDQNKaWUaoEGAqWUCnIaCJRSKshpIFBKqSCngUAppYKcBgKlOpCITHPNJKpUZ6GBQCmlgpwGAqU8EJFrnOsXrBaRp50T1pWKyCPO9Qw+F5F057HjRORbEVkjIu+45oMXkcEi8plzDYSVInKM8/JxbnPgv+ocpa1UwGggUKoJERkOXA6cYOwkdXXA1dhRncuNMSOBBcD9zlNeAu42xowB1rptfxV4wtg1EI7HjhwHO1Prbdi1MQZh5xVSKmDC2j5EqaBzOnbhj++cH9ajsRN5OYDXnce8ArwtIolAkjFmgXP7i8Cbznl3+hpj3gEwxlQCOK+3zBiT4/x+NXYdikX+/7GU8kwDgVLNCfCiMebeRhtF7mty3JHOz1Ll9r4O/X+oAkybhpRq7nPgUhHpAfXr7g7A/n9xzfx5FbDIGFMEHBaRk5zbrwUWGGNKgBwRudB5jUgRienQn0IpL+knEaWaMMZsEJHfYlfbCsHOCPtT7MIlk5z7crF5BLBT/z7lfNDvAG50br8WeFpE/ui8xmUd+GMo5TWdfVQpL4lIqTEmLtDlUMrXtGlIKaWCnNYIlFIqyGmNQCmlgpwGAqWUCnIaCJRSKshpIFBKqSCngUAppYLc/wczvS87TE5ErQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDHJIheU8bm",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydHkZ8pauMe",
        "colab_type": "text"
      },
      "source": [
        "**Non_Adversarial Training Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkjbXCV6KFcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae3aca78-361f-48b7-c106-d12a627cfb56"
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "train_object = Non_adversarial()\n",
        "sgd = SGD(lr=0.1, momentum=0.6)\n",
        "result_df = train_object.train_iterate(X_train, Y_train, X_test, y_test, EPOCHS, BS,sgd, epsilon_list)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 72ms/step - loss: 1.6027 - acc: 0.3087 - val_loss: 1.5699 - val_acc: 0.3564 - lr: 0.1000\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5364 - acc: 0.3611 - val_loss: 1.5548 - val_acc: 0.3244 - lr: 0.1000\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4924 - acc: 0.3711 - val_loss: 1.4825 - val_acc: 0.3722 - lr: 0.1000\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4876 - acc: 0.3768 - val_loss: 1.5030 - val_acc: 0.3559 - lr: 0.1000\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4806 - acc: 0.3677 - val_loss: 1.4903 - val_acc: 0.3663 - lr: 0.1000\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4672 - acc: 0.3741 - val_loss: 1.4591 - val_acc: 0.3675 - lr: 0.1000\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4624 - acc: 0.3919 - val_loss: 1.4491 - val_acc: 0.3990 - lr: 0.1000\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4455 - acc: 0.3919 - val_loss: 1.4735 - val_acc: 0.3920 - lr: 0.1000\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4474 - acc: 0.4001 - val_loss: 1.4525 - val_acc: 0.4153 - lr: 0.1000\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4364 - acc: 0.4116 - val_loss: 1.7140 - val_acc: 0.2691 - lr: 0.1000\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4296 - acc: 0.4228 - val_loss: 1.4293 - val_acc: 0.3995 - lr: 0.1000\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4127 - acc: 0.4377 - val_loss: 1.4441 - val_acc: 0.4362 - lr: 0.1000\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.3860 - acc: 0.4610 - val_loss: 1.4057 - val_acc: 0.4205 - lr: 0.1000\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3779 - acc: 0.4719 - val_loss: 1.3585 - val_acc: 0.4817 - lr: 0.1000\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3574 - acc: 0.4885 - val_loss: 1.4015 - val_acc: 0.4683 - lr: 0.1000\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3338 - acc: 0.5257 - val_loss: 1.4811 - val_acc: 0.4537 - lr: 0.1000\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3166 - acc: 0.5257 - val_loss: 1.3002 - val_acc: 0.5288 - lr: 0.1000\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2969 - acc: 0.5433 - val_loss: 1.2995 - val_acc: 0.5347 - lr: 0.1000\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2596 - acc: 0.5536 - val_loss: 1.8633 - val_acc: 0.3168 - lr: 0.1000\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2788 - acc: 0.5496 - val_loss: 1.2140 - val_acc: 0.5719 - lr: 0.1000\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2248 - acc: 0.5775 - val_loss: 1.2959 - val_acc: 0.5317 - lr: 0.1000\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2068 - acc: 0.5854 - val_loss: 1.1943 - val_acc: 0.5946 - lr: 0.1000\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1646 - acc: 0.6026 - val_loss: 1.5982 - val_acc: 0.4694 - lr: 0.1000\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1718 - acc: 0.6059 - val_loss: 1.2240 - val_acc: 0.5824 - lr: 0.1000\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1433 - acc: 0.6232 - val_loss: 1.2437 - val_acc: 0.5824 - lr: 0.1000\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1156 - acc: 0.6320 - val_loss: 1.2219 - val_acc: 0.6168 - lr: 0.1000\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0907 - acc: 0.6413 - val_loss: 1.2546 - val_acc: 0.5556 - lr: 0.1000\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0708 - acc: 0.6607 - val_loss: 1.1299 - val_acc: 0.6395 - lr: 0.1000\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0718 - acc: 0.6544 - val_loss: 1.3362 - val_acc: 0.5416 - lr: 0.1000\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0517 - acc: 0.6616 - val_loss: 1.2576 - val_acc: 0.5743 - lr: 0.1000\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1497 - acc: 0.6123 - val_loss: 1.1201 - val_acc: 0.6540 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0600 - acc: 0.6538 - val_loss: 1.0707 - val_acc: 0.6628 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0234 - acc: 0.6725 - val_loss: 1.0315 - val_acc: 0.6750 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0002 - acc: 0.6877 - val_loss: 1.0100 - val_acc: 0.6773 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0002 - acc: 0.6837 - val_loss: 1.0170 - val_acc: 0.6814 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9818 - acc: 0.6937 - val_loss: 1.0287 - val_acc: 0.6680 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9755 - acc: 0.6970 - val_loss: 1.0068 - val_acc: 0.6820 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9728 - acc: 0.7110 - val_loss: 1.0009 - val_acc: 0.6814 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9587 - acc: 0.7128 - val_loss: 1.0243 - val_acc: 0.6826 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9610 - acc: 0.7100 - val_loss: 0.9857 - val_acc: 0.6878 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9639 - acc: 0.7019 - val_loss: 0.9876 - val_acc: 0.6861 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9701 - acc: 0.6964 - val_loss: 1.0005 - val_acc: 0.6838 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9596 - acc: 0.7079 - val_loss: 0.9912 - val_acc: 0.6878 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9534 - acc: 0.7125 - val_loss: 0.9809 - val_acc: 0.6925 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9539 - acc: 0.7076 - val_loss: 0.9919 - val_acc: 0.6838 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9507 - acc: 0.7179 - val_loss: 0.9903 - val_acc: 0.6913 - lr: 0.0010\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9413 - acc: 0.7209 - val_loss: 1.0029 - val_acc: 0.6849 - lr: 0.0010\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9375 - acc: 0.7155 - val_loss: 0.9987 - val_acc: 0.6902 - lr: 0.0010\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9409 - acc: 0.7170 - val_loss: 0.9846 - val_acc: 0.6861 - lr: 0.0010\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9478 - acc: 0.7055 - val_loss: 0.9844 - val_acc: 0.6907 - lr: 0.0010\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9529 - acc: 0.7116 - val_loss: 0.9992 - val_acc: 0.6838 - lr: 0.0010\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9474 - acc: 0.7079 - val_loss: 1.0037 - val_acc: 0.6867 - lr: 0.0010\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9360 - acc: 0.7222 - val_loss: 1.0062 - val_acc: 0.6890 - lr: 0.0010\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9351 - acc: 0.7125 - val_loss: 0.9829 - val_acc: 0.6878 - lr: 0.0010\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9319 - acc: 0.7140 - val_loss: 0.9939 - val_acc: 0.6890 - lr: 0.0010\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9359 - acc: 0.7143 - val_loss: 1.0113 - val_acc: 0.6820 - lr: 0.0010\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9423 - acc: 0.7122 - val_loss: 1.0058 - val_acc: 0.6872 - lr: 0.0010\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9361 - acc: 0.7191 - val_loss: 0.9820 - val_acc: 0.6872 - lr: 0.0010\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9304 - acc: 0.7191 - val_loss: 0.9775 - val_acc: 0.6896 - lr: 0.0010\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9350 - acc: 0.7161 - val_loss: 0.9832 - val_acc: 0.6861 - lr: 0.0010\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 2s 59ms/step - loss: 0.9236 - acc: 0.7267 - val_loss: 0.9848 - val_acc: 0.6867 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 58ms/step - loss: 0.9377 - acc: 0.7158 - val_loss: 0.9895 - val_acc: 0.6843 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9304 - acc: 0.7258 - val_loss: 0.9841 - val_acc: 0.6867 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9284 - acc: 0.7222 - val_loss: 1.0013 - val_acc: 0.6849 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9298 - acc: 0.7191 - val_loss: 0.9830 - val_acc: 0.6872 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9414 - acc: 0.7167 - val_loss: 0.9826 - val_acc: 0.6931 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9339 - acc: 0.7203 - val_loss: 1.0045 - val_acc: 0.6855 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9315 - acc: 0.7173 - val_loss: 0.9916 - val_acc: 0.6861 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9182 - acc: 0.7228 - val_loss: 0.9962 - val_acc: 0.6855 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9295 - acc: 0.7145 - val_loss: 0.9986 - val_acc: 0.6884 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9809 - acc: 0.7068\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0720 - acc: 0.6597\n",
            "epsilon: 0.003 and test evaluation : 1.07198166847229, 0.6596858501434326\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.1358 - acc: 0.6213\n",
            "epsilon: 0.005 and test evaluation : 1.1357924938201904, 0.62129145860672\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3045 - acc: 0.5777\n",
            "epsilon: 0.01 and test evaluation : 1.3045381307601929, 0.5776614546775818\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.6682 - acc: 0.4468\n",
            "epsilon: 0.02 and test evaluation : 1.6682318449020386, 0.44677138328552246\n",
            "SNR: 33.74473333358765\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 68ms/step - loss: 1.5934 - acc: 0.3068 - val_loss: 1.6027 - val_acc: 0.2529 - lr: 0.1000\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.5184 - acc: 0.3643 - val_loss: 1.5215 - val_acc: 0.3438 - lr: 0.1000\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4810 - acc: 0.3658 - val_loss: 1.4682 - val_acc: 0.3730 - lr: 0.1000\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4669 - acc: 0.3755 - val_loss: 1.4681 - val_acc: 0.3969 - lr: 0.1000\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4549 - acc: 0.3906 - val_loss: 1.4980 - val_acc: 0.3753 - lr: 0.1000\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4462 - acc: 0.3985 - val_loss: 1.4498 - val_acc: 0.4009 - lr: 0.1000\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4221 - acc: 0.4245 - val_loss: 1.4236 - val_acc: 0.4254 - lr: 0.1000\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4175 - acc: 0.4218 - val_loss: 1.4201 - val_acc: 0.4114 - lr: 0.1000\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 1s 58ms/step - loss: 1.4003 - acc: 0.4516 - val_loss: 1.4303 - val_acc: 0.3957 - lr: 0.1000\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3842 - acc: 0.4678 - val_loss: 1.6018 - val_acc: 0.3380 - lr: 0.1000\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3636 - acc: 0.4911 - val_loss: 1.3352 - val_acc: 0.4953 - lr: 0.1000\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3419 - acc: 0.5047 - val_loss: 1.4696 - val_acc: 0.4231 - lr: 0.1000\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3296 - acc: 0.5132 - val_loss: 1.4770 - val_acc: 0.4569 - lr: 0.1000\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3010 - acc: 0.5416 - val_loss: 1.3197 - val_acc: 0.5490 - lr: 0.1000\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2907 - acc: 0.5510 - val_loss: 1.3572 - val_acc: 0.5425 - lr: 0.1000\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2703 - acc: 0.5501 - val_loss: 1.3253 - val_acc: 0.5070 - lr: 0.1000\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2574 - acc: 0.5567 - val_loss: 1.3412 - val_acc: 0.4959 - lr: 0.1000\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 2s 59ms/step - loss: 1.2255 - acc: 0.5764 - val_loss: 1.3348 - val_acc: 0.5309 - lr: 0.1000\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2259 - acc: 0.5622 - val_loss: 1.2316 - val_acc: 0.5921 - lr: 0.1000\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2064 - acc: 0.5779 - val_loss: 1.2793 - val_acc: 0.5659 - lr: 0.1000\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1833 - acc: 0.5979 - val_loss: 1.2409 - val_acc: 0.5915 - lr: 0.1000\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 2s 59ms/step - loss: 1.1513 - acc: 0.6061 - val_loss: 1.1442 - val_acc: 0.6066 - lr: 0.1000\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1451 - acc: 0.6121 - val_loss: 1.1029 - val_acc: 0.6218 - lr: 0.1000\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1145 - acc: 0.6266 - val_loss: 1.6012 - val_acc: 0.4656 - lr: 0.1000\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1100 - acc: 0.6263 - val_loss: 1.1237 - val_acc: 0.6212 - lr: 0.1000\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0631 - acc: 0.6563 - val_loss: 1.2034 - val_acc: 0.6346 - lr: 0.1000\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0631 - acc: 0.6536 - val_loss: 1.2154 - val_acc: 0.5962 - lr: 0.1000\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0656 - acc: 0.6505 - val_loss: 1.2060 - val_acc: 0.6026 - lr: 0.1000\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0437 - acc: 0.6666 - val_loss: 1.1149 - val_acc: 0.6562 - lr: 0.1000\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0300 - acc: 0.6708 - val_loss: 1.1498 - val_acc: 0.6434 - lr: 0.1000\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0237 - acc: 0.6726 - val_loss: 1.0920 - val_acc: 0.6608 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0123 - acc: 0.6669 - val_loss: 1.0698 - val_acc: 0.6690 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9979 - acc: 0.6756 - val_loss: 1.0097 - val_acc: 0.6824 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9758 - acc: 0.6965 - val_loss: 0.9929 - val_acc: 0.6917 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9659 - acc: 0.6962 - val_loss: 1.0744 - val_acc: 0.6620 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9686 - acc: 0.6980 - val_loss: 1.0214 - val_acc: 0.6888 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9557 - acc: 0.7074 - val_loss: 0.9783 - val_acc: 0.6993 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9507 - acc: 0.7123 - val_loss: 0.9775 - val_acc: 0.7022 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9495 - acc: 0.7053 - val_loss: 1.0394 - val_acc: 0.6731 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9489 - acc: 0.7095 - val_loss: 0.9813 - val_acc: 0.7051 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9475 - acc: 0.7126 - val_loss: 0.9709 - val_acc: 0.7069 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9450 - acc: 0.7116 - val_loss: 0.9585 - val_acc: 0.7075 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9295 - acc: 0.7216 - val_loss: 0.9646 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9386 - acc: 0.7162 - val_loss: 0.9709 - val_acc: 0.7045 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9505 - acc: 0.7056 - val_loss: 0.9619 - val_acc: 0.7045 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9369 - acc: 0.7171 - val_loss: 0.9590 - val_acc: 0.7034 - lr: 0.0010\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9414 - acc: 0.7092 - val_loss: 1.0045 - val_acc: 0.6929 - lr: 0.0010\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9359 - acc: 0.7156 - val_loss: 0.9973 - val_acc: 0.6859 - lr: 0.0010\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9447 - acc: 0.7110 - val_loss: 0.9718 - val_acc: 0.7028 - lr: 0.0010\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9343 - acc: 0.7213 - val_loss: 0.9684 - val_acc: 0.7098 - lr: 0.0010\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9228 - acc: 0.7280 - val_loss: 0.9667 - val_acc: 0.6981 - lr: 0.0010\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9322 - acc: 0.7135 - val_loss: 0.9779 - val_acc: 0.7057 - lr: 0.0010\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9344 - acc: 0.7133 - val_loss: 0.9685 - val_acc: 0.7086 - lr: 0.0010\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9294 - acc: 0.7165 - val_loss: 0.9591 - val_acc: 0.7034 - lr: 0.0010\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9347 - acc: 0.7168 - val_loss: 0.9642 - val_acc: 0.6999 - lr: 0.0010\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9269 - acc: 0.7168 - val_loss: 0.9715 - val_acc: 0.7086 - lr: 0.0010\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9304 - acc: 0.7168 - val_loss: 0.9647 - val_acc: 0.7075 - lr: 0.0010\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9271 - acc: 0.7204 - val_loss: 0.9500 - val_acc: 0.7086 - lr: 0.0010\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9220 - acc: 0.7168 - val_loss: 0.9512 - val_acc: 0.7069 - lr: 0.0010\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9218 - acc: 0.7244 - val_loss: 0.9624 - val_acc: 0.7086 - lr: 0.0010\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9332 - acc: 0.7177 - val_loss: 0.9753 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9255 - acc: 0.7180 - val_loss: 0.9832 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9258 - acc: 0.7104 - val_loss: 0.9526 - val_acc: 0.7139 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9193 - acc: 0.7195 - val_loss: 0.9628 - val_acc: 0.7104 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9221 - acc: 0.7268 - val_loss: 0.9807 - val_acc: 0.7010 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9204 - acc: 0.7204 - val_loss: 0.9729 - val_acc: 0.7104 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9215 - acc: 0.7189 - val_loss: 0.9610 - val_acc: 0.7069 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9256 - acc: 0.7213 - val_loss: 0.9651 - val_acc: 0.7022 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9247 - acc: 0.7201 - val_loss: 0.9829 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9197 - acc: 0.7256 - val_loss: 0.9898 - val_acc: 0.6935 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0058 - acc: 0.6928\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1039 - acc: 0.6632\n",
            "epsilon: 0.003 and test evaluation : 1.103851318359375, 0.6631762385368347\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1724 - acc: 0.6387\n",
            "epsilon: 0.005 and test evaluation : 1.1724457740783691, 0.6387434601783752\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3530 - acc: 0.5550\n",
            "epsilon: 0.01 and test evaluation : 1.3529943227767944, 0.554973840713501\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.7418 - acc: 0.4538\n",
            "epsilon: 0.02 and test evaluation : 1.7418243885040283, 0.45375218987464905\n",
            "SNR: 33.74473333358765\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 68ms/step - loss: 1.6026 - acc: 0.3077 - val_loss: 1.5915 - val_acc: 0.2890 - lr: 0.1000\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5404 - acc: 0.3516 - val_loss: 1.5201 - val_acc: 0.3520 - lr: 0.1000\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.5139 - acc: 0.3552 - val_loss: 1.5003 - val_acc: 0.3677 - lr: 0.1000\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4894 - acc: 0.3682 - val_loss: 1.4615 - val_acc: 0.3648 - lr: 0.1000\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4704 - acc: 0.3694 - val_loss: 1.4401 - val_acc: 0.4114 - lr: 0.1000\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4555 - acc: 0.3961 - val_loss: 1.5411 - val_acc: 0.3386 - lr: 0.1000\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4468 - acc: 0.4091 - val_loss: 1.4390 - val_acc: 0.4196 - lr: 0.1000\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4409 - acc: 0.3955 - val_loss: 1.4322 - val_acc: 0.4103 - lr: 0.1000\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.4233 - acc: 0.4175 - val_loss: 1.3961 - val_acc: 0.4627 - lr: 0.1000\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4037 - acc: 0.4339 - val_loss: 1.7040 - val_acc: 0.3397 - lr: 0.1000\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3808 - acc: 0.4738 - val_loss: 1.3710 - val_acc: 0.4936 - lr: 0.1000\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3611 - acc: 0.4859 - val_loss: 1.4339 - val_acc: 0.4703 - lr: 0.1000\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3286 - acc: 0.5228 - val_loss: 1.3082 - val_acc: 0.5256 - lr: 0.1000\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3261 - acc: 0.5313 - val_loss: 1.3047 - val_acc: 0.5443 - lr: 0.1000\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2817 - acc: 0.5458 - val_loss: 1.8218 - val_acc: 0.3934 - lr: 0.1000\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2740 - acc: 0.5573 - val_loss: 1.6915 - val_acc: 0.4207 - lr: 0.1000\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2347 - acc: 0.5743 - val_loss: 1.2952 - val_acc: 0.5332 - lr: 0.1000\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.2348 - acc: 0.5752 - val_loss: 1.2194 - val_acc: 0.5874 - lr: 0.1000\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1909 - acc: 0.5915 - val_loss: 1.4106 - val_acc: 0.5099 - lr: 0.1000\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1864 - acc: 0.5991 - val_loss: 1.1954 - val_acc: 0.5897 - lr: 0.1000\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1606 - acc: 0.6151 - val_loss: 1.2413 - val_acc: 0.5717 - lr: 0.1000\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1402 - acc: 0.6160 - val_loss: 1.2638 - val_acc: 0.5793 - lr: 0.1000\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1339 - acc: 0.6248 - val_loss: 1.2168 - val_acc: 0.6154 - lr: 0.1000\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 2s 59ms/step - loss: 1.0942 - acc: 0.6399 - val_loss: 1.1729 - val_acc: 0.6282 - lr: 0.1000\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0883 - acc: 0.6457 - val_loss: 1.3584 - val_acc: 0.5962 - lr: 0.1000\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0824 - acc: 0.6442 - val_loss: 1.0747 - val_acc: 0.6538 - lr: 0.1000\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0689 - acc: 0.6566 - val_loss: 1.0458 - val_acc: 0.6766 - lr: 0.1000\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0532 - acc: 0.6578 - val_loss: 1.0891 - val_acc: 0.6568 - lr: 0.1000\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0417 - acc: 0.6708 - val_loss: 1.1209 - val_acc: 0.6358 - lr: 0.1000\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0396 - acc: 0.6690 - val_loss: 1.0374 - val_acc: 0.6638 - lr: 0.1000\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0461 - acc: 0.6675 - val_loss: 1.0505 - val_acc: 0.6474 - lr: 0.0010\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0027 - acc: 0.6962 - val_loss: 1.0338 - val_acc: 0.6608 - lr: 0.0010\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9818 - acc: 0.6965 - val_loss: 1.0405 - val_acc: 0.6667 - lr: 0.0010\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9819 - acc: 0.6932 - val_loss: 1.0176 - val_acc: 0.6748 - lr: 0.0010\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9749 - acc: 0.6950 - val_loss: 1.0278 - val_acc: 0.6626 - lr: 0.0010\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9626 - acc: 0.7017 - val_loss: 1.0142 - val_acc: 0.6725 - lr: 0.0010\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9708 - acc: 0.7004 - val_loss: 1.0345 - val_acc: 0.6655 - lr: 0.0010\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9588 - acc: 0.7011 - val_loss: 1.0308 - val_acc: 0.6725 - lr: 0.0010\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9555 - acc: 0.6992 - val_loss: 1.0018 - val_acc: 0.6830 - lr: 0.0010\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9588 - acc: 0.7071 - val_loss: 1.0078 - val_acc: 0.6801 - lr: 0.0010\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9534 - acc: 0.7059 - val_loss: 0.9899 - val_acc: 0.6900 - lr: 0.0010\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9389 - acc: 0.7123 - val_loss: 0.9992 - val_acc: 0.6900 - lr: 0.0010\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9381 - acc: 0.7126 - val_loss: 0.9911 - val_acc: 0.6900 - lr: 0.0010\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9405 - acc: 0.7077 - val_loss: 1.0200 - val_acc: 0.6818 - lr: 0.0010\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9448 - acc: 0.7083 - val_loss: 1.0118 - val_acc: 0.6865 - lr: 0.0010\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9339 - acc: 0.7147 - val_loss: 0.9789 - val_acc: 0.6993 - lr: 0.0010\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9349 - acc: 0.7171 - val_loss: 0.9907 - val_acc: 0.6958 - lr: 0.0010\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9322 - acc: 0.7213 - val_loss: 0.9835 - val_acc: 0.6964 - lr: 0.0010\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9362 - acc: 0.7150 - val_loss: 0.9893 - val_acc: 0.6970 - lr: 0.0010\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9303 - acc: 0.7198 - val_loss: 0.9811 - val_acc: 0.6993 - lr: 0.0010\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9300 - acc: 0.7162 - val_loss: 0.9750 - val_acc: 0.6958 - lr: 0.0010\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9299 - acc: 0.7194 - val_loss: 0.9811 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9394 - acc: 0.7089 - val_loss: 1.0243 - val_acc: 0.6853 - lr: 0.0010\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9226 - acc: 0.7195 - val_loss: 0.9796 - val_acc: 0.7010 - lr: 0.0010\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9359 - acc: 0.7138 - val_loss: 1.0214 - val_acc: 0.6801 - lr: 0.0010\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9277 - acc: 0.7186 - val_loss: 0.9698 - val_acc: 0.7005 - lr: 0.0010\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9192 - acc: 0.7215 - val_loss: 0.9964 - val_acc: 0.6987 - lr: 0.0010\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9183 - acc: 0.7283 - val_loss: 0.9799 - val_acc: 0.7016 - lr: 0.0010\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9182 - acc: 0.7289 - val_loss: 0.9787 - val_acc: 0.7040 - lr: 0.0010\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9148 - acc: 0.7238 - val_loss: 0.9781 - val_acc: 0.7045 - lr: 0.0010\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9247 - acc: 0.7165 - val_loss: 0.9831 - val_acc: 0.7034 - lr: 1.0000e-05\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9307 - acc: 0.7150 - val_loss: 0.9749 - val_acc: 0.7069 - lr: 1.0000e-05\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9291 - acc: 0.7186 - val_loss: 0.9648 - val_acc: 0.7104 - lr: 1.0000e-05\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9189 - acc: 0.7228 - val_loss: 0.9739 - val_acc: 0.7075 - lr: 1.0000e-05\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9179 - acc: 0.7192 - val_loss: 0.9834 - val_acc: 0.7005 - lr: 1.0000e-05\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9266 - acc: 0.7198 - val_loss: 1.0105 - val_acc: 0.6906 - lr: 1.0000e-05\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9191 - acc: 0.7304 - val_loss: 0.9725 - val_acc: 0.7028 - lr: 1.0000e-05\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9162 - acc: 0.7250 - val_loss: 0.9859 - val_acc: 0.6981 - lr: 1.0000e-05\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9206 - acc: 0.7144 - val_loss: 1.0245 - val_acc: 0.6830 - lr: 1.0000e-05\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9182 - acc: 0.7238 - val_loss: 0.9799 - val_acc: 0.7045 - lr: 1.0000e-05\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9745 - acc: 0.7155\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0731 - acc: 0.6702\n",
            "epsilon: 0.003 and test evaluation : 1.0731276273727417, 0.6701570749282837\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1427 - acc: 0.6353\n",
            "epsilon: 0.005 and test evaluation : 1.142717719078064, 0.6352530717849731\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3282 - acc: 0.5654\n",
            "epsilon: 0.01 and test evaluation : 1.3282272815704346, 0.5654450058937073\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 1.7249 - acc: 0.4363\n",
            "epsilon: 0.02 and test evaluation : 1.7249144315719604, 0.4363001883029938\n",
            "SNR: 33.74473333358765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkS-pL3EkeO",
        "colab_type": "text"
      },
      "source": [
        "# **Show Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH9uGMSFVwwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_df[\"acc_clean_mean\"]= np.sum(result_df['acc_clean'])/3.0\n",
        "result_df[\"acc_0.003_mean\"]= np.sum(result_df['acc1'])/3.0\n",
        "result_df[\"acc_0.005_mean\"]= np.sum(result_df['acc2'])/3.0\n",
        "result_df[\"acc_0.02_mean\"]= np.sum(result_df['acc3'])/3.0\n",
        "result_df[\"acc_0.01_mean\"]= np.sum(result_df['acc4'])/3.0"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvrbKwan2yV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "18b26ebd-afd2-46e7-ae89-3d2c011f97ed"
      },
      "source": [
        "result_df.head(1)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.980906</td>\n",
              "      <td>0.706806</td>\n",
              "      <td>1.071982</td>\n",
              "      <td>0.659686</td>\n",
              "      <td>1.135792</td>\n",
              "      <td>0.621291</td>\n",
              "      <td>1.304538</td>\n",
              "      <td>0.577661</td>\n",
              "      <td>1.668232</td>\n",
              "      <td>0.446771</td>\n",
              "      <td>0.705061</td>\n",
              "      <td>0.66434</td>\n",
              "      <td>0.631763</td>\n",
              "      <td>0.566027</td>\n",
              "      <td>0.445608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    0.980906   0.706806  ...       0.566027       0.445608\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWpwrKuwmsc",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxVNgZvPRHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "class AdversarialTraining(object):\n",
        "    \"\"\"Adversarial Training  \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def train(self, pretrained_model, X_train, Y_train, X_test, y_test, epochs, BS, epsilon_list, sgd):\n",
        "        init = (32, 32,1)\n",
        "        res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "\n",
        "        kfold = KFold(n_splits = 3, random_state = 42)\n",
        "        for j, (train, val) in enumerate(kfold.split(X_train)):\n",
        "          x_train, y_train = self.data_augmentation(X_train[train], Y_train[train], BS, pretrained_model, epsilon_list)\n",
        "          x_val, y_val = self.data_augmentation(X_train[val], Y_train[val], BS, pretrained_model, epsilon_list)\n",
        "          model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "          model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "          hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          validation_steps=x_val.shape[0] // BS,)\n",
        "          loss, acc = model.evaluate(X_test, y_test)\n",
        "          loss1, acc1 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "          loss2, acc2 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "          loss3, acc3 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "          loss4, acc4 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "          row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                  'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "          res_df = res_df.append(row , ignore_index=True)\n",
        "          \n",
        "        return res_df\n",
        "    def mini_batch_train(self, model, X_train,y_train, x_val, y_val, BS, pretrained_model, epsilon):\n",
        "\n",
        "\n",
        "        hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=1,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   validation_steps=x_val.shape[0] // BS, shuffle = True)\n",
        "        \n",
        "        ### TODO ###\n",
        "        ## Save hist on file.###\n",
        "\n",
        "\n",
        "    def data_augmentation(self, X_train, Y_train, batch_size, pretrained_model, epsilon_list):\n",
        "      ### divide data 16,16,16,16 for 4 different epsilons and 64 is true image. ### \n",
        "        #start_index = self.data_iteration(X_train, batch_size)\n",
        "        first_half_end = int(len(X_train)/2)\n",
        "        second_half_end = int(len(X_train))\n",
        "        x_clean = X_train[0:first_half_end,:,:,:]\n",
        "        x_adv = self.get_adversarial(X_train[first_half_end:second_half_end,:,:,:], Y_train[first_half_end:second_half_end], epsilon_list)\n",
        "        x_mix = self.merge_data(x_clean, x_adv)\n",
        "        y_mix = Y_train[0:second_half_end]\n",
        "        ### TODO###\n",
        "        # Mixture data for 4 epsilon values\n",
        "\n",
        "        return x_mix, y_mix\n",
        "\n",
        "    def data_iteration(self, X_train, batch_size):\n",
        "        N = X_train.shape[0]\n",
        "        start = np.random.randint(0, N-batch_size)\n",
        "        return start\n",
        "\n",
        "    def merge_data(self, x_clean, x_adv):\n",
        "        x_mix = []\n",
        "        for i in range(len(x_clean)):\n",
        "          x_mix.append(x_clean[i])\n",
        "        for j in range(len(x_adv)):\n",
        "          x_mix.append(x_adv[j])\n",
        "        x_mix = np.array(x_mix)\n",
        "\n",
        "        return x_mix\n",
        "\n",
        "\n",
        "    def get_adversarial(self, X_true, y_true, epsilon_list):\n",
        "\n",
        "        return self.adversarial_example(X_true, y_true, epsilon_list)\n",
        "\n",
        "    def adversarial_example(self, X_true, Y_true, epsilon_list):\n",
        "        size = len(X_true)\n",
        "        X_adv = []\n",
        "        interval = int(size/4)\n",
        "        index_list = [0,interval, interval*2, interval*3, size]\n",
        "        index = 0\n",
        "        for epsilon in epsilon_list:\n",
        "          if index == 4:\n",
        "            break\n",
        "          x_true = X_true[index_list[index]:index_list[index+1],:,:,:]\n",
        "          y_true = Y_true[index_list[index]:index_list[index+1]]\n",
        "\n",
        "          index = index + 1\n",
        "\n",
        "          for i in range(len(x_true)):\n",
        "            random_index = i\n",
        "            original_image = x_true[random_index]\n",
        "            original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "            original_label = y_true[random_index]\n",
        "            original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "            adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "            X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "          \n",
        "        X_adv = np.array(X_adv)\n",
        "        return X_adv\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5vG0s9PAmw",
        "colab_type": "text"
      },
      "source": [
        "Adversarial Training Second Wide ResNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR3373MWPvSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "adversarial_training =  AdversarialTraining()"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2LxFwajOiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_model = tf.keras.Model(wrn_16_2.input, wrn_16_2.layers[-1].output)\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i82EfjWHP2mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e004620d-c886-452c-d066-0764f6fa9400"
      },
      "source": [
        "result_adv_df = adversarial_training.train(logits_model, X_train, Y_train, X_test, y_test, EPOCHS, BS, epsilon_list, sgd)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 69ms/step - loss: 1.6059 - acc: 0.2769 - val_loss: 1.5796 - val_acc: 0.3442\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.5352 - acc: 0.3605 - val_loss: 1.5156 - val_acc: 0.3471\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4945 - acc: 0.3611 - val_loss: 1.4836 - val_acc: 0.3489\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4625 - acc: 0.3729 - val_loss: 1.4816 - val_acc: 0.3512\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4723 - acc: 0.3759 - val_loss: 1.4663 - val_acc: 0.3885\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4518 - acc: 0.3883 - val_loss: 1.4919 - val_acc: 0.3634\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4414 - acc: 0.3877 - val_loss: 1.4281 - val_acc: 0.4106\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4324 - acc: 0.4056 - val_loss: 1.4311 - val_acc: 0.3966\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4210 - acc: 0.4174 - val_loss: 1.4267 - val_acc: 0.3803\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4168 - acc: 0.4022 - val_loss: 1.4638 - val_acc: 0.3780\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4186 - acc: 0.4159 - val_loss: 1.4375 - val_acc: 0.4147\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4020 - acc: 0.4361 - val_loss: 1.4196 - val_acc: 0.4281\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3989 - acc: 0.4310 - val_loss: 1.4445 - val_acc: 0.3856\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.3822 - acc: 0.4419 - val_loss: 1.4052 - val_acc: 0.4252\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3800 - acc: 0.4504 - val_loss: 1.4401 - val_acc: 0.4013\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3539 - acc: 0.4827 - val_loss: 1.6568 - val_acc: 0.3780\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3302 - acc: 0.5061 - val_loss: 1.2986 - val_acc: 0.5306\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3065 - acc: 0.5281 - val_loss: 1.3032 - val_acc: 0.5265\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.2909 - acc: 0.5336 - val_loss: 1.5891 - val_acc: 0.4298\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.2784 - acc: 0.5481 - val_loss: 1.4745 - val_acc: 0.4525\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2857 - acc: 0.5451 - val_loss: 1.3222 - val_acc: 0.5335\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2353 - acc: 0.5723 - val_loss: 1.2862 - val_acc: 0.5469\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2268 - acc: 0.5814 - val_loss: 1.2631 - val_acc: 0.5743\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2392 - acc: 0.5754 - val_loss: 1.2532 - val_acc: 0.5550\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1998 - acc: 0.5878 - val_loss: 1.2797 - val_acc: 0.5352\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1800 - acc: 0.5905 - val_loss: 1.4650 - val_acc: 0.5294\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1709 - acc: 0.5950 - val_loss: 1.3088 - val_acc: 0.5399\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1658 - acc: 0.6068 - val_loss: 1.2119 - val_acc: 0.5708\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1261 - acc: 0.6247 - val_loss: 1.3033 - val_acc: 0.5457\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1343 - acc: 0.6120 - val_loss: 1.1311 - val_acc: 0.6203\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0938 - acc: 0.6332 - val_loss: 1.3834 - val_acc: 0.5510\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0697 - acc: 0.6556 - val_loss: 1.6687 - val_acc: 0.5067\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0806 - acc: 0.6401 - val_loss: 1.2695 - val_acc: 0.5329\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0500 - acc: 0.6662 - val_loss: 1.0751 - val_acc: 0.6424\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0302 - acc: 0.6643 - val_loss: 1.3917 - val_acc: 0.5900\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0352 - acc: 0.6728 - val_loss: 1.0516 - val_acc: 0.6523\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0052 - acc: 0.6864 - val_loss: 1.0394 - val_acc: 0.6814\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9974 - acc: 0.6792 - val_loss: 1.2248 - val_acc: 0.6127\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9864 - acc: 0.6910 - val_loss: 1.1054 - val_acc: 0.6377\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9939 - acc: 0.6955 - val_loss: 1.0284 - val_acc: 0.6756\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9758 - acc: 0.6955 - val_loss: 1.2602 - val_acc: 0.6075\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9354 - acc: 0.7161 - val_loss: 1.2409 - val_acc: 0.6051\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9723 - acc: 0.6895 - val_loss: 1.4763 - val_acc: 0.5754\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9266 - acc: 0.7107 - val_loss: 1.1418 - val_acc: 0.6395\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9360 - acc: 0.7085 - val_loss: 1.2086 - val_acc: 0.6511\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9218 - acc: 0.7182 - val_loss: 1.1884 - val_acc: 0.6168\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9137 - acc: 0.7279 - val_loss: 1.0265 - val_acc: 0.6820\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9015 - acc: 0.7194 - val_loss: 1.0455 - val_acc: 0.6698\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9096 - acc: 0.7188 - val_loss: 1.1037 - val_acc: 0.6674\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8985 - acc: 0.7252 - val_loss: 1.5209 - val_acc: 0.5696\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8976 - acc: 0.7270 - val_loss: 1.1989 - val_acc: 0.6436\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8661 - acc: 0.7476 - val_loss: 1.3933 - val_acc: 0.6249\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8590 - acc: 0.7470 - val_loss: 1.1350 - val_acc: 0.6756\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8674 - acc: 0.7385 - val_loss: 1.2534 - val_acc: 0.6348\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8693 - acc: 0.7382 - val_loss: 1.0274 - val_acc: 0.6907\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8436 - acc: 0.7479 - val_loss: 1.1635 - val_acc: 0.6639\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8386 - acc: 0.7558 - val_loss: 1.0973 - val_acc: 0.6954\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8367 - acc: 0.7554 - val_loss: 1.0212 - val_acc: 0.6896\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8334 - acc: 0.7579 - val_loss: 1.0577 - val_acc: 0.6843\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8390 - acc: 0.7512 - val_loss: 1.2210 - val_acc: 0.6645\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8037 - acc: 0.7648 - val_loss: 1.0011 - val_acc: 0.6960\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8074 - acc: 0.7612 - val_loss: 1.0327 - val_acc: 0.6960\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8006 - acc: 0.7730 - val_loss: 1.1116 - val_acc: 0.6843\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.7950 - acc: 0.7754 - val_loss: 1.1416 - val_acc: 0.6738\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7667 - acc: 0.7827 - val_loss: 1.0796 - val_acc: 0.7100\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7881 - acc: 0.7712 - val_loss: 1.5060 - val_acc: 0.6267\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7593 - acc: 0.7912 - val_loss: 1.0873 - val_acc: 0.6773\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7755 - acc: 0.7757 - val_loss: 1.1645 - val_acc: 0.6704\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.7716 - acc: 0.7839 - val_loss: 1.0340 - val_acc: 0.7001\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7739 - acc: 0.7842 - val_loss: 1.0974 - val_acc: 0.7001\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0061 - acc: 0.7068\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0284 - acc: 0.7016\n",
            "epsilon: 0.003 and test evaluation : 1.0283770561218262, 0.7015706896781921\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0435 - acc: 0.6946\n",
            "epsilon: 0.005 and test evaluation : 1.0434590578079224, 0.6945898532867432\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0828 - acc: 0.6789\n",
            "epsilon: 0.01 and test evaluation : 1.0828124284744263, 0.6788830757141113\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.1647 - acc: 0.6649\n",
            "epsilon: 0.02 and test evaluation : 1.1646677255630493, 0.6649214625358582\n",
            "SNR: 33.74473333358765\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 1.6056 - acc: 0.2756 - val_loss: 1.5802 - val_acc: 0.3700\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.5422 - acc: 0.3570 - val_loss: 1.5312 - val_acc: 0.3969\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4981 - acc: 0.3737 - val_loss: 1.4948 - val_acc: 0.3846\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4868 - acc: 0.3691 - val_loss: 1.5242 - val_acc: 0.3497\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4739 - acc: 0.3685 - val_loss: 1.5665 - val_acc: 0.3625\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4480 - acc: 0.3964 - val_loss: 1.8748 - val_acc: 0.3089\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4418 - acc: 0.4109 - val_loss: 1.4478 - val_acc: 0.3951\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4501 - acc: 0.4045 - val_loss: 1.4981 - val_acc: 0.3724\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4474 - acc: 0.4047 - val_loss: 1.5698 - val_acc: 0.4003\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4346 - acc: 0.4154 - val_loss: 1.7513 - val_acc: 0.3240\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4165 - acc: 0.4318 - val_loss: 1.4634 - val_acc: 0.4155\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4116 - acc: 0.4436 - val_loss: 1.4181 - val_acc: 0.4528\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3960 - acc: 0.4493 - val_loss: 1.4867 - val_acc: 0.3934\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3889 - acc: 0.4533 - val_loss: 1.3736 - val_acc: 0.4988\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3682 - acc: 0.4811 - val_loss: 1.4809 - val_acc: 0.4411\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3631 - acc: 0.4775 - val_loss: 1.6197 - val_acc: 0.3741\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.3493 - acc: 0.4986 - val_loss: 1.4209 - val_acc: 0.5117\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3193 - acc: 0.5250 - val_loss: 1.4716 - val_acc: 0.4272\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2840 - acc: 0.5346 - val_loss: 1.8927 - val_acc: 0.3846\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2607 - acc: 0.5481 - val_loss: 1.4262 - val_acc: 0.4959\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2231 - acc: 0.5719 - val_loss: 1.3889 - val_acc: 0.4930\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2107 - acc: 0.5846 - val_loss: 1.5910 - val_acc: 0.4545\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1911 - acc: 0.6006 - val_loss: 1.2801 - val_acc: 0.5589\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1744 - acc: 0.6003 - val_loss: 1.2956 - val_acc: 0.5670\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.1500 - acc: 0.6166 - val_loss: 1.3319 - val_acc: 0.5682\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1369 - acc: 0.6157 - val_loss: 1.2112 - val_acc: 0.5845\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1196 - acc: 0.6248 - val_loss: 1.1718 - val_acc: 0.6195\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1056 - acc: 0.6300 - val_loss: 1.1214 - val_acc: 0.6346\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0929 - acc: 0.6390 - val_loss: 1.2800 - val_acc: 0.5962\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0873 - acc: 0.6439 - val_loss: 1.1597 - val_acc: 0.6434\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0666 - acc: 0.6545 - val_loss: 1.5418 - val_acc: 0.5017\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0760 - acc: 0.6475 - val_loss: 1.1979 - val_acc: 0.5938\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0424 - acc: 0.6641 - val_loss: 1.2398 - val_acc: 0.5705\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0299 - acc: 0.6687 - val_loss: 1.1593 - val_acc: 0.6171\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0303 - acc: 0.6699 - val_loss: 1.1614 - val_acc: 0.6282\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0154 - acc: 0.6778 - val_loss: 1.0576 - val_acc: 0.6643\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9860 - acc: 0.6956 - val_loss: 1.2317 - val_acc: 0.5991\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0027 - acc: 0.6856 - val_loss: 1.0628 - val_acc: 0.6533\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9811 - acc: 0.6914 - val_loss: 1.0853 - val_acc: 0.6480\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9650 - acc: 0.6998 - val_loss: 1.0290 - val_acc: 0.6847\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9522 - acc: 0.7050 - val_loss: 1.0946 - val_acc: 0.6667\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9568 - acc: 0.7113 - val_loss: 1.0198 - val_acc: 0.6818\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9402 - acc: 0.7071 - val_loss: 1.0374 - val_acc: 0.6754\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9432 - acc: 0.7083 - val_loss: 1.0294 - val_acc: 0.6725\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9363 - acc: 0.7095 - val_loss: 1.0277 - val_acc: 0.6719\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9078 - acc: 0.7207 - val_loss: 1.1471 - val_acc: 0.6404\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9122 - acc: 0.7289 - val_loss: 1.2567 - val_acc: 0.6166\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9022 - acc: 0.7247 - val_loss: 1.0181 - val_acc: 0.6789\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8914 - acc: 0.7216 - val_loss: 1.0258 - val_acc: 0.6696\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8827 - acc: 0.7319 - val_loss: 1.0138 - val_acc: 0.6911\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8924 - acc: 0.7319 - val_loss: 1.1242 - val_acc: 0.6387\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8834 - acc: 0.7392 - val_loss: 1.0713 - val_acc: 0.6451\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8710 - acc: 0.7365 - val_loss: 0.9681 - val_acc: 0.7034\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8488 - acc: 0.7510 - val_loss: 0.9798 - val_acc: 0.6964\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8597 - acc: 0.7458 - val_loss: 0.9961 - val_acc: 0.6929\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8518 - acc: 0.7413 - val_loss: 1.0151 - val_acc: 0.6923\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8398 - acc: 0.7522 - val_loss: 0.9641 - val_acc: 0.6993\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8351 - acc: 0.7549 - val_loss: 1.0107 - val_acc: 0.6917\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8426 - acc: 0.7507 - val_loss: 0.9512 - val_acc: 0.7133\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8320 - acc: 0.7510 - val_loss: 0.9782 - val_acc: 0.7063\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8067 - acc: 0.7646 - val_loss: 0.9410 - val_acc: 0.6964\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8310 - acc: 0.7558 - val_loss: 0.9989 - val_acc: 0.6795\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8025 - acc: 0.7673 - val_loss: 1.0100 - val_acc: 0.6766\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8108 - acc: 0.7537 - val_loss: 0.9344 - val_acc: 0.7098\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.7933 - acc: 0.7667 - val_loss: 0.9957 - val_acc: 0.6952\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7927 - acc: 0.7716 - val_loss: 0.9861 - val_acc: 0.6993\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.7900 - acc: 0.7725 - val_loss: 0.9038 - val_acc: 0.7372\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7685 - acc: 0.7800 - val_loss: 1.0008 - val_acc: 0.7092\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7872 - acc: 0.7664 - val_loss: 1.0318 - val_acc: 0.6841\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7821 - acc: 0.7691 - val_loss: 1.1491 - val_acc: 0.6626\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1013 - acc: 0.6824\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1215 - acc: 0.6754\n",
            "epsilon: 0.003 and test evaluation : 1.1214661598205566, 0.6753926873207092\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1354 - acc: 0.6702\n",
            "epsilon: 0.005 and test evaluation : 1.135366678237915, 0.6701570749282837\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1717 - acc: 0.6597\n",
            "epsilon: 0.01 and test evaluation : 1.1716943979263306, 0.6596858501434326\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.2489 - acc: 0.6300\n",
            "epsilon: 0.02 and test evaluation : 1.2489458322525024, 0.6300174593925476\n",
            "SNR: 33.74473333358765\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/70\n",
            "26/26 [==============================] - 2s 67ms/step - loss: 1.6001 - acc: 0.3056 - val_loss: 1.5437 - val_acc: 0.3561\n",
            "Epoch 2/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.5327 - acc: 0.3537 - val_loss: 1.5128 - val_acc: 0.3840\n",
            "Epoch 3/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5058 - acc: 0.3458 - val_loss: 1.4595 - val_acc: 0.3974\n",
            "Epoch 4/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4836 - acc: 0.3719 - val_loss: 1.5098 - val_acc: 0.3590\n",
            "Epoch 5/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4798 - acc: 0.3664 - val_loss: 1.4933 - val_acc: 0.3805\n",
            "Epoch 6/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4618 - acc: 0.3776 - val_loss: 1.4349 - val_acc: 0.3963\n",
            "Epoch 7/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4489 - acc: 0.3888 - val_loss: 1.4295 - val_acc: 0.4015\n",
            "Epoch 8/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4356 - acc: 0.3997 - val_loss: 1.4942 - val_acc: 0.3584\n",
            "Epoch 9/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4351 - acc: 0.4094 - val_loss: 1.4039 - val_acc: 0.4312\n",
            "Epoch 10/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4212 - acc: 0.4303 - val_loss: 1.3938 - val_acc: 0.4656\n",
            "Epoch 11/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3977 - acc: 0.4548 - val_loss: 1.3814 - val_acc: 0.4930\n",
            "Epoch 12/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3955 - acc: 0.4772 - val_loss: 1.3524 - val_acc: 0.5087\n",
            "Epoch 13/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3560 - acc: 0.5071 - val_loss: 1.4292 - val_acc: 0.4242\n",
            "Epoch 14/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3325 - acc: 0.5277 - val_loss: 2.0331 - val_acc: 0.3590\n",
            "Epoch 15/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3076 - acc: 0.5292 - val_loss: 1.5764 - val_acc: 0.3840\n",
            "Epoch 16/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2941 - acc: 0.5362 - val_loss: 1.3945 - val_acc: 0.4825\n",
            "Epoch 17/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2682 - acc: 0.5556 - val_loss: 1.3997 - val_acc: 0.5192\n",
            "Epoch 18/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2482 - acc: 0.5655 - val_loss: 1.6980 - val_acc: 0.3537\n",
            "Epoch 19/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2471 - acc: 0.5619 - val_loss: 1.2068 - val_acc: 0.5909\n",
            "Epoch 20/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2381 - acc: 0.5622 - val_loss: 1.6054 - val_acc: 0.4610\n",
            "Epoch 21/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.2056 - acc: 0.5855 - val_loss: 1.1930 - val_acc: 0.5816\n",
            "Epoch 22/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2031 - acc: 0.5812 - val_loss: 1.3135 - val_acc: 0.5262\n",
            "Epoch 23/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.1870 - acc: 0.5861 - val_loss: 1.1897 - val_acc: 0.5892\n",
            "Epoch 24/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1757 - acc: 0.5967 - val_loss: 1.3618 - val_acc: 0.5443\n",
            "Epoch 25/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1578 - acc: 0.6073 - val_loss: 1.1319 - val_acc: 0.6340\n",
            "Epoch 26/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1395 - acc: 0.6142 - val_loss: 1.1640 - val_acc: 0.5868\n",
            "Epoch 27/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1226 - acc: 0.6312 - val_loss: 1.2214 - val_acc: 0.5927\n",
            "Epoch 28/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1190 - acc: 0.6148 - val_loss: 1.1479 - val_acc: 0.6078\n",
            "Epoch 29/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1048 - acc: 0.6293 - val_loss: 1.1203 - val_acc: 0.6247\n",
            "Epoch 30/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0831 - acc: 0.6433 - val_loss: 1.1025 - val_acc: 0.6445\n",
            "Epoch 31/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0807 - acc: 0.6384 - val_loss: 1.0694 - val_acc: 0.6340\n",
            "Epoch 32/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0708 - acc: 0.6454 - val_loss: 1.0818 - val_acc: 0.6131\n",
            "Epoch 33/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0382 - acc: 0.6654 - val_loss: 1.3496 - val_acc: 0.5717\n",
            "Epoch 34/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.0415 - acc: 0.6608 - val_loss: 1.1472 - val_acc: 0.5944\n",
            "Epoch 35/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0258 - acc: 0.6708 - val_loss: 1.2604 - val_acc: 0.5752\n",
            "Epoch 36/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0244 - acc: 0.6638 - val_loss: 1.0377 - val_acc: 0.6643\n",
            "Epoch 37/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9914 - acc: 0.6865 - val_loss: 1.0665 - val_acc: 0.6422\n",
            "Epoch 38/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9974 - acc: 0.6787 - val_loss: 1.1575 - val_acc: 0.6492\n",
            "Epoch 39/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9785 - acc: 0.6835 - val_loss: 1.4085 - val_acc: 0.5536\n",
            "Epoch 40/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9781 - acc: 0.6965 - val_loss: 1.1778 - val_acc: 0.6136\n",
            "Epoch 41/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9698 - acc: 0.6935 - val_loss: 1.0754 - val_acc: 0.6550\n",
            "Epoch 42/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9555 - acc: 0.6971 - val_loss: 1.1299 - val_acc: 0.6329\n",
            "Epoch 43/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9367 - acc: 0.7132 - val_loss: 1.1047 - val_acc: 0.6416\n",
            "Epoch 44/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9324 - acc: 0.7071 - val_loss: 1.0749 - val_acc: 0.6562\n",
            "Epoch 45/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9423 - acc: 0.7144 - val_loss: 1.0050 - val_acc: 0.6900\n",
            "Epoch 46/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9308 - acc: 0.7138 - val_loss: 0.9818 - val_acc: 0.6964\n",
            "Epoch 47/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9086 - acc: 0.7292 - val_loss: 1.0451 - val_acc: 0.6731\n",
            "Epoch 48/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8931 - acc: 0.7250 - val_loss: 1.1022 - val_acc: 0.6247\n",
            "Epoch 49/70\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8988 - acc: 0.7244 - val_loss: 1.1472 - val_acc: 0.6393\n",
            "Epoch 50/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8845 - acc: 0.7440 - val_loss: 0.9591 - val_acc: 0.7051\n",
            "Epoch 51/70\n",
            "26/26 [==============================] - 1s 58ms/step - loss: 0.8927 - acc: 0.7298 - val_loss: 1.0767 - val_acc: 0.6812\n",
            "Epoch 52/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8587 - acc: 0.7401 - val_loss: 1.1007 - val_acc: 0.6836\n",
            "Epoch 53/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8555 - acc: 0.7410 - val_loss: 1.0587 - val_acc: 0.6766\n",
            "Epoch 54/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.8703 - acc: 0.7422 - val_loss: 1.0096 - val_acc: 0.6882\n",
            "Epoch 55/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8715 - acc: 0.7334 - val_loss: 1.0213 - val_acc: 0.6783\n",
            "Epoch 56/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8519 - acc: 0.7362 - val_loss: 1.1974 - val_acc: 0.6224\n",
            "Epoch 57/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.8459 - acc: 0.7443 - val_loss: 1.0532 - val_acc: 0.6871\n",
            "Epoch 58/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8355 - acc: 0.7537 - val_loss: 0.9966 - val_acc: 0.7057\n",
            "Epoch 59/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8184 - acc: 0.7631 - val_loss: 1.1838 - val_acc: 0.6667\n",
            "Epoch 60/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8347 - acc: 0.7461 - val_loss: 0.9836 - val_acc: 0.7022\n",
            "Epoch 61/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8234 - acc: 0.7595 - val_loss: 0.9938 - val_acc: 0.7057\n",
            "Epoch 62/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8098 - acc: 0.7582 - val_loss: 1.0861 - val_acc: 0.6801\n",
            "Epoch 63/70\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.8067 - acc: 0.7640 - val_loss: 0.9850 - val_acc: 0.6836\n",
            "Epoch 64/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7936 - acc: 0.7688 - val_loss: 0.9744 - val_acc: 0.7098\n",
            "Epoch 65/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8119 - acc: 0.7486 - val_loss: 1.0880 - val_acc: 0.6847\n",
            "Epoch 66/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.7857 - acc: 0.7685 - val_loss: 1.0184 - val_acc: 0.6818\n",
            "Epoch 67/70\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.7918 - acc: 0.7746 - val_loss: 1.0296 - val_acc: 0.7063\n",
            "Epoch 68/70\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.7791 - acc: 0.7652 - val_loss: 1.0792 - val_acc: 0.6661\n",
            "Epoch 69/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7813 - acc: 0.7734 - val_loss: 0.9998 - val_acc: 0.7185\n",
            "Epoch 70/70\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.7657 - acc: 0.7831 - val_loss: 0.9741 - val_acc: 0.7075\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9835 - acc: 0.7155\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0026 - acc: 0.7138\n",
            "epsilon: 0.003 and test evaluation : 1.002604603767395, 0.7137870788574219\n",
            "SNR: 50.22322177886963\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0156 - acc: 0.7086\n",
            "epsilon: 0.005 and test evaluation : 1.0155906677246094, 0.7085514664649963\n",
            "SNR: 45.78592777252197\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0487 - acc: 0.6911\n",
            "epsilon: 0.01 and test evaluation : 1.0487189292907715, 0.6910994648933411\n",
            "SNR: 39.765334129333496\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1185 - acc: 0.6632\n",
            "epsilon: 0.02 and test evaluation : 1.1185352802276611, 0.6631762385368347\n",
            "SNR: 33.74473333358765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBzRKS8IoCR",
        "colab_type": "text"
      },
      "source": [
        "# **Show Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KYCUfIFW6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_adv_df[\"acc_clean_mean\"]= np.sum(result_adv_df['acc_clean'])/3.0\n",
        "result_adv_df[\"acc_0.003_mean\"]= np.sum(result_adv_df['acc1'])/3.0\n",
        "result_adv_df[\"acc_0.005_mean\"]= np.sum(result_adv_df['acc2'])/3.0\n",
        "result_adv_df[\"acc_0.02_mean\"]= np.sum(result_adv_df['acc3'])/3.0\n",
        "result_adv_df[\"acc_0.01_mean\"]= np.sum(result_adv_df['acc4'])/3.0"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgmnwxkO5xdV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "92261dc6-d420-41d1-bd21-ac68e4855174"
      },
      "source": [
        "result_adv_df.head(1)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.006134</td>\n",
              "      <td>0.706806</td>\n",
              "      <td>1.028377</td>\n",
              "      <td>0.701571</td>\n",
              "      <td>1.043459</td>\n",
              "      <td>0.69459</td>\n",
              "      <td>1.082812</td>\n",
              "      <td>0.678883</td>\n",
              "      <td>1.164668</td>\n",
              "      <td>0.664921</td>\n",
              "      <td>0.701571</td>\n",
              "      <td>0.696917</td>\n",
              "      <td>0.691099</td>\n",
              "      <td>0.676556</td>\n",
              "      <td>0.652705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    1.006134   0.706806  ...       0.676556       0.652705\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7ANBcRCh4ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "43dd673a-b621-4b03-a0b7-c97ffbf2d33a"
      },
      "source": [
        "result_adv_df.head(1)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.020775</td>\n",
              "      <td>0.664921</td>\n",
              "      <td>1.03504</td>\n",
              "      <td>0.657941</td>\n",
              "      <td>1.044713</td>\n",
              "      <td>0.65096</td>\n",
              "      <td>1.069563</td>\n",
              "      <td>0.642234</td>\n",
              "      <td>1.121984</td>\n",
              "      <td>0.619546</td>\n",
              "      <td>0.690518</td>\n",
              "      <td>0.682955</td>\n",
              "      <td>0.678883</td>\n",
              "      <td>0.664921</td>\n",
              "      <td>0.639325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean    loss1  ...  acc_0.005_mean  acc_0.02_mean  acc_0.01_mean\n",
              "0    1.020775   0.664921  1.03504  ...        0.678883       0.664921       0.639325\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}