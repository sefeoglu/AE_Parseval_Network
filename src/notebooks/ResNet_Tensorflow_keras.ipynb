{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Tensorflow_keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sefeoglu/AE_Parseval_Network/blob/master/src/notebooks/ResNet_Tensorflow_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cczYDRrfFlDx",
        "colab_type": "text"
      },
      "source": [
        "# Wide ResNet 16_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWvd9YADGtMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df5e59d6-7be1-404a-b17a-b0f2b647e8a8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, EarlyStopping\n",
        "import tensorflow\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tensorflow Version: 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqbIFJTwXLH",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRdSMgRjG8ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2b6c98b9-1654-412a-f0dd-6c3041167d5f"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "weight_decay = 0.0001\n",
        "\n",
        "\n",
        "def initial_conv(input):\n",
        "  \n",
        "    x = Convolution2D(16, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(input)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv(init, base, k, strides=(1, 1)):\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', strides=strides, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Convolution2D(base * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    skip = Convolution2D(base * k, (1, 1), padding='same', strides=strides, kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(init)\n",
        "\n",
        "    m = Add()([x, skip])\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "def conv1_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(16 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv2_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv2:channel:  {}\".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(32 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def conv3_block(input, k=1, dropout=0.0):\n",
        "    init = input\n",
        "\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    print(\"conv3 channel_axis:{} \".format(channel_axis))\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(input)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    if dropout > 0.0: x = Dropout(dropout)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Convolution2D(64 * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "                      kernel_regularizer=l2(weight_decay),\n",
        "                      use_bias=False)(x)\n",
        "\n",
        "    m = Add()([init, x])\n",
        "    return m\n",
        "\n",
        "def create_wide_residual_network(input_dim, nb_classes=100, N=2, k=1, dropout=0.0, verbose=1):\n",
        "    \"\"\"\n",
        "    Creates a Wide Residual Network with specified parameters\n",
        "\n",
        "    :param input: Input Keras object\n",
        "    :param nb_classes: Number of output classes\n",
        "    :param N: Depth of the network. Compute N = (n - 4) / 6.\n",
        "              Example : For a depth of 16, n = 16, N = (16 - 4) / 6 = 2\n",
        "              Example2: For a depth of 28, n = 28, N = (28 - 4) / 6 = 4\n",
        "              Example3: For a depth of 40, n = 40, N = (40 - 4) / 6 = 6\n",
        "    :param k: Width of the network.\n",
        "    :param dropout: Adds dropout if value is greater than 0.0\n",
        "    :param verbose: Debug info to describe created WRN\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    ip = Input(shape=input_dim)\n",
        "\n",
        "    x = initial_conv(ip)\n",
        "    nb_conv = 4\n",
        "\n",
        "    x = expand_conv(x, 16, k)\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv1_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 32, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv2_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = expand_conv(x, 64, k, strides=(2, 2))\n",
        "    nb_conv += 2\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        x = conv3_block(x, k, dropout)\n",
        "        nb_conv += 2\n",
        "\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.1, epsilon=1e-5, gamma_initializer='uniform')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = AveragePooling2D((8, 8))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(nb_classes, kernel_regularizer=l2(weight_decay), activation='softmax')(x)\n",
        "\n",
        "    model = Model(ip, x)\n",
        "\n",
        "    if verbose: print(\"Wide Residual Network-%d-%d created.\" % (nb_conv, k))\n",
        "    return model\n",
        "if __name__ == \"__main__\":\n",
        "  init = (32, 32,1)\n",
        "  wrn_16_2 = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNo5x-Ft9Fe",
        "colab_type": "text"
      },
      "source": [
        "# Data Prepare and Processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqH742XcPQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNBI_SkvuzgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    with open(\"data.pz\", 'rb') as file_:\n",
        "        with gzip.GzipFile(fileobj=file_) as gzf:\n",
        "            data = pickle.load(gzf, encoding='latin1', fix_imports=True)\n",
        "    return data\n",
        "data = read_data()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4euxwMe2jIoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c67229f-851a-4d96-de00-bb9617fba053"
      },
      "source": [
        "import cv2\n",
        "new_data_X = []\n",
        "Y_data = []\n",
        "for row in data:\n",
        "    new_data_X.append(cv2.resize(row['crop'], (32,32)))\n",
        "    Y_data.append(row['label'])\n",
        "new_data_X = np.array(new_data_X)\n",
        "new_data_X.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNBsNVDNu6Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6c9c039-c07b-495e-dfff-3d9b3ccf081f"
      },
      "source": [
        "X = new_data_X.astype('float32')\n",
        "X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5722, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFQdrnTKuM8c",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqf-dZOrvC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_rows, img_cols = X[0].shape\n",
        "\n",
        "# transform data set\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEHVf2Bu9xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "y_df = pd.DataFrame(Y_data, columns=['Label'])\n",
        "y_df['Encoded'] = labelencoder.fit_transform(y_df['Label'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdkpb2Jkqu6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_cat = to_categorical(y_df['Encoded'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb5M1kDQnX5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, y_test = train_test_split(X, y_cat, test_size = 0.1)\n",
        "x_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k70n9nAnO24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data_set.pickle', 'rb') as f:\n",
        "    x = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBUCSuWOnP5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train,X_test, y_test, X_val, y_val = x['X_train'], x['y_train'], x['X_test'], x['y_test'], x['X_val'], x['y_val']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kif3Li9NuSnV",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88yOqhbSwjPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_sch(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.1\n",
        "    elif epoch < 50:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.001\n",
        "    else:\n",
        "        return 0.00001\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_sch)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbpiWMEgRpWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = tensorflow.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                               width_shift_range=5./32,\n",
        "                               height_shift_range=5./32,)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-W3MPorKESw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,X_train,y_train, X_val, y_val):\n",
        "  \n",
        "  hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "  show_graph(hist)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVs_QNHoEKji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import  KFold\n",
        "\n",
        "class Non_adversarial(object):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def train_iterate(self, X_train, Y_train, X_test, y_test, epochs, BS,sgd, epsilon_list):\n",
        "          init = (32, 32,1)\n",
        "          res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                  'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                    'acc3','loss4', 'acc4'])\n",
        "          kf = KFold(n_splits=3, random_state=42, shuffle=False)\n",
        "          \n",
        "          for j, (train, val) in enumerate(kf.split(X_train)):\n",
        "            x_train, y_train,  x_val, y_val = X_train[train], Y_train[train], X_train[val], Y_train[val]\n",
        "            model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "\n",
        "            model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "            hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                            validation_data=(x_val, y_val),\n",
        "                            validation_steps=x_val.shape[0] // BS,)\n",
        "            loss, acc = model.evaluate(X_test, y_test)\n",
        "            loss1, acc1 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "            loss2, acc2 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "            loss3, acc3 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "            loss4, acc4 = print_test(model, get_adversarial_examples(model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "            row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                    'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "            res_df = res_df.append(row , ignore_index=True)\n",
        "            \n",
        "          return res_df"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFgKVWiHKYsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install git+https://github.com/tensorflow/cleverhans.git#egg=cleverhans\n",
        "\n",
        "import cleverhans\n",
        "\n",
        "print(\"\\nTensorflow Version: \" + tf.__version__)\n",
        "print(\"Cleverhans Version: \" + cleverhans.__version__)\n",
        "print(\"GPU Available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6HjbpvKslU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from cleverhans.future.tf2.attacks import fast_gradient_method\n",
        "\n",
        "def get_adversarial_examples(pretrained_model, X_true, y_true, epsilon):\n",
        "  #The attack requires the model to ouput the logits\n",
        "   \n",
        "  logits_model = tf.keras.Model(pretrained_model.input,pretrained_model.layers[-1].output)\n",
        "  X_adv = []\n",
        "  for i in range(len(X_true)):\n",
        "    random_index = i\n",
        "    original_image = X_true[random_index]\n",
        "    original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "    original_label = y_true[random_index]\n",
        "    original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "    adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "    X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "  X_adv = np.array(X_adv)\n",
        "  return X_adv\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_quzDGwKGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_graph(hist):\n",
        "  history = hist\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"wrn_tensor.png\")\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  plt.savefig(\"deneme.png\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbbLi83NyVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_test(model,X_adv, X_test, y_test, epsilon):\n",
        "  loss, acc = model.evaluate(X_adv,y_test)\n",
        "  print(\"epsilon: {} and test evaluation : {}, {}\".format(epsilon,loss, acc))\n",
        "  SNR = 20*np.log10(np.linalg.norm(X_test)/np.linalg.norm(X_test-X_adv))\n",
        "  print(\"SNR: {}\".format(SNR))\n",
        "  return loss, acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxNDNAa6MWu7",
        "colab_type": "text"
      },
      "source": [
        "**Train a Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghSgp3NvhhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 200\n",
        "BS = 128\n",
        "sgd = SGD(lr=0.1, momentum=0.6)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBnqXaiNwHGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f90f0826-376b-43d2-ed01-b504c98e6db1"
      },
      "source": [
        "#wrn_16_2.summary()\n",
        "wrn_16_2.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "print(\"Finished compiling\")\n",
        "\n",
        "hist = wrn_16_2.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=EPOCHS,\n",
        "                   callbacks = [lr_scheduler],\n",
        "                   validation_data=(X_val, y_val),\n",
        "                   validation_steps=X_val.shape[0] // BS,)\n",
        "wrn_16_2.save(\"wrn_model.h5\")\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished compiling\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 2s 58ms/step - loss: 1.5049 - acc: 0.3675 - val_loss: 1.5352 - val_acc: 0.3728 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4835 - acc: 0.3739 - val_loss: 1.4463 - val_acc: 0.3961 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.4616 - acc: 0.3835 - val_loss: 1.4805 - val_acc: 0.4058 - lr: 0.1000\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4522 - acc: 0.3981 - val_loss: 1.4298 - val_acc: 0.4252 - lr: 0.1000\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4325 - acc: 0.4214 - val_loss: 1.5391 - val_acc: 0.3340 - lr: 0.1000\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.4093 - acc: 0.4410 - val_loss: 1.4326 - val_acc: 0.4136 - lr: 0.1000\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.3808 - acc: 0.4836 - val_loss: 1.4127 - val_acc: 0.4388 - lr: 0.1000\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.3533 - acc: 0.5058 - val_loss: 1.3913 - val_acc: 0.4816 - lr: 0.1000\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 1.3157 - acc: 0.5311 - val_loss: 1.4624 - val_acc: 0.4369 - lr: 0.1000\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.3082 - acc: 0.5275 - val_loss: 1.5988 - val_acc: 0.4757 - lr: 0.1000\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.2677 - acc: 0.5684 - val_loss: 1.2488 - val_acc: 0.5845 - lr: 0.1000\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 1.2275 - acc: 0.5808 - val_loss: 1.5180 - val_acc: 0.5184 - lr: 0.1000\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.2083 - acc: 0.5888 - val_loss: 1.5072 - val_acc: 0.5029 - lr: 0.1000\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1929 - acc: 0.5970 - val_loss: 1.3283 - val_acc: 0.5553 - lr: 0.1000\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1544 - acc: 0.6119 - val_loss: 1.3817 - val_acc: 0.5631 - lr: 0.1000\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.1570 - acc: 0.6085 - val_loss: 1.2191 - val_acc: 0.5767 - lr: 0.1000\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.1165 - acc: 0.6320 - val_loss: 1.2090 - val_acc: 0.6155 - lr: 0.1000\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 1.0923 - acc: 0.6407 - val_loss: 1.1713 - val_acc: 0.6369 - lr: 0.1000\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0782 - acc: 0.6505 - val_loss: 1.1580 - val_acc: 0.6447 - lr: 0.1000\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0486 - acc: 0.6542 - val_loss: 1.1556 - val_acc: 0.6233 - lr: 0.1000\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0281 - acc: 0.6707 - val_loss: 0.9837 - val_acc: 0.7126 - lr: 0.1000\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.0399 - acc: 0.6620 - val_loss: 0.9727 - val_acc: 0.7165 - lr: 0.1000\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 1.0139 - acc: 0.6758 - val_loss: 1.0697 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9985 - acc: 0.6851 - val_loss: 1.2143 - val_acc: 0.6194 - lr: 0.1000\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.9906 - acc: 0.6937 - val_loss: 1.0044 - val_acc: 0.6932 - lr: 0.1000\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9585 - acc: 0.6980 - val_loss: 1.1321 - val_acc: 0.6777 - lr: 0.1000\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9521 - acc: 0.7037 - val_loss: 1.0165 - val_acc: 0.6913 - lr: 0.1000\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9447 - acc: 0.7097 - val_loss: 1.0316 - val_acc: 0.7087 - lr: 0.1000\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.9213 - acc: 0.7153 - val_loss: 1.1291 - val_acc: 0.6583 - lr: 0.1000\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.9186 - acc: 0.7195 - val_loss: 1.0758 - val_acc: 0.6951 - lr: 0.1000\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.9537 - acc: 0.7075 - val_loss: 1.1703 - val_acc: 0.6583 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.9083 - acc: 0.7239 - val_loss: 1.0197 - val_acc: 0.6893 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8867 - acc: 0.7366 - val_loss: 1.0504 - val_acc: 0.6816 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8671 - acc: 0.7443 - val_loss: 1.0271 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8737 - acc: 0.7375 - val_loss: 1.0447 - val_acc: 0.6854 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8480 - acc: 0.7559 - val_loss: 0.9918 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8460 - acc: 0.7563 - val_loss: 0.9501 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8506 - acc: 0.7541 - val_loss: 0.9175 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8580 - acc: 0.7435 - val_loss: 1.0011 - val_acc: 0.6971 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8433 - acc: 0.7512 - val_loss: 0.9503 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8323 - acc: 0.7612 - val_loss: 1.0021 - val_acc: 0.6951 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8317 - acc: 0.7632 - val_loss: 0.9337 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8331 - acc: 0.7639 - val_loss: 0.9609 - val_acc: 0.7107 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8350 - acc: 0.7552 - val_loss: 0.9406 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8355 - acc: 0.7565 - val_loss: 0.9375 - val_acc: 0.7204 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8335 - acc: 0.7565 - val_loss: 0.9441 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8302 - acc: 0.7634 - val_loss: 0.9902 - val_acc: 0.7126 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8265 - acc: 0.7674 - val_loss: 0.9250 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8256 - acc: 0.7630 - val_loss: 0.9269 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8280 - acc: 0.7539 - val_loss: 0.9278 - val_acc: 0.7243 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8269 - acc: 0.7628 - val_loss: 0.9208 - val_acc: 0.7262 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8263 - acc: 0.7592 - val_loss: 0.9734 - val_acc: 0.7087 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8195 - acc: 0.7659 - val_loss: 0.9234 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8145 - acc: 0.7676 - val_loss: 0.9619 - val_acc: 0.7146 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8250 - acc: 0.7614 - val_loss: 0.9762 - val_acc: 0.7223 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8214 - acc: 0.7630 - val_loss: 0.9188 - val_acc: 0.7437 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8208 - acc: 0.7648 - val_loss: 0.9262 - val_acc: 0.7359 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8082 - acc: 0.7730 - val_loss: 0.9111 - val_acc: 0.7417 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8180 - acc: 0.7714 - val_loss: 0.9034 - val_acc: 0.7301 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8202 - acc: 0.7621 - val_loss: 0.9236 - val_acc: 0.7340 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8130 - acc: 0.7652 - val_loss: 0.9582 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8144 - acc: 0.7704 - val_loss: 0.9591 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8166 - acc: 0.7659 - val_loss: 0.9911 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8096 - acc: 0.7699 - val_loss: 0.9245 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8154 - acc: 0.7636 - val_loss: 0.9187 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8099 - acc: 0.7645 - val_loss: 0.9665 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8071 - acc: 0.7727 - val_loss: 0.9015 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8180 - acc: 0.7594 - val_loss: 0.9628 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8280 - acc: 0.7639 - val_loss: 0.9036 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8220 - acc: 0.7663 - val_loss: 0.9507 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8222 - acc: 0.7659 - val_loss: 0.9597 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8169 - acc: 0.7656 - val_loss: 0.9336 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8177 - acc: 0.7670 - val_loss: 0.9479 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8180 - acc: 0.7650 - val_loss: 1.0237 - val_acc: 0.7087 - lr: 1.0000e-05\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8190 - acc: 0.7643 - val_loss: 0.9016 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8094 - acc: 0.7710 - val_loss: 0.9339 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8225 - acc: 0.7605 - val_loss: 0.9386 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8172 - acc: 0.7699 - val_loss: 0.9345 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8132 - acc: 0.7665 - val_loss: 0.9027 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8175 - acc: 0.7690 - val_loss: 0.9349 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8152 - acc: 0.7656 - val_loss: 0.9503 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8031 - acc: 0.7696 - val_loss: 0.9892 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8209 - acc: 0.7656 - val_loss: 0.9473 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8153 - acc: 0.7659 - val_loss: 0.9253 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8135 - acc: 0.7701 - val_loss: 0.9482 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8090 - acc: 0.7727 - val_loss: 0.9386 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8187 - acc: 0.7639 - val_loss: 0.9463 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8183 - acc: 0.7614 - val_loss: 0.9441 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8156 - acc: 0.7661 - val_loss: 0.9809 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8111 - acc: 0.7670 - val_loss: 0.9263 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8100 - acc: 0.7732 - val_loss: 0.9375 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8142 - acc: 0.7648 - val_loss: 0.9668 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8131 - acc: 0.7648 - val_loss: 0.9653 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8176 - acc: 0.7688 - val_loss: 0.9146 - val_acc: 0.7456 - lr: 1.0000e-05\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8027 - acc: 0.7648 - val_loss: 0.9469 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8136 - acc: 0.7674 - val_loss: 0.9498 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8173 - acc: 0.7597 - val_loss: 0.9435 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8198 - acc: 0.7612 - val_loss: 0.9383 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8044 - acc: 0.7781 - val_loss: 1.0008 - val_acc: 0.7029 - lr: 1.0000e-05\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8090 - acc: 0.7650 - val_loss: 0.9311 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8071 - acc: 0.7710 - val_loss: 0.9383 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8160 - acc: 0.7681 - val_loss: 0.9590 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8107 - acc: 0.7650 - val_loss: 0.9690 - val_acc: 0.7184 - lr: 1.0000e-05\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8137 - acc: 0.7699 - val_loss: 0.9717 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8167 - acc: 0.7619 - val_loss: 0.9228 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8224 - acc: 0.7581 - val_loss: 0.9433 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8193 - acc: 0.7625 - val_loss: 0.9172 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8140 - acc: 0.7670 - val_loss: 0.9327 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8062 - acc: 0.7704 - val_loss: 0.9177 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8178 - acc: 0.7699 - val_loss: 0.9454 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8109 - acc: 0.7694 - val_loss: 0.9586 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8045 - acc: 0.7712 - val_loss: 0.9739 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8121 - acc: 0.7694 - val_loss: 0.9477 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8175 - acc: 0.7630 - val_loss: 0.9372 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8136 - acc: 0.7630 - val_loss: 0.9099 - val_acc: 0.7476 - lr: 1.0000e-05\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8149 - acc: 0.7690 - val_loss: 0.9124 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8112 - acc: 0.7636 - val_loss: 0.9408 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8172 - acc: 0.7674 - val_loss: 0.9261 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8204 - acc: 0.7597 - val_loss: 0.9189 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8142 - acc: 0.7623 - val_loss: 0.9329 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8073 - acc: 0.7719 - val_loss: 0.9625 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8146 - acc: 0.7663 - val_loss: 0.9348 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8165 - acc: 0.7597 - val_loss: 0.9288 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8136 - acc: 0.7676 - val_loss: 0.8972 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8062 - acc: 0.7670 - val_loss: 0.9889 - val_acc: 0.7068 - lr: 1.0000e-05\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8157 - acc: 0.7643 - val_loss: 0.9748 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8178 - acc: 0.7683 - val_loss: 0.9295 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8108 - acc: 0.7668 - val_loss: 0.9485 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8137 - acc: 0.7712 - val_loss: 0.9395 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8209 - acc: 0.7663 - val_loss: 0.9625 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8147 - acc: 0.7665 - val_loss: 0.9712 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8121 - acc: 0.7699 - val_loss: 0.9490 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8174 - acc: 0.7623 - val_loss: 0.9287 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8121 - acc: 0.7652 - val_loss: 0.9544 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8201 - acc: 0.7634 - val_loss: 0.9298 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8209 - acc: 0.7639 - val_loss: 0.9131 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8112 - acc: 0.7650 - val_loss: 0.9781 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8229 - acc: 0.7532 - val_loss: 0.9415 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8166 - acc: 0.7701 - val_loss: 0.9235 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8202 - acc: 0.7568 - val_loss: 0.9375 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8198 - acc: 0.7672 - val_loss: 0.9690 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8166 - acc: 0.7679 - val_loss: 0.9245 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8096 - acc: 0.7623 - val_loss: 0.9000 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8150 - acc: 0.7679 - val_loss: 0.9301 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8185 - acc: 0.7639 - val_loss: 0.9307 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8184 - acc: 0.7685 - val_loss: 0.9274 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8191 - acc: 0.7685 - val_loss: 0.9569 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8144 - acc: 0.7685 - val_loss: 0.9421 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8060 - acc: 0.7732 - val_loss: 0.9360 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8158 - acc: 0.7699 - val_loss: 0.9150 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8089 - acc: 0.7719 - val_loss: 0.9189 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8189 - acc: 0.7659 - val_loss: 1.0238 - val_acc: 0.6932 - lr: 1.0000e-05\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8124 - acc: 0.7696 - val_loss: 0.9506 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8145 - acc: 0.7727 - val_loss: 0.9374 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8182 - acc: 0.7643 - val_loss: 0.9380 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8134 - acc: 0.7701 - val_loss: 0.9042 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 0.8151 - acc: 0.7643 - val_loss: 0.9692 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8139 - acc: 0.7688 - val_loss: 0.9550 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8163 - acc: 0.7639 - val_loss: 0.9322 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8227 - acc: 0.7636 - val_loss: 0.9345 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8168 - acc: 0.7656 - val_loss: 0.9585 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8202 - acc: 0.7626 - val_loss: 0.9097 - val_acc: 0.7437 - lr: 1.0000e-05\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8139 - acc: 0.7614 - val_loss: 0.9604 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8163 - acc: 0.7705 - val_loss: 0.9388 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8073 - acc: 0.7699 - val_loss: 0.9346 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8104 - acc: 0.7665 - val_loss: 0.9505 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8099 - acc: 0.7652 - val_loss: 0.9452 - val_acc: 0.7204 - lr: 1.0000e-05\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8169 - acc: 0.7650 - val_loss: 0.9460 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8137 - acc: 0.7685 - val_loss: 0.9220 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8168 - acc: 0.7661 - val_loss: 0.9421 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8177 - acc: 0.7654 - val_loss: 0.9351 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8040 - acc: 0.7685 - val_loss: 0.9102 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8110 - acc: 0.7714 - val_loss: 0.9612 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8062 - acc: 0.7703 - val_loss: 0.9370 - val_acc: 0.7320 - lr: 1.0000e-05\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8164 - acc: 0.7619 - val_loss: 0.9139 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8223 - acc: 0.7637 - val_loss: 0.9513 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8138 - acc: 0.7670 - val_loss: 0.9414 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 2s 49ms/step - loss: 0.8188 - acc: 0.7636 - val_loss: 0.9253 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8142 - acc: 0.7656 - val_loss: 0.9333 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8009 - acc: 0.7734 - val_loss: 0.9323 - val_acc: 0.7282 - lr: 1.0000e-05\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8129 - acc: 0.7617 - val_loss: 0.9157 - val_acc: 0.7398 - lr: 1.0000e-05\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8067 - acc: 0.7692 - val_loss: 0.9407 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8163 - acc: 0.7685 - val_loss: 0.9268 - val_acc: 0.7379 - lr: 1.0000e-05\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8122 - acc: 0.7696 - val_loss: 0.9800 - val_acc: 0.7049 - lr: 1.0000e-05\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8117 - acc: 0.7668 - val_loss: 0.9283 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8165 - acc: 0.7681 - val_loss: 0.9763 - val_acc: 0.7126 - lr: 1.0000e-05\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8258 - acc: 0.7594 - val_loss: 0.9799 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8189 - acc: 0.7608 - val_loss: 1.0410 - val_acc: 0.6913 - lr: 1.0000e-05\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8056 - acc: 0.7770 - val_loss: 0.9688 - val_acc: 0.7301 - lr: 1.0000e-05\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8062 - acc: 0.7752 - val_loss: 0.9426 - val_acc: 0.7262 - lr: 1.0000e-05\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8129 - acc: 0.7696 - val_loss: 0.9141 - val_acc: 0.7417 - lr: 1.0000e-05\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8027 - acc: 0.7739 - val_loss: 0.9292 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8168 - acc: 0.7630 - val_loss: 0.9521 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8108 - acc: 0.7679 - val_loss: 0.9942 - val_acc: 0.7165 - lr: 1.0000e-05\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.8162 - acc: 0.7656 - val_loss: 0.9430 - val_acc: 0.7223 - lr: 1.0000e-05\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8136 - acc: 0.7670 - val_loss: 0.9519 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 2s 50ms/step - loss: 0.8105 - acc: 0.7665 - val_loss: 0.9176 - val_acc: 0.7340 - lr: 1.0000e-05\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8138 - acc: 0.7669 - val_loss: 0.9444 - val_acc: 0.7243 - lr: 1.0000e-05\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.8148 - acc: 0.7668 - val_loss: 0.9376 - val_acc: 0.7359 - lr: 1.0000e-05\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 0.8195 - acc: 0.7603 - val_loss: 0.9158 - val_acc: 0.7398 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvd1NVKvpH0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "a8027931-9dd7-430a-901b-e16aeb71258d"
      },
      "source": [
        "show_graph(hist)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'acc', 'val_loss', 'val_acc', 'lr'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEDHJIheU8bm",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydHkZ8pauMe",
        "colab_type": "text"
      },
      "source": [
        "**Non_Adversarial Training Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkjbXCV6KFcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d534ab39-210b-4d4b-be0b-8d1f5e3d7538"
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "train_object = Non_adversarial()\n",
        "result_df = train_object.train_iterate(X_train, Y_train, X_test, y_test, 50, BS,sgd, epsilon_list)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 68ms/step - loss: 1.6068 - acc: 0.2785 - val_loss: 1.5886 - val_acc: 0.3477\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5480 - acc: 0.3686 - val_loss: 1.5321 - val_acc: 0.3331\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.5015 - acc: 0.3680 - val_loss: 1.4922 - val_acc: 0.3908\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4745 - acc: 0.3886 - val_loss: 1.4794 - val_acc: 0.3751\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4620 - acc: 0.3864 - val_loss: 1.7096 - val_acc: 0.2592\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4434 - acc: 0.3980 - val_loss: 1.4585 - val_acc: 0.3815\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4327 - acc: 0.4240 - val_loss: 1.4412 - val_acc: 0.4426\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4136 - acc: 0.4582 - val_loss: 1.3984 - val_acc: 0.4793\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 1.3919 - acc: 0.4781 - val_loss: 1.4368 - val_acc: 0.4473\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3691 - acc: 0.4985 - val_loss: 1.4139 - val_acc: 0.4589\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3439 - acc: 0.5266 - val_loss: 1.9932 - val_acc: 0.3320\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3254 - acc: 0.5227 - val_loss: 1.4996 - val_acc: 0.4054\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3091 - acc: 0.5385 - val_loss: 1.3144 - val_acc: 0.5218\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2901 - acc: 0.5508 - val_loss: 1.2594 - val_acc: 0.5609\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2634 - acc: 0.5672 - val_loss: 1.3160 - val_acc: 0.5312\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2528 - acc: 0.5642 - val_loss: 1.3398 - val_acc: 0.4997\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2357 - acc: 0.5884 - val_loss: 1.1722 - val_acc: 0.5906\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2188 - acc: 0.5847 - val_loss: 1.2167 - val_acc: 0.5783\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.1878 - acc: 0.5975 - val_loss: 1.2301 - val_acc: 0.5667\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1960 - acc: 0.6014 - val_loss: 1.3845 - val_acc: 0.5015\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1680 - acc: 0.6108 - val_loss: 1.1833 - val_acc: 0.6022\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1355 - acc: 0.6180 - val_loss: 1.2143 - val_acc: 0.5964\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1097 - acc: 0.6386 - val_loss: 1.1563 - val_acc: 0.6249\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0936 - acc: 0.6371 - val_loss: 1.0871 - val_acc: 0.6459\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1065 - acc: 0.6389 - val_loss: 1.1284 - val_acc: 0.6226\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0724 - acc: 0.6471 - val_loss: 1.1210 - val_acc: 0.6331\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0546 - acc: 0.6698 - val_loss: 1.0838 - val_acc: 0.6453\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0470 - acc: 0.6659 - val_loss: 1.0758 - val_acc: 0.6570\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0308 - acc: 0.6746 - val_loss: 1.1676 - val_acc: 0.6360\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0052 - acc: 0.6873 - val_loss: 1.1580 - val_acc: 0.6226\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9832 - acc: 0.7037 - val_loss: 1.6541 - val_acc: 0.5096\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9902 - acc: 0.6964 - val_loss: 1.1182 - val_acc: 0.6616\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9652 - acc: 0.6973 - val_loss: 1.2436 - val_acc: 0.6302\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9611 - acc: 0.7064 - val_loss: 1.1517 - val_acc: 0.6290\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9509 - acc: 0.7137 - val_loss: 1.0117 - val_acc: 0.6913\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9440 - acc: 0.7176 - val_loss: 1.1312 - val_acc: 0.6540\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9328 - acc: 0.7158 - val_loss: 1.0577 - val_acc: 0.6797\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8954 - acc: 0.7379 - val_loss: 1.0063 - val_acc: 0.6942\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9046 - acc: 0.7373 - val_loss: 1.3436 - val_acc: 0.6075\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 0.9001 - acc: 0.7364 - val_loss: 1.0418 - val_acc: 0.6966\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8999 - acc: 0.7358 - val_loss: 1.1574 - val_acc: 0.6517\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9177 - acc: 0.7246 - val_loss: 1.0696 - val_acc: 0.6570\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8823 - acc: 0.7421 - val_loss: 1.1390 - val_acc: 0.6610\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8524 - acc: 0.7582 - val_loss: 1.0308 - val_acc: 0.7088\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8388 - acc: 0.7542 - val_loss: 1.0164 - val_acc: 0.7030\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8516 - acc: 0.7561 - val_loss: 1.0144 - val_acc: 0.6989\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8266 - acc: 0.7706 - val_loss: 1.0375 - val_acc: 0.7094\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8196 - acc: 0.7688 - val_loss: 0.9867 - val_acc: 0.7123\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8264 - acc: 0.7570 - val_loss: 1.0021 - val_acc: 0.7140\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8048 - acc: 0.7721 - val_loss: 1.0651 - val_acc: 0.7012\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1288 - acc: 0.7155\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2656 - acc: 0.6736\n",
            "epsilon: 0.003 and test evaluation : 1.2656415700912476, 0.6736474633216858\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.3649 - acc: 0.6440\n",
            "epsilon: 0.005 and test evaluation : 1.364870309829712, 0.6439790725708008\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.6349 - acc: 0.5620\n",
            "epsilon: 0.01 and test evaluation : 1.6348941326141357, 0.5619546175003052\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 2.2129 - acc: 0.4380\n",
            "epsilon: 0.02 and test evaluation : 2.2128703594207764, 0.4380453824996948\n",
            "SNR: 33.76535892486572\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 67ms/step - loss: 1.6019 - acc: 0.2868 - val_loss: 1.5691 - val_acc: 0.3502\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.5308 - acc: 0.3613 - val_loss: 1.6106 - val_acc: 0.2593\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4897 - acc: 0.3752 - val_loss: 1.4938 - val_acc: 0.3695\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4658 - acc: 0.3743 - val_loss: 1.4582 - val_acc: 0.3986\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4402 - acc: 0.3970 - val_loss: 1.6719 - val_acc: 0.2902\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4476 - acc: 0.3982 - val_loss: 1.4564 - val_acc: 0.3864\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4244 - acc: 0.4224 - val_loss: 1.4357 - val_acc: 0.3980\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4171 - acc: 0.4472 - val_loss: 1.5810 - val_acc: 0.3397\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3972 - acc: 0.4726 - val_loss: 1.7217 - val_acc: 0.3374\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.3584 - acc: 0.5053 - val_loss: 1.3581 - val_acc: 0.4755\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3266 - acc: 0.5277 - val_loss: 1.5407 - val_acc: 0.4132\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3023 - acc: 0.5401 - val_loss: 1.3641 - val_acc: 0.4860\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2784 - acc: 0.5631 - val_loss: 1.3587 - val_acc: 0.5175\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.2614 - acc: 0.5661 - val_loss: 1.6915 - val_acc: 0.4802\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2539 - acc: 0.5761 - val_loss: 1.2775 - val_acc: 0.5478\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.2249 - acc: 0.5831 - val_loss: 1.2646 - val_acc: 0.5798\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1956 - acc: 0.5961 - val_loss: 1.2270 - val_acc: 0.6020\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1915 - acc: 0.5997 - val_loss: 1.2319 - val_acc: 0.5874\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1567 - acc: 0.6042 - val_loss: 1.2236 - val_acc: 0.5676\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1596 - acc: 0.6039 - val_loss: 1.1288 - val_acc: 0.6428\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1210 - acc: 0.6457 - val_loss: 1.1556 - val_acc: 0.6247\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1073 - acc: 0.6415 - val_loss: 1.1105 - val_acc: 0.6369\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0960 - acc: 0.6442 - val_loss: 1.3334 - val_acc: 0.5688\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0919 - acc: 0.6566 - val_loss: 1.1601 - val_acc: 0.6247\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0629 - acc: 0.6599 - val_loss: 1.1784 - val_acc: 0.6189\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0433 - acc: 0.6672 - val_loss: 1.1146 - val_acc: 0.6492\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0302 - acc: 0.6747 - val_loss: 1.0982 - val_acc: 0.6469\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0363 - acc: 0.6669 - val_loss: 1.1711 - val_acc: 0.6381\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0216 - acc: 0.6702 - val_loss: 1.0184 - val_acc: 0.6783\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9994 - acc: 0.6911 - val_loss: 1.0549 - val_acc: 0.6737\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9882 - acc: 0.6983 - val_loss: 0.9965 - val_acc: 0.6894\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9857 - acc: 0.6838 - val_loss: 1.0847 - val_acc: 0.6737\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9685 - acc: 0.7026 - val_loss: 1.1046 - val_acc: 0.6725\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9436 - acc: 0.7153 - val_loss: 1.0862 - val_acc: 0.6632\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9366 - acc: 0.7250 - val_loss: 1.0071 - val_acc: 0.6935\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9205 - acc: 0.7262 - val_loss: 1.1225 - val_acc: 0.6801\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9009 - acc: 0.7340 - val_loss: 1.0823 - val_acc: 0.6766\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8947 - acc: 0.7380 - val_loss: 1.1214 - val_acc: 0.6824\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9088 - acc: 0.7295 - val_loss: 1.0244 - val_acc: 0.6894\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8877 - acc: 0.7343 - val_loss: 0.9587 - val_acc: 0.7121\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8687 - acc: 0.7422 - val_loss: 0.9732 - val_acc: 0.7302\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8568 - acc: 0.7452 - val_loss: 1.0580 - val_acc: 0.6993\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8527 - acc: 0.7513 - val_loss: 0.9940 - val_acc: 0.7075\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8551 - acc: 0.7455 - val_loss: 0.9844 - val_acc: 0.7273\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8456 - acc: 0.7510 - val_loss: 1.0450 - val_acc: 0.7133\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8510 - acc: 0.7561 - val_loss: 1.1474 - val_acc: 0.6579\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8372 - acc: 0.7634 - val_loss: 1.0768 - val_acc: 0.6981\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8398 - acc: 0.7573 - val_loss: 1.0142 - val_acc: 0.7209\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8273 - acc: 0.7604 - val_loss: 0.9383 - val_acc: 0.7331\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8115 - acc: 0.7703 - val_loss: 0.9358 - val_acc: 0.7459\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9043 - acc: 0.7627\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0189 - acc: 0.7225\n",
            "epsilon: 0.003 and test evaluation : 1.0189074277877808, 0.7225130796432495\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.1005 - acc: 0.6928\n",
            "epsilon: 0.005 and test evaluation : 1.1004629135131836, 0.6928446888923645\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3175 - acc: 0.6300\n",
            "epsilon: 0.01 and test evaluation : 1.3175164461135864, 0.6300174593925476\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.7863 - acc: 0.5148\n",
            "epsilon: 0.02 and test evaluation : 1.786332130432129, 0.5148342251777649\n",
            "SNR: 33.76535892486572\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 67ms/step - loss: 1.6036 - acc: 0.2802 - val_loss: 1.5877 - val_acc: 0.3858\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5546 - acc: 0.3585 - val_loss: 1.5162 - val_acc: 0.3706\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4987 - acc: 0.3776 - val_loss: 1.4723 - val_acc: 0.3747\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4681 - acc: 0.3834 - val_loss: 1.4792 - val_acc: 0.3858\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4595 - acc: 0.3924 - val_loss: 1.4470 - val_acc: 0.3939\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4489 - acc: 0.4018 - val_loss: 1.4464 - val_acc: 0.3858\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4433 - acc: 0.4003 - val_loss: 1.4224 - val_acc: 0.3998\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 53ms/step - loss: 1.4284 - acc: 0.4163 - val_loss: 1.4541 - val_acc: 0.4231\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4202 - acc: 0.4182 - val_loss: 1.3900 - val_acc: 0.4347\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4049 - acc: 0.4411 - val_loss: 1.5092 - val_acc: 0.4184\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.3973 - acc: 0.4669 - val_loss: 1.3458 - val_acc: 0.5076\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3853 - acc: 0.4793 - val_loss: 1.4050 - val_acc: 0.4831\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.3456 - acc: 0.5116 - val_loss: 1.4239 - val_acc: 0.5303\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3359 - acc: 0.5098 - val_loss: 1.4549 - val_acc: 0.5017\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.2963 - acc: 0.5413 - val_loss: 1.3230 - val_acc: 0.5484\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2730 - acc: 0.5464 - val_loss: 1.5349 - val_acc: 0.4913\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2583 - acc: 0.5625 - val_loss: 1.6192 - val_acc: 0.4073\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2297 - acc: 0.5643 - val_loss: 1.3339 - val_acc: 0.5216\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2012 - acc: 0.5794 - val_loss: 1.2775 - val_acc: 0.5414\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1819 - acc: 0.5900 - val_loss: 1.3954 - val_acc: 0.5058\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1689 - acc: 0.6067 - val_loss: 1.1286 - val_acc: 0.6241\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1522 - acc: 0.6006 - val_loss: 1.1320 - val_acc: 0.6270\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1170 - acc: 0.6293 - val_loss: 1.1510 - val_acc: 0.6358\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0924 - acc: 0.6384 - val_loss: 1.1804 - val_acc: 0.6072\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0804 - acc: 0.6411 - val_loss: 1.1165 - val_acc: 0.6329\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0913 - acc: 0.6372 - val_loss: 1.2298 - val_acc: 0.5629\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0755 - acc: 0.6481 - val_loss: 1.0249 - val_acc: 0.6754\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0579 - acc: 0.6548 - val_loss: 1.0834 - val_acc: 0.6509\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0155 - acc: 0.6811 - val_loss: 1.1499 - val_acc: 0.6206\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0298 - acc: 0.6669 - val_loss: 1.0347 - val_acc: 0.6690\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0024 - acc: 0.6817 - val_loss: 1.0010 - val_acc: 0.6976\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0033 - acc: 0.6890 - val_loss: 1.1586 - val_acc: 0.6515\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9743 - acc: 0.7005 - val_loss: 1.0606 - val_acc: 0.6812\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9689 - acc: 0.6932 - val_loss: 1.3377 - val_acc: 0.5798\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9687 - acc: 0.7032 - val_loss: 0.9596 - val_acc: 0.6993\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9449 - acc: 0.7095 - val_loss: 1.2100 - val_acc: 0.6393\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9427 - acc: 0.6965 - val_loss: 1.4242 - val_acc: 0.5833\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9299 - acc: 0.7153 - val_loss: 1.0833 - val_acc: 0.6766\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9488 - acc: 0.7098 - val_loss: 0.9751 - val_acc: 0.7016\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9236 - acc: 0.7148 - val_loss: 1.1642 - val_acc: 0.6538\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8980 - acc: 0.7256 - val_loss: 1.0290 - val_acc: 0.6882\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9026 - acc: 0.7280 - val_loss: 0.9722 - val_acc: 0.7174\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8707 - acc: 0.7401 - val_loss: 1.3428 - val_acc: 0.6090\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.8617 - acc: 0.7467 - val_loss: 1.1032 - val_acc: 0.6638\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.8681 - acc: 0.7413 - val_loss: 0.9669 - val_acc: 0.7238\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8540 - acc: 0.7455 - val_loss: 1.0854 - val_acc: 0.6777\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8800 - acc: 0.7319 - val_loss: 1.1458 - val_acc: 0.6841\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.8446 - acc: 0.7404 - val_loss: 0.9932 - val_acc: 0.7121\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8623 - acc: 0.7431 - val_loss: 1.2647 - val_acc: 0.6597\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.8357 - acc: 0.7504 - val_loss: 1.1118 - val_acc: 0.6911\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0542 - acc: 0.6946\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.2211 - acc: 0.6370\n",
            "epsilon: 0.003 and test evaluation : 1.2211391925811768, 0.6369982361793518\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.3442 - acc: 0.5986\n",
            "epsilon: 0.005 and test evaluation : 1.344235897064209, 0.5986038446426392\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.6807 - acc: 0.5201\n",
            "epsilon: 0.01 and test evaluation : 1.6806895732879639, 0.5200698375701904\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 2.4020 - acc: 0.3909\n",
            "epsilon: 0.02 and test evaluation : 2.40201735496521, 0.39092496037483215\n",
            "SNR: 33.76535892486572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkS-pL3EkeO",
        "colab_type": "text"
      },
      "source": [
        "# **Show Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH9uGMSFVwwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_df[\"acc_clean_mean\"]= np.sum(result_df['acc_clean'])/3.0\n",
        "result_df[\"acc_0.003_mean\"]= np.sum(result_df['acc1'])/3.0\n",
        "result_df[\"acc_0.005_mean\"]= np.sum(result_df['acc2'])/3.0\n",
        "result_df[\"acc_0.02_mean\"]= np.sum(result_df['acc3'])/3.0\n",
        "result_df[\"acc_0.01_mean\"]= np.sum(result_df['acc4'])/3.0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUfT0VpHiK0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "0ba6fd0e-8e69-475f-ee0b-fd9c0dd70d9d"
      },
      "source": [
        "result_df.head(1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.128819</td>\n",
              "      <td>0.715532</td>\n",
              "      <td>1.265642</td>\n",
              "      <td>0.673647</td>\n",
              "      <td>1.36487</td>\n",
              "      <td>0.643979</td>\n",
              "      <td>1.634894</td>\n",
              "      <td>0.561955</td>\n",
              "      <td>2.21287</td>\n",
              "      <td>0.438045</td>\n",
              "      <td>0.724258</td>\n",
              "      <td>0.67772</td>\n",
              "      <td>0.645143</td>\n",
              "      <td>0.570681</td>\n",
              "      <td>0.447935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean  ...  acc_0.02_mean  acc_0.01_mean\n",
              "0    1.128819   0.715532  ...       0.570681       0.447935\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWpwrKuwmsc",
        "colab_type": "text"
      },
      "source": [
        "# Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxVNgZvPRHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Adversarial Training \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "class AdversarialTraining(object):\n",
        "    \"\"\"Adversarial Training  \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def train(self, pretrained_model, X_train, Y_train, X_test, y_test, epochs, BS, epsilon_list, sgd):\n",
        "        init = (32, 32,1)\n",
        "        res_df = pd.DataFrame(columns=['loss_clean','acc_clean',\n",
        "                                 'loss1', 'acc1','loss2', 'acc2','loss3',\n",
        "                                  'acc3','loss4', 'acc4'])\n",
        "\n",
        "        kfold = KFold(n_splits = 3, random_state = 42)\n",
        "        for j, (train, val) in enumerate(kfold.split(X_train)):\n",
        "          x_train, y_train = self.data_augmentation(X_train[train], Y_train[train], BS, pretrained_model, epsilon_list)\n",
        "          x_val, y_val = self.data_augmentation(X_train[val], Y_train[val], BS, pretrained_model, epsilon_list)\n",
        "          model = create_wide_residual_network(init, nb_classes=4, N=2, k=2, dropout=0.5)\n",
        "          model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"acc\"])\n",
        "          hist = model.fit(generator.flow(x_train, y_train, batch_size=BS), steps_per_epoch=len(x_train) // BS, epochs=epochs,\n",
        "                          validation_data=(x_val, y_val),\n",
        "                          validation_steps=x_val.shape[0] // BS,)\n",
        "          loss, acc = model.evaluate(X_test, y_test)\n",
        "          loss1, acc1 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[0]),X_test, y_test, epsilon_list[0])\n",
        "          loss2, acc2 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[1]),X_test, y_test, epsilon_list[1])\n",
        "          loss3, acc3 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[2]),X_test, y_test, epsilon_list[2])\n",
        "          loss4, acc4 = print_test(model, get_adversarial_examples(pretrained_model, X_test, y_test, epsilon_list[3]),X_test, y_test, epsilon_list[3])\n",
        "          row = {'loss_clean':loss,'acc_clean':acc, 'loss1':loss1, 'acc1':acc1, 'loss2':loss2,\n",
        "                  'acc2':acc2, 'loss3':loss3, 'acc3':acc3, 'loss4':loss4, 'acc4':acc4}\n",
        "          res_df = res_df.append(row , ignore_index=True)\n",
        "          \n",
        "        return res_df\n",
        "    def mini_batch_train(self, model, X_train,y_train, x_val, y_val, BS, pretrained_model, epsilon):\n",
        "\n",
        "\n",
        "        hist = model.fit(generator.flow(X_train, y_train, batch_size=BS), steps_per_epoch=len(X_train) // BS, epochs=1,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   validation_steps=x_val.shape[0] // BS, shuffle = True)\n",
        "        \n",
        "        ### TODO ###\n",
        "        ## Save hist on file.###\n",
        "\n",
        "\n",
        "    def data_augmentation(self, X_train, Y_train, batch_size, pretrained_model, epsilon_list):\n",
        "      ### divide data 16,16,16,16 for 4 different epsilons and 64 is true image. ### \n",
        "        #start_index = self.data_iteration(X_train, batch_size)\n",
        "        first_half_end = int(len(X_train)/2)\n",
        "        second_half_end = int(len(X_train))\n",
        "        x_clean = X_train[0:first_half_end,:,:,:]\n",
        "        x_adv = self.get_adversarial(X_train[first_half_end:second_half_end,:,:,:], Y_train[first_half_end:second_half_end], epsilon_list)\n",
        "        x_mix = self.merge_data(x_clean, x_adv)\n",
        "        y_mix = Y_train[0:second_half_end]\n",
        "        ### TODO###\n",
        "        # Mixture data for 4 epsilon values\n",
        "\n",
        "        return x_mix, y_mix\n",
        "\n",
        "    def data_iteration(self, X_train, batch_size):\n",
        "        N = X_train.shape[0]\n",
        "        start = np.random.randint(0, N-batch_size)\n",
        "        return start\n",
        "\n",
        "    def merge_data(self, x_clean, x_adv):\n",
        "        x_mix = []\n",
        "        for i in range(len(x_clean)):\n",
        "          x_mix.append(x_clean[i])\n",
        "        for j in range(len(x_adv)):\n",
        "          x_mix.append(x_adv[j])\n",
        "        x_mix = np.array(x_mix)\n",
        "\n",
        "        return x_mix\n",
        "\n",
        "\n",
        "    def get_adversarial(self, X_true, y_true, epsilon_list):\n",
        "\n",
        "        return self.adversarial_example(X_true, y_true, epsilon_list)\n",
        "\n",
        "    def adversarial_example(self, X_true, Y_true, epsilon_list):\n",
        "        size = len(X_true)\n",
        "        X_adv = []\n",
        "        interval = int(size/4)\n",
        "        index_list = [0,interval, interval*2, interval*3, size]\n",
        "        index = 0\n",
        "        for epsilon in epsilon_list:\n",
        "          if index == 4:\n",
        "            break\n",
        "          x_true = X_true[index_list[index]:index_list[index+1],:,:,:]\n",
        "          y_true = Y_true[index_list[index]:index_list[index+1]]\n",
        "\n",
        "          index = index + 1\n",
        "\n",
        "          for i in range(len(x_true)):\n",
        "            random_index = i\n",
        "            original_image = x_true[random_index]\n",
        "            original_image = tf.convert_to_tensor(original_image.reshape((1,32,32))) #The .reshape just gives it the proper form to input into the model, a batch of 1 a.k.a a tensor\n",
        "            original_label = y_true[random_index]\n",
        "            original_label = np.reshape(np.argmax(original_label), (1,)).astype('int64')\n",
        "            adv_example_targeted_label = fast_gradient_method(logits_model, original_image, epsilon, np.inf,y=original_label, targeted=False)\n",
        "            X_adv.append(np.array(adv_example_targeted_label).reshape(32,32,1))\n",
        "          \n",
        "        X_adv = np.array(X_adv)\n",
        "        return X_adv\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5vG0s9PAmw",
        "colab_type": "text"
      },
      "source": [
        "Adversarial Training Second Wide ResNet "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR3373MWPvSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon_list = [0.003,0.005,0.01,0.02]\n",
        "adversarial_training =  AdversarialTraining()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2LxFwajOiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits_model = tf.keras.Model(wrn_16_2.input, wrn_16_2.layers[-1].output)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i82EfjWHP2mv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2aa6094-a91c-479a-e5fe-649d7a74cfd5"
      },
      "source": [
        "result_adv_df = adversarial_training.train(logits_model, X_train, Y_train, X_test, y_test, 50, BS, epsilon_list, sgd)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26/26 [==============================] - 2s 68ms/step - loss: 1.6062 - acc: 0.3148 - val_loss: 1.5954 - val_acc: 0.3203 - lr: 0.1000\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.5553 - acc: 0.3611 - val_loss: 1.5925 - val_acc: 0.2720 - lr: 0.1000\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5027 - acc: 0.3714 - val_loss: 1.5651 - val_acc: 0.3512 - lr: 0.1000\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4754 - acc: 0.3811 - val_loss: 1.4927 - val_acc: 0.3559 - lr: 0.1000\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4674 - acc: 0.3844 - val_loss: 1.4599 - val_acc: 0.3727 - lr: 0.1000\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4559 - acc: 0.3886 - val_loss: 1.4393 - val_acc: 0.3844 - lr: 0.1000\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4403 - acc: 0.3811 - val_loss: 1.4563 - val_acc: 0.3990 - lr: 0.1000\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4305 - acc: 0.4028 - val_loss: 1.4319 - val_acc: 0.4118 - lr: 0.1000\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4278 - acc: 0.4159 - val_loss: 1.4311 - val_acc: 0.3873 - lr: 0.1000\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4258 - acc: 0.4116 - val_loss: 1.4062 - val_acc: 0.4333 - lr: 0.1000\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4252 - acc: 0.4128 - val_loss: 1.4296 - val_acc: 0.4199 - lr: 0.1000\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4013 - acc: 0.4428 - val_loss: 1.4232 - val_acc: 0.4578 - lr: 0.1000\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4082 - acc: 0.4346 - val_loss: 1.4105 - val_acc: 0.4467 - lr: 0.1000\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4009 - acc: 0.4549 - val_loss: 1.3932 - val_acc: 0.4298 - lr: 0.1000\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3866 - acc: 0.4734 - val_loss: 1.3896 - val_acc: 0.5003 - lr: 0.1000\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 58ms/step - loss: 1.3569 - acc: 0.4982 - val_loss: 1.3983 - val_acc: 0.4601 - lr: 0.1000\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3394 - acc: 0.5136 - val_loss: 1.4001 - val_acc: 0.4904 - lr: 0.1000\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.3086 - acc: 0.5275 - val_loss: 1.2898 - val_acc: 0.5463 - lr: 0.1000\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2928 - acc: 0.5333 - val_loss: 1.3079 - val_acc: 0.5213 - lr: 0.1000\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2703 - acc: 0.5508 - val_loss: 1.3192 - val_acc: 0.5195 - lr: 0.1000\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2504 - acc: 0.5630 - val_loss: 1.2600 - val_acc: 0.5667 - lr: 0.1000\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2345 - acc: 0.5714 - val_loss: 1.3220 - val_acc: 0.5632 - lr: 0.1000\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2210 - acc: 0.5802 - val_loss: 1.2602 - val_acc: 0.5743 - lr: 0.1000\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 58ms/step - loss: 1.2015 - acc: 0.6010 - val_loss: 1.1832 - val_acc: 0.5900 - lr: 0.1000\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1890 - acc: 0.6005 - val_loss: 1.4667 - val_acc: 0.4916 - lr: 0.1000\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1636 - acc: 0.6102 - val_loss: 1.1360 - val_acc: 0.5929 - lr: 0.1000\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1739 - acc: 0.5962 - val_loss: 1.1737 - val_acc: 0.5911 - lr: 0.1000\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1439 - acc: 0.6217 - val_loss: 1.1915 - val_acc: 0.5935 - lr: 0.1000\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.1092 - acc: 0.6341 - val_loss: 1.1879 - val_acc: 0.6226 - lr: 0.1000\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0960 - acc: 0.6386 - val_loss: 1.1558 - val_acc: 0.6337 - lr: 0.1000\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1121 - acc: 0.6353 - val_loss: 1.1561 - val_acc: 0.6331 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 2s 59ms/step - loss: 1.0813 - acc: 0.6480 - val_loss: 1.2075 - val_acc: 0.6080 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0610 - acc: 0.6489 - val_loss: 1.1157 - val_acc: 0.6476 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0479 - acc: 0.6634 - val_loss: 1.1485 - val_acc: 0.6261 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0364 - acc: 0.6650 - val_loss: 1.0950 - val_acc: 0.6523 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.0369 - acc: 0.6659 - val_loss: 1.1121 - val_acc: 0.6372 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0226 - acc: 0.6737 - val_loss: 1.0816 - val_acc: 0.6564 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0183 - acc: 0.6722 - val_loss: 1.1190 - val_acc: 0.6337 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0252 - acc: 0.6746 - val_loss: 1.1064 - val_acc: 0.6430 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0102 - acc: 0.6904 - val_loss: 1.1275 - val_acc: 0.6418 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0043 - acc: 0.6849 - val_loss: 1.0771 - val_acc: 0.6523 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0197 - acc: 0.6749 - val_loss: 1.0966 - val_acc: 0.6465 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.0072 - acc: 0.6840 - val_loss: 1.0783 - val_acc: 0.6482 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9889 - acc: 0.6870 - val_loss: 1.1202 - val_acc: 0.6459 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9999 - acc: 0.6810 - val_loss: 1.0885 - val_acc: 0.6529 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9849 - acc: 0.7052 - val_loss: 1.0698 - val_acc: 0.6581 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9979 - acc: 0.6837 - val_loss: 1.0932 - val_acc: 0.6465 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9945 - acc: 0.6864 - val_loss: 1.0502 - val_acc: 0.6587 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9903 - acc: 0.6901 - val_loss: 1.0658 - val_acc: 0.6558 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9845 - acc: 0.6964 - val_loss: 1.0623 - val_acc: 0.6651 - lr: 0.0010\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0208 - acc: 0.6649\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0350 - acc: 0.6579\n",
            "epsilon: 0.003 and test evaluation : 1.0350396633148193, 0.657940685749054\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0447 - acc: 0.6510\n",
            "epsilon: 0.005 and test evaluation : 1.0447131395339966, 0.650959849357605\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0696 - acc: 0.6422\n",
            "epsilon: 0.01 and test evaluation : 1.069562554359436, 0.6422338485717773\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.1220 - acc: 0.6195\n",
            "epsilon: 0.02 and test evaluation : 1.1219836473464966, 0.6195462346076965\n",
            "SNR: 33.76535892486572\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 70ms/step - loss: 1.6070 - acc: 0.2759 - val_loss: 1.5841 - val_acc: 0.3584 - lr: 0.1000\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.5450 - acc: 0.3628 - val_loss: 1.5092 - val_acc: 0.3596 - lr: 0.1000\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4974 - acc: 0.3740 - val_loss: 1.5148 - val_acc: 0.3456 - lr: 0.1000\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4868 - acc: 0.3694 - val_loss: 1.4647 - val_acc: 0.3765 - lr: 0.1000\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4749 - acc: 0.3870 - val_loss: 1.4851 - val_acc: 0.3805 - lr: 0.1000\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4569 - acc: 0.3894 - val_loss: 1.4429 - val_acc: 0.4038 - lr: 0.1000\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4508 - acc: 0.3958 - val_loss: 1.4348 - val_acc: 0.3904 - lr: 0.1000\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4381 - acc: 0.3958 - val_loss: 1.6267 - val_acc: 0.3182 - lr: 0.1000\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4286 - acc: 0.4124 - val_loss: 1.4512 - val_acc: 0.3980 - lr: 0.1000\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4197 - acc: 0.4312 - val_loss: 1.4190 - val_acc: 0.4295 - lr: 0.1000\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4192 - acc: 0.4278 - val_loss: 1.4251 - val_acc: 0.4318 - lr: 0.1000\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3997 - acc: 0.4499 - val_loss: 1.4107 - val_acc: 0.4376 - lr: 0.1000\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3927 - acc: 0.4530 - val_loss: 1.4220 - val_acc: 0.4330 - lr: 0.1000\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3662 - acc: 0.4974 - val_loss: 1.4448 - val_acc: 0.4441 - lr: 0.1000\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3322 - acc: 0.5198 - val_loss: 1.3908 - val_acc: 0.4720 - lr: 0.1000\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.3044 - acc: 0.5367 - val_loss: 1.4194 - val_acc: 0.4924 - lr: 0.1000\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2741 - acc: 0.5507 - val_loss: 1.4802 - val_acc: 0.4610 - lr: 0.1000\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2577 - acc: 0.5595 - val_loss: 1.2185 - val_acc: 0.5798 - lr: 0.1000\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2480 - acc: 0.5634 - val_loss: 1.2466 - val_acc: 0.5746 - lr: 0.1000\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2038 - acc: 0.5903 - val_loss: 1.3619 - val_acc: 0.5023 - lr: 0.1000\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 2s 58ms/step - loss: 1.1772 - acc: 0.6100 - val_loss: 1.2933 - val_acc: 0.5664 - lr: 0.1000\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1616 - acc: 0.6172 - val_loss: 1.2614 - val_acc: 0.5746 - lr: 0.1000\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.1322 - acc: 0.6227 - val_loss: 1.1440 - val_acc: 0.6241 - lr: 0.1000\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1261 - acc: 0.6303 - val_loss: 1.1279 - val_acc: 0.6276 - lr: 0.1000\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1134 - acc: 0.6300 - val_loss: 1.2107 - val_acc: 0.5571 - lr: 0.1000\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0651 - acc: 0.6436 - val_loss: 1.1000 - val_acc: 0.6393 - lr: 0.1000\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0464 - acc: 0.6648 - val_loss: 1.0661 - val_acc: 0.6661 - lr: 0.1000\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.0808 - acc: 0.6439 - val_loss: 1.0831 - val_acc: 0.6591 - lr: 0.1000\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0204 - acc: 0.6747 - val_loss: 1.0826 - val_acc: 0.6422 - lr: 0.1000\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0034 - acc: 0.6799 - val_loss: 1.2101 - val_acc: 0.5728 - lr: 0.1000\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0506 - acc: 0.6635 - val_loss: 1.0717 - val_acc: 0.6480 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9919 - acc: 0.6853 - val_loss: 1.0346 - val_acc: 0.6696 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9721 - acc: 0.6893 - val_loss: 1.0503 - val_acc: 0.6603 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9583 - acc: 0.7035 - val_loss: 1.0084 - val_acc: 0.6725 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9428 - acc: 0.7068 - val_loss: 1.0077 - val_acc: 0.6702 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9468 - acc: 0.7095 - val_loss: 1.0126 - val_acc: 0.6684 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9499 - acc: 0.6953 - val_loss: 0.9933 - val_acc: 0.6841 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9429 - acc: 0.7147 - val_loss: 0.9976 - val_acc: 0.6789 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9423 - acc: 0.7083 - val_loss: 0.9836 - val_acc: 0.6929 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9313 - acc: 0.7123 - val_loss: 0.9872 - val_acc: 0.6935 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9303 - acc: 0.7098 - val_loss: 0.9864 - val_acc: 0.6882 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9219 - acc: 0.7234 - val_loss: 0.9962 - val_acc: 0.6772 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9201 - acc: 0.7213 - val_loss: 0.9812 - val_acc: 0.6981 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9120 - acc: 0.7216 - val_loss: 0.9894 - val_acc: 0.6836 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9187 - acc: 0.7153 - val_loss: 0.9960 - val_acc: 0.6748 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 0.9136 - acc: 0.7225 - val_loss: 0.9840 - val_acc: 0.6853 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9114 - acc: 0.7250 - val_loss: 0.9895 - val_acc: 0.6865 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9196 - acc: 0.7156 - val_loss: 0.9726 - val_acc: 0.6999 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9068 - acc: 0.7295 - val_loss: 0.9805 - val_acc: 0.6952 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9018 - acc: 0.7271 - val_loss: 0.9753 - val_acc: 0.6976 - lr: 0.0010\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9204 - acc: 0.7190\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9345 - acc: 0.7103\n",
            "epsilon: 0.003 and test evaluation : 0.9344874620437622, 0.7102966904640198\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.9441 - acc: 0.7086\n",
            "epsilon: 0.005 and test evaluation : 0.9440914988517761, 0.7085514664649963\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 0.9690 - acc: 0.6963\n",
            "epsilon: 0.01 and test evaluation : 0.9690415263175964, 0.6963350772857666\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0223 - acc: 0.6667\n",
            "epsilon: 0.02 and test evaluation : 1.0222744941711426, 0.6666666865348816\n",
            "SNR: 33.76535892486572\n",
            "conv2:channel:  -1\n",
            "conv3 channel_axis:-1 \n",
            "Wide Residual Network-16-2 created.\n",
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 67ms/step - loss: 1.6067 - acc: 0.2859 - val_loss: 1.5701 - val_acc: 0.3747 - lr: 0.1000\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.5315 - acc: 0.3628 - val_loss: 1.5811 - val_acc: 0.3124 - lr: 0.1000\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4941 - acc: 0.3694 - val_loss: 1.4889 - val_acc: 0.3875 - lr: 0.1000\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4797 - acc: 0.3801 - val_loss: 1.4615 - val_acc: 0.3858 - lr: 0.1000\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4626 - acc: 0.3882 - val_loss: 1.4675 - val_acc: 0.4015 - lr: 0.1000\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.4608 - acc: 0.4027 - val_loss: 1.5158 - val_acc: 0.3409 - lr: 0.1000\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4515 - acc: 0.4070 - val_loss: 1.4529 - val_acc: 0.3735 - lr: 0.1000\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.4307 - acc: 0.4189 - val_loss: 1.4447 - val_acc: 0.3928 - lr: 0.1000\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4287 - acc: 0.4239 - val_loss: 1.4012 - val_acc: 0.4353 - lr: 0.1000\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.4244 - acc: 0.4339 - val_loss: 1.3825 - val_acc: 0.4779 - lr: 0.1000\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.4028 - acc: 0.4563 - val_loss: 1.4310 - val_acc: 0.4033 - lr: 0.1000\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3824 - acc: 0.4877 - val_loss: 1.4532 - val_acc: 0.4476 - lr: 0.1000\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3666 - acc: 0.4859 - val_loss: 1.3231 - val_acc: 0.5117 - lr: 0.1000\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3708 - acc: 0.4947 - val_loss: 1.3857 - val_acc: 0.4580 - lr: 0.1000\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3274 - acc: 0.5259 - val_loss: 1.3197 - val_acc: 0.5455 - lr: 0.1000\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.3069 - acc: 0.5286 - val_loss: 1.4036 - val_acc: 0.4936 - lr: 0.1000\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.3130 - acc: 0.5250 - val_loss: 1.4854 - val_acc: 0.4161 - lr: 0.1000\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2664 - acc: 0.5613 - val_loss: 1.4079 - val_acc: 0.5017 - lr: 0.1000\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.2445 - acc: 0.5761 - val_loss: 1.5137 - val_acc: 0.4178 - lr: 0.1000\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.2235 - acc: 0.5800 - val_loss: 1.2975 - val_acc: 0.5589 - lr: 0.1000\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 57ms/step - loss: 1.2001 - acc: 0.5991 - val_loss: 1.1435 - val_acc: 0.6125 - lr: 0.1000\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 1.2000 - acc: 0.5961 - val_loss: 1.5433 - val_acc: 0.4452 - lr: 0.1000\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1997 - acc: 0.5952 - val_loss: 1.2554 - val_acc: 0.5793 - lr: 0.1000\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1528 - acc: 0.6103 - val_loss: 1.1315 - val_acc: 0.6387 - lr: 0.1000\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1457 - acc: 0.6112 - val_loss: 1.1969 - val_acc: 0.5985 - lr: 0.1000\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1207 - acc: 0.6245 - val_loss: 1.4308 - val_acc: 0.5367 - lr: 0.1000\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.1208 - acc: 0.6348 - val_loss: 1.1162 - val_acc: 0.6381 - lr: 0.1000\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0931 - acc: 0.6496 - val_loss: 1.1704 - val_acc: 0.6119 - lr: 0.1000\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0826 - acc: 0.6481 - val_loss: 1.0730 - val_acc: 0.6550 - lr: 0.1000\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0714 - acc: 0.6536 - val_loss: 1.0696 - val_acc: 0.6597 - lr: 0.1000\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.1201 - acc: 0.6327 - val_loss: 1.1139 - val_acc: 0.6329 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0535 - acc: 0.6605 - val_loss: 1.0497 - val_acc: 0.6521 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 1.0344 - acc: 0.6660 - val_loss: 1.0360 - val_acc: 0.6591 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 1.0100 - acc: 0.6853 - val_loss: 1.0382 - val_acc: 0.6568 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9963 - acc: 0.6823 - val_loss: 1.0190 - val_acc: 0.6690 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9903 - acc: 0.6911 - val_loss: 1.0162 - val_acc: 0.6649 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9852 - acc: 0.6950 - val_loss: 1.0077 - val_acc: 0.6713 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9868 - acc: 0.6983 - val_loss: 1.0092 - val_acc: 0.6684 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9863 - acc: 0.6971 - val_loss: 1.0059 - val_acc: 0.6719 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9860 - acc: 0.6965 - val_loss: 0.9990 - val_acc: 0.6818 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9884 - acc: 0.6947 - val_loss: 1.0002 - val_acc: 0.6742 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9840 - acc: 0.6938 - val_loss: 0.9999 - val_acc: 0.6777 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9706 - acc: 0.6986 - val_loss: 1.0121 - val_acc: 0.6801 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9799 - acc: 0.6905 - val_loss: 0.9943 - val_acc: 0.6772 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9727 - acc: 0.6947 - val_loss: 0.9960 - val_acc: 0.6847 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 54ms/step - loss: 0.9762 - acc: 0.6962 - val_loss: 0.9899 - val_acc: 0.6865 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9735 - acc: 0.6959 - val_loss: 0.9960 - val_acc: 0.6812 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9681 - acc: 0.6992 - val_loss: 0.9922 - val_acc: 0.6783 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 56ms/step - loss: 0.9695 - acc: 0.6890 - val_loss: 1.0174 - val_acc: 0.6562 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 55ms/step - loss: 0.9740 - acc: 0.6977 - val_loss: 1.0105 - val_acc: 0.6579 - lr: 0.0010\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0089 - acc: 0.6876\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0212 - acc: 0.6806\n",
            "epsilon: 0.003 and test evaluation : 1.021162986755371, 0.6806282997131348\n",
            "SNR: 50.243844985961914\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 1.0296 - acc: 0.6771\n",
            "epsilon: 0.005 and test evaluation : 1.0295672416687012, 0.6771378517150879\n",
            "SNR: 45.80655574798584\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0511 - acc: 0.6562\n",
            "epsilon: 0.01 and test evaluation : 1.0510777235031128, 0.6561954617500305\n",
            "SNR: 39.78595972061157\n",
            "18/18 [==============================] - 0s 6ms/step - loss: 1.0966 - acc: 0.6318\n",
            "epsilon: 0.02 and test evaluation : 1.0966191291809082, 0.6317626237869263\n",
            "SNR: 33.76535892486572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBzRKS8IoCR",
        "colab_type": "text"
      },
      "source": [
        "# **Show Result**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KYCUfIFW6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_adv_df[\"acc_clean_mean\"]= np.sum(result_adv_df['acc_clean'])/3.0\n",
        "result_adv_df[\"acc_0.003_mean\"]= np.sum(result_adv_df['acc1'])/3.0\n",
        "result_adv_df[\"acc_0.005_mean\"]= np.sum(result_adv_df['acc2'])/3.0\n",
        "result_adv_df[\"acc_0.02_mean\"]= np.sum(result_adv_df['acc3'])/3.0\n",
        "result_adv_df[\"acc_0.01_mean\"]= np.sum(result_adv_df['acc4'])/3.0"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7ANBcRCh4ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "43dd673a-b621-4b03-a0b7-c97ffbf2d33a"
      },
      "source": [
        "result_adv_df.head(1)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss_clean</th>\n",
              "      <th>acc_clean</th>\n",
              "      <th>loss1</th>\n",
              "      <th>acc1</th>\n",
              "      <th>loss2</th>\n",
              "      <th>acc2</th>\n",
              "      <th>loss3</th>\n",
              "      <th>acc3</th>\n",
              "      <th>loss4</th>\n",
              "      <th>acc4</th>\n",
              "      <th>acc_clean_mean</th>\n",
              "      <th>acc_0.003_mean</th>\n",
              "      <th>acc_0.005_mean</th>\n",
              "      <th>acc_0.02_mean</th>\n",
              "      <th>acc_0.01_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.020775</td>\n",
              "      <td>0.664921</td>\n",
              "      <td>1.03504</td>\n",
              "      <td>0.657941</td>\n",
              "      <td>1.044713</td>\n",
              "      <td>0.65096</td>\n",
              "      <td>1.069563</td>\n",
              "      <td>0.642234</td>\n",
              "      <td>1.121984</td>\n",
              "      <td>0.619546</td>\n",
              "      <td>0.690518</td>\n",
              "      <td>0.682955</td>\n",
              "      <td>0.678883</td>\n",
              "      <td>0.664921</td>\n",
              "      <td>0.639325</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loss_clean  acc_clean    loss1  ...  acc_0.005_mean  acc_0.02_mean  acc_0.01_mean\n",
              "0    1.020775   0.664921  1.03504  ...        0.678883       0.664921       0.639325\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}